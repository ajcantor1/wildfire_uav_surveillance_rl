{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgpath2mpl\n",
      "  Downloading svgpath2mpl-1.0.0-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "Installing collected packages: svgpath2mpl\n",
      "Successfully installed svgpath2mpl-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import  deque, namedtuple\n",
    "import random\n",
    "Transition = namedtuple('Transition',('belief_map', 'state_vector', 'action', 'next_belief_map', 'next_state_vector', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.capicity = capacity\n",
    "    self.memory = deque([],maxlen=self.capicity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    \"\"\"Save a transition\"\"\"\n",
    "    self.memory.append(Transition(*args))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "class AbstractFireEnv(metaclass = ABCMeta):\n",
    "\n",
    "  def __init__(self, _height, _width):\n",
    "    self._height = _height\n",
    "    self._width  = _width\n",
    "    self._time_steps = 0\n",
    "    self._observation = None\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "  \n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @property \n",
    "  def time_steps(self):\n",
    "    return self._time_steps\n",
    "\n",
    "  @time_steps.setter\n",
    "  def time_steps(self, _time_steps):\n",
    "    self._time_steps = _time_steps\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return self._observation\n",
    "\n",
    "  @observation.setter\n",
    "  def observation(self, _observation):\n",
    "    self._observation = _observation\n",
    "\n",
    "  def step(self):\n",
    "    self._time_steps += 1\n",
    "    self.observation = self.next_observation()\n",
    "    return self.observation\n",
    "\n",
    "  def plot_heat_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)  \n",
    "    heat_map_plot = ax.imshow(self.observation, cmap='hot')\n",
    "    return heat_map_plot\n",
    "\n",
    "  def reset(self):\n",
    "    self._time_steps = 0\n",
    "    self.observation = self.reset_observation()\n",
    "    seed = self.observation.copy()\n",
    "    for _ in range(30):\n",
    "      self.step()\n",
    "    return seed, self.observation\n",
    "\n",
    "  def fire_in_range(self,margin=2):\n",
    "    burnX, burnY = np.where(self.observation==1)\n",
    "    return min(burnX)>=margin and min(burnY)>=margin and max(burnX)<=99-margin and max(burnY)<=99-margin\n",
    "\n",
    "  @abstractmethod\n",
    "  def next_observation(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def reset_observation(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "D = 2\n",
    "K = 0.05\n",
    "\n",
    "def getNeighbors(point):\n",
    "    neighbors = []\n",
    "    min_x = max(0, point[1]-D)\n",
    "    max_x = min(99, point[1]+D)\n",
    "    min_y = max(0, point[0]-D)\n",
    "    max_y = min(99, point[0]+D)\n",
    "\n",
    "    for y in range(min_y, max_y): \n",
    "      for x in range(min_x, max_x):\n",
    "        neighbors.append((y, x))\n",
    "    return neighbors\n",
    "\n",
    "class ProbabilisticFireEnv(AbstractFireEnv):\n",
    "\n",
    "  def next_observation(self):\n",
    "\n",
    "    probability_map = np.zeros(shape=(HEIGHT,WIDTH), dtype=float)\n",
    "    for row in range(self.height):\n",
    "      for col in range(self.width):\n",
    "        if self.observation[row,col] == 1:\n",
    "          if self.fuel[row, col] > 0:\n",
    "            self.fuel[row, col] -= 1\n",
    "          else:\n",
    "            self.observation[row,col] = 0\n",
    "\n",
    "        elif self.observation[row,col] == 0 and self.fuel[row, col] > 0:\n",
    "          neighboring_cells = getNeighbors((row, col))\n",
    "          pnm = 1\n",
    "          for neighboring_cell in neighboring_cells:\n",
    "            if self.observation[neighboring_cell] == 1:\n",
    "              dnmkl = np.array([a-b for a, b in zip(neighboring_cell, (row,col))])\n",
    "              norm = np.sum(dnmkl**2)\n",
    "              pnmkl0 = K/norm\n",
    "              pnmklw = K*(dnmkl @ self.wind)/norm \n",
    "              pnmkl  = max(0, min(1, (pnmkl0+pnmklw)))\n",
    "              pnm *= (1-pnmkl)\n",
    "          pmn = 1 - pnm\n",
    "          probability_map[row, col] = pmn\n",
    "\n",
    "    self.observation[probability_map > np.random.rand(HEIGHT,WIDTH)]  = 1\n",
    "\n",
    "    return self.observation\n",
    "\n",
    "  def reset_observation(self):\n",
    "    center = [49, 49]\n",
    "    self.observation = np.zeros(shape=(self.height, self.width), dtype=float)\n",
    "    self.observation[center[0]-2:center[0]+2, center[1]-2:center[1]+2] = 1\n",
    "    self.fuel = np.random.randint(low=15, high=20, size=(self.height, self.width))\n",
    "    self.wind = np.random.uniform(low=-0.25, high=0.25, size=2)\n",
    "    return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import random\n",
    "\n",
    "HEIGHT = WIDTH = 100\n",
    "C = 50\n",
    "Cx = 50\n",
    "Cy = 50\n",
    "\n",
    "VELOCITY = 2\n",
    "GRAVITY  = 0.981\n",
    "MINRANGE = 15   # Minimium initial distance from wildfire seed\n",
    "MAXRANGE = 30   # Maximum initial distance from wildfire seed\n",
    "BANK_ANGLE_DELTA  = 5\n",
    "\n",
    "plane_marker = parse_path('M 11.640625 15.0625 L 9.304688 13.015625 L 9.300781 9.621094 L 15.125 11.511719 L 15.117188 10.109375 L 9.257812 5.535156 L 9.25 2.851562 L 9.25 1.296875 C 9.253906 1.019531 9.140625 0.777344 8.960938 0.585938 C 8.738281 0.324219 8.410156 0.15625 8.039062 0.160156 C 8.027344 0.160156 8.011719 0.164062 8 0.164062 C 7.988281 0.164062 7.972656 0.160156 7.960938 0.160156 C 7.589844 0.15625 7.257812 0.324219 7.035156 0.585938 C 6.859375 0.777344 6.746094 1.019531 6.746094 1.296875 L 6.746094 2.851562 L 6.742188 5.535156 L 0.882812 10.109375 L 0.875 11.511719 L 6.699219 9.621094 L 6.691406 13.011719 L 4.359375 15.0625 L 4.355469 15.761719 L 4.628906 15.695312 L 4.628906 15.839844 L 7.511719 14.992188 L 8 14.875 L 8.484375 14.992188 L 11.371094 15.839844 L 11.375 15.695312 L 11.644531 15.761719 Z M 11.640625 15.0625 ')\n",
    "plane_marker.vertices -= plane_marker.vertices.mean(axis=0)\n",
    "#plane_marker = plane_marker.transformed(matplotlib.transforms.Affine2D().rotate_deg(180))\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "  return math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "\n",
    "def shift_matrix(matrix, x, y, padding_value=0):\n",
    "  deltaX = Cx-x\n",
    "  deltaY = Cy-y\n",
    "\n",
    "\n",
    "  if deltaX==0 and deltaY==0:\n",
    "    return matrix\n",
    "\n",
    "  return shift(matrix, (deltaY, deltaX), cval = padding_value)\n",
    "  \n",
    "\n",
    "class Drone:\n",
    "\n",
    "  def __init__(self, _droneEnv, _dt, _dti):\n",
    "    self._bank_angle = 0\n",
    "    self._droneEnv = _droneEnv\n",
    "    self._trajectory = []\n",
    "    self._otherDrone = None\n",
    "    self.dt = _dt\n",
    "    self.dti = _dti\n",
    "\n",
    "  def reset(self):\n",
    "    radius = random.random()*(MAXRANGE-MINRANGE) + MINRANGE\n",
    "    angle = (random.random()-0.5)*2*np.pi\n",
    "    self._x = radius*np.cos(angle) + 50\n",
    "    self._y = radius*np.sin(angle) + 50\n",
    "    self._bank_angle = 0\n",
    "    self._trajectory = [(self.x, self.y)]\n",
    "    self._heading_angle = (random.random()-0.5)*2*np.pi\n",
    "\n",
    "  @property\n",
    "  def otherDrone(self):\n",
    "    return self._otherDrone\n",
    "\n",
    "  @otherDrone.setter\n",
    "  def otherDrone(self, _otherDrone):\n",
    "    self._otherDrone = _otherDrone\n",
    "\n",
    "  @property\n",
    "  def trajectory(self):\n",
    "    return self._trajectory\n",
    "\n",
    "  @trajectory.setter\n",
    "  def trajectory(self, _trajectory):\n",
    "    self._trajectory = _trajectory\n",
    "\n",
    "  @property\n",
    "  def x(self):\n",
    "    return self._x\n",
    "\n",
    "  @x.setter\n",
    "  def x(self, _x):\n",
    "    self._x = _x\n",
    "\n",
    "  @property\n",
    "  def y(self):\n",
    "    return self._y\n",
    "\n",
    "  @y.setter\n",
    "  def y(self, _y):\n",
    "    self._y = _y\n",
    "\n",
    "  @property\n",
    "  def mask(self):\n",
    "    Y, X = np.ogrid[:HEIGHT, :WIDTH]\n",
    "    dist_from_center = np.sqrt((X - self.x)**2 + (Y-self.y)**2)\n",
    "    mask = dist_from_center <= self._droneEnv.scan_radius\n",
    "    return mask\n",
    "\n",
    "\n",
    "  @property\n",
    "  def bank_angle(self):\n",
    "    return self._bank_angle\n",
    "\n",
    "  @bank_angle.setter\n",
    "  def bank_angle(self, _bank_angle):\n",
    "    self._bank_angle = _bank_angle\n",
    "\n",
    "  @property\n",
    "  def heading_angle(self):\n",
    "    return self._heading_angle\n",
    "\n",
    "  @heading_angle.setter\n",
    "  def heading_angle(self, _heading_angle):\n",
    "    self._heading_angle = _heading_angle\n",
    "  \n",
    "  @property\n",
    "  def rho(self):\n",
    "    return euclidean_distance(self.x, self.y, self.otherDrone.x, self.otherDrone.y)\n",
    "\n",
    "  @property\n",
    "  def theta(self):\n",
    "    _theta = np.arctan2((self.otherDrone.y-self.y),(self.otherDrone.x-self.x)) - self.heading_angle\n",
    "    \n",
    "    if (_theta > math.pi):\n",
    "      _theta -= 2*math.pi\n",
    "    elif (_theta<-math.pi):\n",
    "      _theta+= 2*math.pi\n",
    "\n",
    "    return _theta\n",
    "\n",
    "  @property\n",
    "  def psi(self):\n",
    "    _psi = self.otherDrone.heading_angle - self.heading_angle\n",
    "\n",
    "    if (_psi > math.pi):\n",
    "      _psi -= 2*math.pi\n",
    "    elif (_psi<-math.pi):\n",
    "      _psi += 2*math.pi\n",
    "\n",
    "    return _psi\n",
    "    \n",
    "  @property\n",
    "  def state(self):\n",
    "    return np.array([\n",
    "        self.bank_angle, \n",
    "        self.rho,\n",
    "        self.theta,\n",
    "        self.psi,\n",
    "        self.otherDrone.bank_angle\n",
    "    ])[np.newaxis,...]\n",
    "\n",
    "  @property\n",
    "  def belief_map(self):\n",
    "    return self._transform_map(self._droneEnv.belief_map_channel.copy())\n",
    "  \n",
    "  @property\n",
    "  def time_elasped_map(self):\n",
    "    return self._transform_map(self._droneEnv.time_map_channel.copy(), 250.0)/250.0\n",
    "\n",
    "  def _transform_map(self, map, padding_value=0):\n",
    "    return rotate(shift_matrix(map, self.x, self.y, padding_value), angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value)\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return np.stack((self.time_elasped_map, self.belief_map), axis=0)[np.newaxis,...]\n",
    "    \n",
    "  def step(self, input):\n",
    "\n",
    "    self.x +=  VELOCITY*math.cos(self.heading_angle)\n",
    "    self.y +=  VELOCITY*math.sin(self.heading_angle)\n",
    "    self.trajectory.append((self.x, self.y))  \n",
    "    self.heading_angle += GRAVITY*np.tan(self.bank_angle)/(VELOCITY)\n",
    "\n",
    "    if (self.heading_angle>np.pi):\n",
    "      self.heading_angle-=2*np.pi\n",
    "    elif (self.heading_angle<-math.pi):\n",
    "      self.heading_angle+=2*np.pi\n",
    "\n",
    "    action =  5.0*np.pi/180.0 if input==1 else -5.0*np.pi/180.0\n",
    "    self.bank_angle += action\n",
    "    \n",
    "\n",
    "    if self.bank_angle >  50.0*np.pi/180.0 or self.bank_angle < -50.0*np.pi/180.0:\n",
    "      self.bank_angle -= action\n",
    "\n",
    "  @property\n",
    "  def reward(self):\n",
    "\n",
    "    return self._reward1()+self._reward2()+self._reward3()+self._reward4()\n",
    "      \n",
    "\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self.time_elasped_map*250.0, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot \n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self.belief_map, cmap='gray_r', vmin=0, vmax=1)  \n",
    "    return belief_map_plot\n",
    "\n",
    "\n",
    "class DronesEnv:\n",
    "  def __init__(self, _height, _width, _dt, _dti, _scan_radius=10):\n",
    "    self._drones = [Drone(self, _dt, _dti), Drone(self, _dt, _dti)]\n",
    "    self._drones[0].otherDrone = self._drones[1]\n",
    "    self._drones[1].otherDrone = self._drones[0]\n",
    "    self._height = _height \n",
    "    self._width  = _width\n",
    "    self._scan_radius = _scan_radius\n",
    "\n",
    "  @property \n",
    "  def scan_radius(self):\n",
    "    return self._scan_radius\n",
    "\n",
    "  @scan_radius.setter\n",
    "  def scan_radius(self, _scan_radius):\n",
    "    self._scan_radius = _scan_radius\n",
    "        \n",
    "  def reset(self, seed, fireMap):\n",
    "\n",
    "\n",
    "    self._drones[0].reset()\n",
    "    self._drones[1].reset()\n",
    "\n",
    "    self._belief_map_channel = seed\n",
    "    self._time_elapsed_channel = np.full(shape=(self._height, self._width), fill_value=250)\n",
    "    self._scan(fireMap)\n",
    "\n",
    "\n",
    "  def _reward(self, drone, fireMap):\n",
    "    return np.count_nonzero(drone.mask & (self._belief_map_channel==0) & (fireMap==1))\n",
    "\n",
    "  def _scan(self, fireMap):\n",
    "\n",
    "    mask = self.drones[0].mask | self.drones[1].mask\n",
    "\n",
    "    self._belief_map_channel[mask] = fireMap[mask]\n",
    "    \n",
    "    self._time_elapsed_channel[mask] = 0\n",
    "\n",
    "    self._time_elapsed_channel[~mask & (self._time_elapsed_channel < 250)] += 1\n",
    "\n",
    "\n",
    "  @property \n",
    "  def belief_map_channel(self):\n",
    "    return self._belief_map_channel\n",
    "\n",
    "  @belief_map_channel.setter\n",
    "  def belief_map_channel(self, _belief_map_channel):\n",
    "    self._belief_map_channel = _belief_map_channel\n",
    "\n",
    "  @property \n",
    "  def time_map_channel(self):\n",
    "    return self._time_elapsed_channel\n",
    "\n",
    "  @time_map_channel.setter\n",
    "  def time_map_channel(self, _time_elapsed_channel):\n",
    "    self._time_elapsed_channel = _time_elapsed_channel\n",
    "\n",
    "  @property\n",
    "  def drones(self):\n",
    "    return self._drones\n",
    "\n",
    "  def step(self, input, fireMap):\n",
    "    \n",
    "    rewards = []\n",
    "\n",
    "    for move, drone in zip(input,self.drones):\n",
    "      drone.step(move) \n",
    "      rewards.append(self._reward(drone, fireMap))\n",
    "\n",
    "    self._scan(fireMap)\n",
    "    return rewards\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self._time_elapsed_channel, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot\n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self._belief_map_channel, cmap='gray_r', vmin=0, vmax=1)\n",
    "    return belief_map_plot   \n",
    "\n",
    "  def plot_drones(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self.drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self.drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x1 = self._drones[0].x + np.cos(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "    y1 = self._drones[0].y + np.sin(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x2 = self._drones[1].x + np.cos(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    y2 = self._drones[1].y + np.sin(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    \n",
    "    ax.plot(x1, y1, '--')\n",
    "    ax.plot(x2, y2, '--')\n",
    "\n",
    "  def plot_trajectory(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self._drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self._drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    x1, y1 = zip(*self.drones[0].trajectory)\n",
    "    x2, y2 = zip(*self.drones[1].trajectory)\n",
    "\n",
    "    ax.plot(x1, y1, '.')\n",
    "    ax.plot(x2, y2, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class BaseDQN(nn.Module):\n",
    "\n",
    "  def _get_conv_out(self):\n",
    "    o = self.conv(torch.zeros((1, self.channels, self.height, self.width)))\n",
    "    return int(np.prod(o.size()))\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "\n",
    "  @height.setter\n",
    "  def height(self, _height):\n",
    "    self._height = _height\n",
    "\n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @width.setter\n",
    "  def width(self, _width):\n",
    "    self._width = _width\n",
    "\n",
    "  @property\n",
    "  def channels(self):\n",
    "    return self._channels\n",
    "\n",
    "  @channels.setter\n",
    "  def channels(self, _channels):\n",
    "    self._channels = _channels\n",
    "\n",
    "  @property\n",
    "  def outputs(self):\n",
    "    return self._outputs\n",
    "\n",
    "  @outputs.setter\n",
    "  def outputs(self, _outputs):\n",
    "    self._outputs = _outputs\n",
    "    \n",
    "  def __init__(self, _channels, _height, _width, _outputs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.channels = _channels\n",
    "    self.height = _height\n",
    "    self.width = _width\n",
    "    self.outputs = _outputs\n",
    "\n",
    "  def forward(self, belief_map, state_vector):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class DQN(BaseDQN):\n",
    "\n",
    "  def __init__(self, channels, height, width, outputs):\n",
    "    super().__init__(channels, height, width, outputs)\n",
    "\n",
    "    self.fc1  = nn.Sequential(\n",
    "      nn.Linear(5, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "      nn.Conv2d(2, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "  \n",
    "    conv_out_size = self._get_conv_out()\n",
    "\n",
    "    self.fc2 = nn.Sequential(\n",
    "      nn.Linear(conv_out_size, 500),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(500, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc3 = nn.Sequential(\n",
    "      nn.Linear(200, 200),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(200, outputs),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, belief_map, state_vector):\n",
    "    fc1_out = self.fc1(state_vector)\n",
    "    conv_out = torch.flatten(self.conv(belief_map),1)\n",
    "    fc2_out = self.fc2(conv_out)\n",
    "    fc3_out = self.fc3(torch.cat((fc1_out, fc2_out), dim=1))\n",
    "    return fc3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 250000\n",
    "INIT_SIZE = 10000\n",
    "STEPS_TILL_UPDATE = 30\n",
    "SAVE_NETWORKS = 100\n",
    "TAU = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "channels = 2\n",
    "policy_net = DQN(channels, HEIGHT, WIDTH, n_actions).to(device)\n",
    "target_net = DQN(channels, HEIGHT, WIDTH, n_actions).to(device)\n",
    "steps = 0\n",
    "policy_file_path = f'./policy_weights.pt'\n",
    "target_file_path = f'./target_weights.pt'\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = ReplayMemory(50000)\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(belief_map, state_vector, steps):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      output = policy_net(belief_map.cuda(), state_vector.cuda()).max(1)[1].view(1, 1).cpu()\n",
    "      return output\n",
    "  else:\n",
    "    return torch.tensor([[random.randrange(2)]], dtype=torch.long)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(transitions):\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector).cuda()\n",
    "    next_belief_map = torch.cat(batch.next_belief_map).cuda()\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map).cuda()\n",
    "    state_vector_batch = torch.cat(batch.state_vector).cuda()\n",
    "    \n",
    "    action_batch = torch.cat(batch.action).cuda()\n",
    "\n",
    "    \n",
    "    reward_batch = torch.cat(batch.reward).cuda()\n",
    "\n",
    "    state_action_values = policy_net(belief_map_batch, state_vector_batch).gather(1, action_batch)\n",
    "\n",
    "\n",
    "    next_state_values = target_net(next_belief_map, next_states).max(1)[0].detach()\n",
    "\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values.squeeze(), expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    target_net_state_dict = target_net.state_dict()\n",
    "    policy_net_state_dict = policy_net.state_dict()\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "    target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episodes completed\n",
      "total reward 613\n",
      "loss None\n",
      "steps done 1000\n",
      "2 episodes completed\n",
      "total reward 375\n",
      "loss None\n",
      "steps done 1820\n",
      "3 episodes completed\n",
      "total reward 886\n",
      "loss None\n",
      "steps done 2680\n",
      "4 episodes completed\n",
      "total reward 115\n",
      "loss None\n",
      "steps done 3490\n",
      "5 episodes completed\n",
      "total reward 288\n",
      "loss None\n",
      "steps done 4400\n",
      "6 episodes completed\n",
      "total reward 78\n",
      "loss None\n",
      "steps done 5260\n",
      "7 episodes completed\n",
      "total reward 150\n",
      "loss None\n",
      "steps done 6120\n",
      "8 episodes completed\n",
      "total reward 564\n",
      "loss None\n",
      "steps done 6800\n",
      "9 episodes completed\n",
      "total reward 1160\n",
      "loss None\n",
      "steps done 7680\n",
      "10 episodes completed\n",
      "total reward 857\n",
      "loss None\n",
      "steps done 8290\n",
      "11 episodes completed\n",
      "total reward 0\n",
      "loss None\n",
      "steps done 8930\n",
      "12 episodes completed\n",
      "total reward 1390\n",
      "loss None\n",
      "steps done 9880\n",
      "13 episodes completed\n",
      "total reward 5\n",
      "loss 0.7087002992630005\n",
      "steps done 10580\n",
      "14 episodes completed\n",
      "total reward 266\n",
      "loss 0.17173251509666443\n",
      "steps done 11140\n",
      "15 episodes completed\n",
      "total reward 485\n",
      "loss 0.42670509219169617\n",
      "steps done 12020\n",
      "16 episodes completed\n",
      "total reward 1024\n",
      "loss 0.49572092294692993\n",
      "steps done 13320\n",
      "17 episodes completed\n",
      "total reward 939\n",
      "loss 0.3090362548828125\n",
      "steps done 14670\n",
      "18 episodes completed\n",
      "total reward 118\n",
      "loss 0.5132843852043152\n",
      "steps done 15360\n",
      "19 episodes completed\n",
      "total reward 76\n",
      "loss 0.22110430896282196\n",
      "steps done 16020\n",
      "20 episodes completed\n",
      "total reward 446\n",
      "loss 0.9058540463447571\n",
      "steps done 16670\n",
      "21 episodes completed\n",
      "total reward 135\n",
      "loss 0.9214601516723633\n",
      "steps done 17500\n",
      "22 episodes completed\n",
      "total reward 66\n",
      "loss 0.20233768224716187\n",
      "steps done 18250\n",
      "23 episodes completed\n",
      "total reward 619\n",
      "loss 0.3096490502357483\n",
      "steps done 18860\n",
      "24 episodes completed\n",
      "total reward 1201\n",
      "loss 0.5790307521820068\n",
      "steps done 19780\n",
      "update target\n",
      "25 episodes completed\n",
      "total reward 883\n",
      "loss 0.17128340899944305\n",
      "steps done 20450\n",
      "26 episodes completed\n",
      "total reward 860\n",
      "loss 0.7663216590881348\n",
      "steps done 21120\n",
      "27 episodes completed\n",
      "total reward 350\n",
      "loss 0.20888157188892365\n",
      "steps done 22070\n",
      "28 episodes completed\n",
      "total reward 522\n",
      "loss 0.15809060633182526\n",
      "steps done 22710\n",
      "29 episodes completed\n",
      "total reward 743\n",
      "loss 0.5091264247894287\n",
      "steps done 23760\n",
      "30 episodes completed\n",
      "total reward 142\n",
      "loss 0.5566608309745789\n",
      "steps done 24320\n",
      "31 episodes completed\n",
      "total reward 90\n",
      "loss 0.3353821039199829\n",
      "steps done 25170\n",
      "32 episodes completed\n",
      "total reward 663\n",
      "loss 0.16489177942276\n",
      "steps done 25810\n",
      "33 episodes completed\n",
      "total reward 497\n",
      "loss 0.11486459523439407\n",
      "steps done 26770\n",
      "34 episodes completed\n",
      "total reward 2\n",
      "loss 0.47136080265045166\n",
      "steps done 27770\n",
      "35 episodes completed\n",
      "total reward 488\n",
      "loss 1.134477138519287\n",
      "steps done 28330\n",
      "36 episodes completed\n",
      "total reward 208\n",
      "loss 0.4754478931427002\n",
      "steps done 29020\n",
      "37 episodes completed\n",
      "total reward 1168\n",
      "loss 0.4152127206325531\n",
      "steps done 29770\n",
      "update target\n",
      "38 episodes completed\n",
      "total reward 19\n",
      "loss 0.18722492456436157\n",
      "steps done 30560\n",
      "39 episodes completed\n",
      "total reward 0\n",
      "loss 0.2832872271537781\n",
      "steps done 31260\n",
      "40 episodes completed\n",
      "total reward 1519\n",
      "loss 0.6015545129776001\n",
      "steps done 32450\n",
      "41 episodes completed\n",
      "total reward 451\n",
      "loss 0.6117461919784546\n",
      "steps done 33210\n",
      "42 episodes completed\n",
      "total reward 209\n",
      "loss 0.38421154022216797\n",
      "steps done 34020\n",
      "43 episodes completed\n",
      "total reward 1\n",
      "loss 1.161026120185852\n",
      "steps done 34890\n",
      "44 episodes completed\n",
      "total reward 953\n",
      "loss 0.33547747135162354\n",
      "steps done 35700\n",
      "45 episodes completed\n",
      "total reward 418\n",
      "loss 0.37923330068588257\n",
      "steps done 36670\n",
      "46 episodes completed\n",
      "total reward 64\n",
      "loss 0.4531978368759155\n",
      "steps done 37660\n",
      "47 episodes completed\n",
      "total reward 388\n",
      "loss 0.6598491668701172\n",
      "steps done 38360\n",
      "48 episodes completed\n",
      "total reward 978\n",
      "loss 0.24990291893482208\n",
      "steps done 39120\n",
      "49 episodes completed\n",
      "total reward 999\n",
      "loss 0.37962472438812256\n",
      "steps done 39700\n",
      "update target\n",
      "50 episodes completed\n",
      "total reward 237\n",
      "loss 0.3714897632598877\n",
      "steps done 40340\n",
      "51 episodes completed\n",
      "total reward 789\n",
      "loss 0.6517066955566406\n",
      "steps done 41110\n",
      "52 episodes completed\n",
      "total reward 915\n",
      "loss 0.39353328943252563\n",
      "steps done 41820\n",
      "53 episodes completed\n",
      "total reward 143\n",
      "loss 0.22666022181510925\n",
      "steps done 42650\n",
      "54 episodes completed\n",
      "total reward 406\n",
      "loss 0.5510027408599854\n",
      "steps done 43490\n",
      "55 episodes completed\n",
      "total reward 84\n",
      "loss 0.49315527081489563\n",
      "steps done 44310\n",
      "56 episodes completed\n",
      "total reward 813\n",
      "loss 0.8027722835540771\n",
      "steps done 44980\n",
      "57 episodes completed\n",
      "total reward 357\n",
      "loss 0.7636468410491943\n",
      "steps done 45720\n",
      "58 episodes completed\n",
      "total reward 778\n",
      "loss 0.45523256063461304\n",
      "steps done 46370\n",
      "59 episodes completed\n",
      "total reward 74\n",
      "loss 0.16633334755897522\n",
      "steps done 46970\n",
      "60 episodes completed\n",
      "total reward 16\n",
      "loss 0.11303910613059998\n",
      "steps done 47670\n",
      "61 episodes completed\n",
      "total reward 603\n",
      "loss 0.49089375138282776\n",
      "steps done 48640\n",
      "62 episodes completed\n",
      "total reward 440\n",
      "loss 0.8108778595924377\n",
      "steps done 49640\n",
      "update target\n",
      "63 episodes completed\n",
      "total reward 1145\n",
      "loss 0.64989173412323\n",
      "steps done 50530\n",
      "64 episodes completed\n",
      "total reward 43\n",
      "loss 0.4810371398925781\n",
      "steps done 51220\n",
      "65 episodes completed\n",
      "total reward 393\n",
      "loss 0.7820622324943542\n",
      "steps done 51970\n",
      "66 episodes completed\n",
      "total reward 895\n",
      "loss 0.24688754975795746\n",
      "steps done 53020\n",
      "67 episodes completed\n",
      "total reward 144\n",
      "loss 0.5654256939888\n",
      "steps done 54070\n",
      "68 episodes completed\n",
      "total reward 189\n",
      "loss 0.12332671135663986\n",
      "steps done 55320\n",
      "69 episodes completed\n",
      "total reward 591\n",
      "loss 0.8660593032836914\n",
      "steps done 56060\n",
      "70 episodes completed\n",
      "total reward 895\n",
      "loss 0.627670168876648\n",
      "steps done 57250\n",
      "71 episodes completed\n",
      "total reward 234\n",
      "loss 0.341508686542511\n",
      "steps done 58000\n",
      "72 episodes completed\n",
      "total reward 568\n",
      "loss 0.29691681265830994\n",
      "steps done 58850\n",
      "73 episodes completed\n",
      "total reward 342\n",
      "loss 0.24859687685966492\n",
      "steps done 59450\n",
      "update target\n",
      "74 episodes completed\n",
      "total reward 18\n",
      "loss 0.5332576632499695\n",
      "steps done 60110\n",
      "75 episodes completed\n",
      "total reward 663\n",
      "loss 0.5289714932441711\n",
      "steps done 61110\n",
      "76 episodes completed\n",
      "total reward 536\n",
      "loss 1.2448968887329102\n",
      "steps done 61720\n",
      "77 episodes completed\n",
      "total reward 760\n",
      "loss 0.732619047164917\n",
      "steps done 62250\n",
      "78 episodes completed\n",
      "total reward 890\n",
      "loss 0.6981105208396912\n",
      "steps done 63000\n",
      "79 episodes completed\n",
      "total reward 780\n",
      "loss 1.0489221811294556\n",
      "steps done 63660\n",
      "80 episodes completed\n",
      "total reward 578\n",
      "loss 1.4423396587371826\n",
      "steps done 64290\n",
      "81 episodes completed\n",
      "total reward 256\n",
      "loss 1.260095238685608\n",
      "steps done 65060\n",
      "82 episodes completed\n",
      "total reward 1289\n",
      "loss 1.2964346408843994\n",
      "steps done 65820\n",
      "83 episodes completed\n",
      "total reward 547\n",
      "loss 0.26282623410224915\n",
      "steps done 66580\n",
      "84 episodes completed\n",
      "total reward 553\n",
      "loss 1.3012601137161255\n",
      "steps done 67230\n",
      "85 episodes completed\n",
      "total reward 742\n",
      "loss 0.4238784909248352\n",
      "steps done 67970\n",
      "86 episodes completed\n",
      "total reward 290\n",
      "loss 0.2820904850959778\n",
      "steps done 68780\n",
      "87 episodes completed\n",
      "total reward 468\n",
      "loss 0.5901114344596863\n",
      "steps done 69600\n",
      "update target\n",
      "88 episodes completed\n",
      "total reward 460\n",
      "loss 0.70057612657547\n",
      "steps done 70360\n",
      "89 episodes completed\n",
      "total reward 637\n",
      "loss 0.9353867769241333\n",
      "steps done 71080\n",
      "90 episodes completed\n",
      "total reward 794\n",
      "loss 1.066643238067627\n",
      "steps done 71790\n",
      "91 episodes completed\n",
      "total reward 6\n",
      "loss 0.8529768586158752\n",
      "steps done 72910\n",
      "92 episodes completed\n",
      "total reward 884\n",
      "loss 0.331798255443573\n",
      "steps done 73870\n",
      "93 episodes completed\n",
      "total reward 1037\n",
      "loss 1.1182410717010498\n",
      "steps done 74570\n",
      "94 episodes completed\n",
      "total reward 433\n",
      "loss 0.8907589316368103\n",
      "steps done 75240\n",
      "95 episodes completed\n",
      "total reward 1444\n",
      "loss 0.23278822004795074\n",
      "steps done 76090\n",
      "96 episodes completed\n",
      "total reward 634\n",
      "loss 0.6374871730804443\n",
      "steps done 76830\n",
      "97 episodes completed\n",
      "total reward 0\n",
      "loss 0.6516608595848083\n",
      "steps done 77580\n",
      "98 episodes completed\n",
      "total reward 1177\n",
      "loss 1.0502071380615234\n",
      "steps done 78350\n",
      "99 episodes completed\n",
      "total reward 1286\n",
      "loss 1.0830049514770508\n",
      "steps done 78990\n",
      "100 episodes completed\n",
      "total reward 21\n",
      "loss 0.5368847250938416\n",
      "steps done 79570\n",
      "update target\n",
      "101 episodes completed\n",
      "total reward 885\n",
      "loss 0.7187669277191162\n",
      "steps done 80090\n",
      "102 episodes completed\n",
      "total reward 148\n",
      "loss 0.8188439607620239\n",
      "steps done 80710\n",
      "103 episodes completed\n",
      "total reward 282\n",
      "loss 0.7753738164901733\n",
      "steps done 81650\n",
      "104 episodes completed\n",
      "total reward 653\n",
      "loss 0.31290411949157715\n",
      "steps done 82250\n",
      "105 episodes completed\n",
      "total reward 636\n",
      "loss 0.5122634172439575\n",
      "steps done 83070\n",
      "106 episodes completed\n",
      "total reward 467\n",
      "loss 0.9626742005348206\n",
      "steps done 83770\n",
      "107 episodes completed\n",
      "total reward 630\n",
      "loss 0.7924792766571045\n",
      "steps done 84260\n",
      "108 episodes completed\n",
      "total reward 859\n",
      "loss 0.631565272808075\n",
      "steps done 84970\n",
      "109 episodes completed\n",
      "total reward 427\n",
      "loss 1.0047047138214111\n",
      "steps done 85700\n",
      "110 episodes completed\n",
      "total reward 317\n",
      "loss 1.447227120399475\n",
      "steps done 86880\n",
      "111 episodes completed\n",
      "total reward 113\n",
      "loss 0.8018227815628052\n",
      "steps done 87670\n",
      "112 episodes completed\n",
      "total reward 415\n",
      "loss 1.678338646888733\n",
      "steps done 88410\n",
      "113 episodes completed\n",
      "total reward 338\n",
      "loss 0.42291587591171265\n",
      "steps done 88960\n",
      "114 episodes completed\n",
      "total reward 67\n",
      "loss 1.1277097463607788\n",
      "steps done 89480\n",
      "update target\n",
      "115 episodes completed\n",
      "total reward 51\n",
      "loss 1.4090347290039062\n",
      "steps done 90290\n",
      "116 episodes completed\n",
      "total reward 765\n",
      "loss 1.005094289779663\n",
      "steps done 90900\n",
      "117 episodes completed\n",
      "total reward 328\n",
      "loss 1.5940742492675781\n",
      "steps done 91930\n",
      "118 episodes completed\n",
      "total reward 0\n",
      "loss 0.35728269815444946\n",
      "steps done 92460\n",
      "119 episodes completed\n",
      "total reward 850\n",
      "loss 0.7877478003501892\n",
      "steps done 93090\n",
      "120 episodes completed\n",
      "total reward 37\n",
      "loss 1.1235023736953735\n",
      "steps done 93750\n",
      "121 episodes completed\n",
      "total reward 1124\n",
      "loss 0.8476747274398804\n",
      "steps done 94620\n",
      "122 episodes completed\n",
      "total reward 832\n",
      "loss 1.7469956874847412\n",
      "steps done 95310\n",
      "123 episodes completed\n",
      "total reward 253\n",
      "loss 1.4551546573638916\n",
      "steps done 96150\n",
      "124 episodes completed\n",
      "total reward 872\n",
      "loss 1.0395046472549438\n",
      "steps done 96770\n",
      "125 episodes completed\n",
      "total reward 1200\n",
      "loss 2.948676824569702\n",
      "steps done 97430\n",
      "126 episodes completed\n",
      "total reward 1004\n",
      "loss 1.7984671592712402\n",
      "steps done 98060\n",
      "127 episodes completed\n",
      "total reward 654\n",
      "loss 3.0011746883392334\n",
      "steps done 98790\n",
      "128 episodes completed\n",
      "total reward 1017\n",
      "loss 0.6400959491729736\n",
      "steps done 99550\n",
      "update target\n",
      "129 episodes completed\n",
      "total reward 18\n",
      "loss 1.0528494119644165\n",
      "steps done 100530\n",
      "130 episodes completed\n",
      "total reward 354\n",
      "loss 1.0037293434143066\n",
      "steps done 101330\n",
      "131 episodes completed\n",
      "total reward 19\n",
      "loss 1.5341068506240845\n",
      "steps done 102490\n",
      "132 episodes completed\n",
      "total reward 783\n",
      "loss 0.8889815211296082\n",
      "steps done 103770\n",
      "133 episodes completed\n",
      "total reward 544\n",
      "loss 2.4611949920654297\n",
      "steps done 104520\n",
      "134 episodes completed\n",
      "total reward 348\n",
      "loss 0.44578689336776733\n",
      "steps done 105260\n",
      "135 episodes completed\n",
      "total reward 0\n",
      "loss 0.7071385979652405\n",
      "steps done 105930\n",
      "136 episodes completed\n",
      "total reward 830\n",
      "loss 1.5488505363464355\n",
      "steps done 106820\n",
      "137 episodes completed\n",
      "total reward 98\n",
      "loss 1.4974775314331055\n",
      "steps done 107580\n",
      "138 episodes completed\n",
      "total reward 697\n",
      "loss 1.0312727689743042\n",
      "steps done 108130\n",
      "139 episodes completed\n",
      "total reward 810\n",
      "loss 1.2819287776947021\n",
      "steps done 108860\n",
      "140 episodes completed\n",
      "total reward 1081\n",
      "loss 3.1101129055023193\n",
      "steps done 109490\n",
      "update target\n",
      "141 episodes completed\n",
      "total reward 773\n",
      "loss 3.3325204849243164\n",
      "steps done 110210\n",
      "142 episodes completed\n",
      "total reward 1837\n",
      "loss 2.041823387145996\n",
      "steps done 110890\n",
      "143 episodes completed\n",
      "total reward 954\n",
      "loss 1.2878036499023438\n",
      "steps done 111640\n",
      "144 episodes completed\n",
      "total reward 838\n",
      "loss 0.9108167886734009\n",
      "steps done 112700\n",
      "145 episodes completed\n",
      "total reward 903\n",
      "loss 2.536301851272583\n",
      "steps done 113500\n",
      "146 episodes completed\n",
      "total reward 1475\n",
      "loss 2.187950372695923\n",
      "steps done 114160\n",
      "147 episodes completed\n",
      "total reward 874\n",
      "loss 1.0386452674865723\n",
      "steps done 114730\n",
      "148 episodes completed\n",
      "total reward 384\n",
      "loss 1.9199384450912476\n",
      "steps done 115520\n",
      "149 episodes completed\n",
      "total reward 793\n",
      "loss 1.166097640991211\n",
      "steps done 116260\n",
      "150 episodes completed\n",
      "total reward 135\n",
      "loss 1.6892966032028198\n",
      "steps done 116990\n",
      "151 episodes completed\n",
      "total reward 319\n",
      "loss 1.5406049489974976\n",
      "steps done 117540\n",
      "152 episodes completed\n",
      "total reward 714\n",
      "loss 1.5442110300064087\n",
      "steps done 118250\n",
      "153 episodes completed\n",
      "total reward 1289\n",
      "loss 1.625234842300415\n",
      "steps done 119090\n",
      "154 episodes completed\n",
      "total reward 7\n",
      "loss 1.9793894290924072\n",
      "steps done 119790\n",
      "update target\n",
      "155 episodes completed\n",
      "total reward 92\n",
      "loss 1.618460774421692\n",
      "steps done 120410\n",
      "156 episodes completed\n",
      "total reward 1205\n",
      "loss 1.494466781616211\n",
      "steps done 121650\n",
      "157 episodes completed\n",
      "total reward 82\n",
      "loss 3.2254979610443115\n",
      "steps done 122340\n",
      "158 episodes completed\n",
      "total reward 269\n",
      "loss 2.2092456817626953\n",
      "steps done 123120\n",
      "159 episodes completed\n",
      "total reward 295\n",
      "loss 1.2040661573410034\n",
      "steps done 123760\n",
      "160 episodes completed\n",
      "total reward 1111\n",
      "loss 3.2807178497314453\n",
      "steps done 124520\n",
      "161 episodes completed\n",
      "total reward 776\n",
      "loss 2.399146318435669\n",
      "steps done 125180\n",
      "162 episodes completed\n",
      "total reward 975\n",
      "loss 2.874565362930298\n",
      "steps done 125740\n",
      "163 episodes completed\n",
      "total reward 1121\n",
      "loss 1.2968741655349731\n",
      "steps done 126290\n",
      "164 episodes completed\n",
      "total reward 965\n",
      "loss 1.2386958599090576\n",
      "steps done 127060\n",
      "165 episodes completed\n",
      "total reward 1457\n",
      "loss 2.261604070663452\n",
      "steps done 128060\n",
      "166 episodes completed\n",
      "total reward 1085\n",
      "loss 1.7439948320388794\n",
      "steps done 128760\n",
      "167 episodes completed\n",
      "total reward 903\n",
      "loss 0.8870739340782166\n",
      "steps done 129730\n",
      "update target\n",
      "168 episodes completed\n",
      "total reward 389\n",
      "loss 1.6542623043060303\n",
      "steps done 130420\n",
      "169 episodes completed\n",
      "total reward 1317\n",
      "loss 1.5036567449569702\n",
      "steps done 131000\n",
      "170 episodes completed\n",
      "total reward 0\n",
      "loss 2.6422617435455322\n",
      "steps done 131870\n",
      "171 episodes completed\n",
      "total reward 304\n",
      "loss 1.4590703248977661\n",
      "steps done 132630\n",
      "172 episodes completed\n",
      "total reward 11\n",
      "loss 1.8421339988708496\n",
      "steps done 133360\n",
      "173 episodes completed\n",
      "total reward 435\n",
      "loss 1.6178913116455078\n",
      "steps done 134020\n",
      "174 episodes completed\n",
      "total reward 1081\n",
      "loss 0.8409883379936218\n",
      "steps done 134890\n",
      "175 episodes completed\n",
      "total reward 834\n",
      "loss 0.6367566585540771\n",
      "steps done 135580\n",
      "176 episodes completed\n",
      "total reward 295\n",
      "loss 1.0704349279403687\n",
      "steps done 136190\n",
      "177 episodes completed\n",
      "total reward 335\n",
      "loss 1.6769787073135376\n",
      "steps done 136870\n",
      "178 episodes completed\n",
      "total reward 410\n",
      "loss 1.067428469657898\n",
      "steps done 137560\n",
      "179 episodes completed\n",
      "total reward 6\n",
      "loss 1.0035877227783203\n",
      "steps done 138180\n",
      "180 episodes completed\n",
      "total reward 460\n",
      "loss 1.759573221206665\n",
      "steps done 138750\n",
      "181 episodes completed\n",
      "total reward 0\n",
      "loss 0.879685640335083\n",
      "steps done 139310\n",
      "182 episodes completed\n",
      "total reward 200\n",
      "loss 1.5371406078338623\n",
      "steps done 139940\n",
      "update target\n",
      "183 episodes completed\n",
      "total reward 296\n",
      "loss 2.3620643615722656\n",
      "steps done 140740\n",
      "184 episodes completed\n",
      "total reward 289\n",
      "loss 1.1062393188476562\n",
      "steps done 141440\n",
      "185 episodes completed\n",
      "total reward 1213\n",
      "loss 0.8928066492080688\n",
      "steps done 142140\n",
      "186 episodes completed\n",
      "total reward 1101\n",
      "loss 1.3204561471939087\n",
      "steps done 142870\n",
      "187 episodes completed\n",
      "total reward 167\n",
      "loss 2.4878697395324707\n",
      "steps done 143580\n",
      "188 episodes completed\n",
      "total reward 656\n",
      "loss 1.725650429725647\n",
      "steps done 144300\n",
      "189 episodes completed\n",
      "total reward 623\n",
      "loss 1.0075817108154297\n",
      "steps done 145550\n",
      "190 episodes completed\n",
      "total reward 301\n",
      "loss 1.4756298065185547\n",
      "steps done 146180\n",
      "191 episodes completed\n",
      "total reward 804\n",
      "loss 1.5876253843307495\n",
      "steps done 146850\n",
      "192 episodes completed\n",
      "total reward 984\n",
      "loss 1.9943872690200806\n",
      "steps done 147500\n",
      "193 episodes completed\n",
      "total reward 505\n",
      "loss 1.6809492111206055\n",
      "steps done 148160\n",
      "194 episodes completed\n",
      "total reward 896\n",
      "loss 1.7462763786315918\n",
      "steps done 148720\n",
      "195 episodes completed\n",
      "total reward 1382\n",
      "loss 0.899357795715332\n",
      "steps done 149800\n",
      "update target\n",
      "196 episodes completed\n",
      "total reward 1246\n",
      "loss 1.4182024002075195\n",
      "steps done 150660\n",
      "197 episodes completed\n",
      "total reward 160\n",
      "loss 1.827757716178894\n",
      "steps done 151260\n",
      "198 episodes completed\n",
      "total reward 1\n",
      "loss 2.285046100616455\n",
      "steps done 151890\n",
      "199 episodes completed\n",
      "total reward 1124\n",
      "loss 1.6746861934661865\n",
      "steps done 152620\n",
      "200 episodes completed\n",
      "total reward 930\n",
      "loss 1.8203282356262207\n",
      "steps done 153590\n",
      "201 episodes completed\n",
      "total reward 462\n",
      "loss 2.141592025756836\n",
      "steps done 154350\n",
      "202 episodes completed\n",
      "total reward 303\n",
      "loss 2.6514992713928223\n",
      "steps done 154980\n",
      "203 episodes completed\n",
      "total reward 779\n",
      "loss 2.4167795181274414\n",
      "steps done 155620\n",
      "204 episodes completed\n",
      "total reward 239\n",
      "loss 1.180444359779358\n",
      "steps done 156320\n",
      "205 episodes completed\n",
      "total reward 368\n",
      "loss 1.8540546894073486\n",
      "steps done 156980\n",
      "206 episodes completed\n",
      "total reward 438\n",
      "loss 2.9239661693573\n",
      "steps done 158120\n",
      "207 episodes completed\n",
      "total reward 230\n",
      "loss 1.9959239959716797\n",
      "steps done 158870\n",
      "208 episodes completed\n",
      "total reward 510\n",
      "loss 1.633299708366394\n",
      "steps done 159540\n",
      "update target\n",
      "209 episodes completed\n",
      "total reward 785\n",
      "loss 1.3007028102874756\n",
      "steps done 160510\n",
      "210 episodes completed\n",
      "total reward 270\n",
      "loss 1.690682053565979\n",
      "steps done 161120\n",
      "211 episodes completed\n",
      "total reward 540\n",
      "loss 1.5024449825286865\n",
      "steps done 161760\n",
      "212 episodes completed\n",
      "total reward 718\n",
      "loss 0.8602460026741028\n",
      "steps done 162650\n",
      "213 episodes completed\n",
      "total reward 952\n",
      "loss 3.0254507064819336\n",
      "steps done 163830\n",
      "214 episodes completed\n",
      "total reward 734\n",
      "loss 3.3410801887512207\n",
      "steps done 164450\n",
      "215 episodes completed\n",
      "total reward 652\n",
      "loss 2.4983506202697754\n",
      "steps done 165320\n",
      "216 episodes completed\n",
      "total reward 954\n",
      "loss 1.434201955795288\n",
      "steps done 166610\n",
      "217 episodes completed\n",
      "total reward 707\n",
      "loss 1.252372145652771\n",
      "steps done 167350\n",
      "218 episodes completed\n",
      "total reward 412\n",
      "loss 2.8265228271484375\n",
      "steps done 167940\n",
      "219 episodes completed\n",
      "total reward 144\n",
      "loss 1.7361204624176025\n",
      "steps done 169060\n",
      "220 episodes completed\n",
      "total reward 219\n",
      "loss 1.0725737810134888\n",
      "steps done 169840\n",
      "update target\n",
      "221 episodes completed\n",
      "total reward 1038\n",
      "loss 2.6526808738708496\n",
      "steps done 171060\n",
      "222 episodes completed\n",
      "total reward 789\n",
      "loss 2.181985855102539\n",
      "steps done 171810\n",
      "223 episodes completed\n",
      "total reward 1118\n",
      "loss 2.282954692840576\n",
      "steps done 172530\n",
      "224 episodes completed\n",
      "total reward 948\n",
      "loss 2.796281337738037\n",
      "steps done 173120\n",
      "225 episodes completed\n",
      "total reward 551\n",
      "loss 2.548017978668213\n",
      "steps done 173820\n",
      "226 episodes completed\n",
      "total reward 1230\n",
      "loss 2.1816554069519043\n",
      "steps done 174640\n",
      "227 episodes completed\n",
      "total reward 434\n",
      "loss 2.458693504333496\n",
      "steps done 175630\n",
      "228 episodes completed\n",
      "total reward 409\n",
      "loss 3.40290904045105\n",
      "steps done 176170\n",
      "229 episodes completed\n",
      "total reward 649\n",
      "loss 1.8907047510147095\n",
      "steps done 177130\n",
      "230 episodes completed\n",
      "total reward 621\n",
      "loss 1.5774449110031128\n",
      "steps done 177760\n",
      "231 episodes completed\n",
      "total reward 173\n",
      "loss 2.008190393447876\n",
      "steps done 178570\n",
      "232 episodes completed\n",
      "total reward 664\n",
      "loss 1.6114017963409424\n",
      "steps done 179210\n",
      "233 episodes completed\n",
      "total reward 993\n",
      "loss 1.4194320440292358\n",
      "steps done 179840\n",
      "update target\n",
      "234 episodes completed\n",
      "total reward 718\n",
      "loss 1.464255690574646\n",
      "steps done 180740\n",
      "235 episodes completed\n",
      "total reward 485\n",
      "loss 2.220459461212158\n",
      "steps done 181390\n",
      "236 episodes completed\n",
      "total reward 531\n",
      "loss 2.5137643814086914\n",
      "steps done 181830\n",
      "237 episodes completed\n",
      "total reward 746\n",
      "loss 1.865847110748291\n",
      "steps done 182460\n",
      "238 episodes completed\n",
      "total reward 590\n",
      "loss 2.240938186645508\n",
      "steps done 183650\n",
      "239 episodes completed\n",
      "total reward 370\n",
      "loss 2.1370949745178223\n",
      "steps done 184210\n",
      "240 episodes completed\n",
      "total reward 667\n",
      "loss 2.616123676300049\n",
      "steps done 184830\n",
      "241 episodes completed\n",
      "total reward 990\n",
      "loss 1.8766717910766602\n",
      "steps done 185720\n",
      "242 episodes completed\n",
      "total reward 488\n",
      "loss 1.8001065254211426\n",
      "steps done 186230\n",
      "243 episodes completed\n",
      "total reward 473\n",
      "loss 2.9511876106262207\n",
      "steps done 186910\n",
      "244 episodes completed\n",
      "total reward 809\n",
      "loss 1.4898849725723267\n",
      "steps done 187690\n",
      "245 episodes completed\n",
      "total reward 152\n",
      "loss 2.388730049133301\n",
      "steps done 188610\n",
      "246 episodes completed\n",
      "total reward 1114\n",
      "loss 1.4231305122375488\n",
      "steps done 189350\n",
      "247 episodes completed\n",
      "total reward 586\n",
      "loss 1.9510726928710938\n",
      "steps done 189890\n",
      "update target\n",
      "248 episodes completed\n",
      "total reward 258\n",
      "loss 2.8482825756073\n",
      "steps done 190880\n",
      "249 episodes completed\n",
      "total reward 701\n",
      "loss 1.2713706493377686\n",
      "steps done 191960\n",
      "250 episodes completed\n",
      "total reward 438\n",
      "loss 2.921776056289673\n",
      "steps done 192550\n",
      "251 episodes completed\n",
      "total reward 498\n",
      "loss 2.6246728897094727\n",
      "steps done 193410\n",
      "252 episodes completed\n",
      "total reward 97\n",
      "loss 0.9895896911621094\n",
      "steps done 194130\n",
      "253 episodes completed\n",
      "total reward 328\n",
      "loss 3.0867981910705566\n",
      "steps done 194940\n",
      "254 episodes completed\n",
      "total reward 949\n",
      "loss 1.404127836227417\n",
      "steps done 195480\n",
      "255 episodes completed\n",
      "total reward 1015\n",
      "loss 2.9865927696228027\n",
      "steps done 196040\n",
      "256 episodes completed\n",
      "total reward 182\n",
      "loss 1.6704440116882324\n",
      "steps done 196620\n",
      "257 episodes completed\n",
      "total reward 1272\n",
      "loss 1.593047857284546\n",
      "steps done 197420\n",
      "258 episodes completed\n",
      "total reward 861\n",
      "loss 2.3567428588867188\n",
      "steps done 198040\n",
      "259 episodes completed\n",
      "total reward 226\n",
      "loss 2.7615063190460205\n",
      "steps done 198760\n",
      "260 episodes completed\n",
      "total reward 935\n",
      "loss 1.723263144493103\n",
      "steps done 199570\n",
      "update target\n",
      "261 episodes completed\n",
      "total reward 777\n",
      "loss 1.9529366493225098\n",
      "steps done 200590\n",
      "262 episodes completed\n",
      "total reward 105\n",
      "loss 2.524573802947998\n",
      "steps done 201260\n",
      "263 episodes completed\n",
      "total reward 1254\n",
      "loss 2.7720303535461426\n",
      "steps done 202260\n",
      "264 episodes completed\n",
      "total reward 897\n",
      "loss 2.2286911010742188\n",
      "steps done 203520\n",
      "265 episodes completed\n",
      "total reward 914\n",
      "loss 2.327272653579712\n",
      "steps done 204110\n",
      "266 episodes completed\n",
      "total reward 396\n",
      "loss 1.9258160591125488\n",
      "steps done 204660\n",
      "267 episodes completed\n",
      "total reward 695\n",
      "loss 1.6890807151794434\n",
      "steps done 205270\n",
      "268 episodes completed\n",
      "total reward 583\n",
      "loss 2.3296310901641846\n",
      "steps done 206110\n",
      "269 episodes completed\n",
      "total reward 785\n",
      "loss 1.060347557067871\n",
      "steps done 207260\n",
      "270 episodes completed\n",
      "total reward 481\n",
      "loss 2.727078676223755\n",
      "steps done 208150\n",
      "271 episodes completed\n",
      "total reward 658\n",
      "loss 2.379251003265381\n",
      "steps done 208920\n",
      "272 episodes completed\n",
      "total reward 30\n",
      "loss 1.660771369934082\n",
      "steps done 209790\n",
      "update target\n",
      "273 episodes completed\n",
      "total reward 787\n",
      "loss 2.855517864227295\n",
      "steps done 210920\n",
      "274 episodes completed\n",
      "total reward 814\n",
      "loss 1.7003483772277832\n",
      "steps done 212230\n",
      "275 episodes completed\n",
      "total reward 746\n",
      "loss 2.146793842315674\n",
      "steps done 212930\n",
      "276 episodes completed\n",
      "total reward 693\n",
      "loss 2.0846118927001953\n",
      "steps done 213580\n",
      "277 episodes completed\n",
      "total reward 876\n",
      "loss 2.037065267562866\n",
      "steps done 214520\n",
      "278 episodes completed\n",
      "total reward 647\n",
      "loss 1.7455456256866455\n",
      "steps done 215310\n",
      "279 episodes completed\n",
      "total reward 1138\n",
      "loss 1.4581830501556396\n",
      "steps done 216230\n",
      "280 episodes completed\n",
      "total reward 843\n",
      "loss 3.447631359100342\n",
      "steps done 216850\n",
      "281 episodes completed\n",
      "total reward 872\n",
      "loss 1.7702231407165527\n",
      "steps done 217530\n",
      "282 episodes completed\n",
      "total reward 675\n",
      "loss 2.013947010040283\n",
      "steps done 218320\n",
      "283 episodes completed\n",
      "total reward 1632\n",
      "loss 2.6024794578552246\n",
      "steps done 219110\n",
      "update target\n",
      "284 episodes completed\n",
      "total reward 0\n",
      "loss 1.2107726335525513\n",
      "steps done 220210\n",
      "285 episodes completed\n",
      "total reward 1457\n",
      "loss 2.926814079284668\n",
      "steps done 220980\n",
      "286 episodes completed\n",
      "total reward 808\n",
      "loss 1.959743857383728\n",
      "steps done 221640\n",
      "287 episodes completed\n",
      "total reward 1627\n",
      "loss 1.927480697631836\n",
      "steps done 222570\n",
      "288 episodes completed\n",
      "total reward 1586\n",
      "loss 2.6991312503814697\n",
      "steps done 223610\n",
      "289 episodes completed\n",
      "total reward 264\n",
      "loss 1.6665468215942383\n",
      "steps done 224160\n",
      "290 episodes completed\n",
      "total reward 400\n",
      "loss 2.4973177909851074\n",
      "steps done 224940\n",
      "291 episodes completed\n",
      "total reward 377\n",
      "loss 1.4979071617126465\n",
      "steps done 225800\n",
      "292 episodes completed\n",
      "total reward 23\n",
      "loss 2.381319761276245\n",
      "steps done 226410\n",
      "293 episodes completed\n",
      "total reward 716\n",
      "loss 2.0683937072753906\n",
      "steps done 227150\n",
      "294 episodes completed\n",
      "total reward 656\n",
      "loss 2.2106425762176514\n",
      "steps done 227970\n",
      "295 episodes completed\n",
      "total reward 67\n",
      "loss 1.7327392101287842\n",
      "steps done 228640\n",
      "296 episodes completed\n",
      "total reward 1515\n",
      "loss 1.6525946855545044\n",
      "steps done 229300\n",
      "update target\n",
      "297 episodes completed\n",
      "total reward 1290\n",
      "loss 2.6290841102600098\n",
      "steps done 230080\n",
      "298 episodes completed\n",
      "total reward 1644\n",
      "loss 2.185678482055664\n",
      "steps done 231060\n",
      "299 episodes completed\n",
      "total reward 1748\n",
      "loss 1.8330514430999756\n",
      "steps done 231970\n",
      "300 episodes completed\n",
      "total reward 236\n",
      "loss 1.7812213897705078\n",
      "steps done 232610\n",
      "301 episodes completed\n",
      "total reward 1370\n",
      "loss 2.128023386001587\n",
      "steps done 233200\n",
      "302 episodes completed\n",
      "total reward 1043\n",
      "loss 1.6505531072616577\n",
      "steps done 233980\n",
      "303 episodes completed\n",
      "total reward 986\n",
      "loss 3.6068038940429688\n",
      "steps done 234930\n",
      "304 episodes completed\n",
      "total reward 194\n",
      "loss 3.394552230834961\n",
      "steps done 235650\n",
      "305 episodes completed\n",
      "total reward 949\n",
      "loss 3.0536255836486816\n",
      "steps done 236640\n",
      "306 episodes completed\n",
      "total reward 7\n",
      "loss 2.96089768409729\n",
      "steps done 237360\n",
      "307 episodes completed\n",
      "total reward 859\n",
      "loss 2.168025016784668\n",
      "steps done 238030\n",
      "308 episodes completed\n",
      "total reward 1206\n",
      "loss 1.9922949075698853\n",
      "steps done 238740\n",
      "309 episodes completed\n",
      "total reward 1647\n",
      "loss 3.5474443435668945\n",
      "steps done 239330\n",
      "310 episodes completed\n",
      "total reward 1083\n",
      "loss 2.0595266819000244\n",
      "steps done 239940\n",
      "update target\n",
      "311 episodes completed\n",
      "total reward 1630\n",
      "loss 3.3332080841064453\n",
      "steps done 240700\n",
      "312 episodes completed\n",
      "total reward 287\n",
      "loss 1.7331584692001343\n",
      "steps done 241740\n",
      "313 episodes completed\n",
      "total reward 1164\n",
      "loss 2.920687437057495\n",
      "steps done 242470\n",
      "314 episodes completed\n",
      "total reward 1211\n",
      "loss 4.094049453735352\n",
      "steps done 243090\n",
      "315 episodes completed\n",
      "total reward 1241\n",
      "loss 2.0920047760009766\n",
      "steps done 243720\n",
      "316 episodes completed\n",
      "total reward 320\n",
      "loss 1.5367565155029297\n",
      "steps done 244480\n",
      "317 episodes completed\n",
      "total reward 1219\n",
      "loss 1.7497210502624512\n",
      "steps done 245330\n",
      "318 episodes completed\n",
      "total reward 922\n",
      "loss 3.3747482299804688\n",
      "steps done 246200\n",
      "319 episodes completed\n",
      "total reward 125\n",
      "loss 2.2889771461486816\n",
      "steps done 246760\n",
      "320 episodes completed\n",
      "total reward 323\n",
      "loss 1.425062656402588\n",
      "steps done 247360\n",
      "321 episodes completed\n",
      "total reward 467\n",
      "loss 2.6143717765808105\n",
      "steps done 247860\n",
      "322 episodes completed\n",
      "total reward 689\n",
      "loss 1.928997278213501\n",
      "steps done 248740\n",
      "323 episodes completed\n",
      "total reward 1068\n",
      "loss 2.205681562423706\n",
      "steps done 249480\n",
      "update target\n",
      "324 episodes completed\n",
      "total reward 748\n",
      "loss 3.7240216732025146\n",
      "steps done 250390\n",
      "325 episodes completed\n",
      "total reward 599\n",
      "loss 2.6552670001983643\n",
      "steps done 251020\n",
      "326 episodes completed\n",
      "total reward 1236\n",
      "loss 2.7940244674682617\n",
      "steps done 251950\n",
      "327 episodes completed\n",
      "total reward 1477\n",
      "loss 1.4634455442428589\n",
      "steps done 252730\n",
      "328 episodes completed\n",
      "total reward 1677\n",
      "loss 4.12345027923584\n",
      "steps done 253440\n",
      "329 episodes completed\n",
      "total reward 1390\n",
      "loss 2.030085563659668\n",
      "steps done 254340\n",
      "330 episodes completed\n",
      "total reward 1600\n",
      "loss 1.9895861148834229\n",
      "steps done 255160\n",
      "331 episodes completed\n",
      "total reward 1255\n",
      "loss 2.9763400554656982\n",
      "steps done 255990\n",
      "332 episodes completed\n",
      "total reward 665\n",
      "loss 4.67901086807251\n",
      "steps done 256700\n",
      "333 episodes completed\n",
      "total reward 473\n",
      "loss 3.2927370071411133\n",
      "steps done 257370\n",
      "334 episodes completed\n",
      "total reward 973\n",
      "loss 3.116626024246216\n",
      "steps done 257950\n",
      "335 episodes completed\n",
      "total reward 947\n",
      "loss 1.420831322669983\n",
      "steps done 258820\n",
      "336 episodes completed\n",
      "total reward 1064\n",
      "loss 1.8999050855636597\n",
      "steps done 259370\n",
      "update target\n",
      "337 episodes completed\n",
      "total reward 955\n",
      "loss 2.620171546936035\n",
      "steps done 260100\n",
      "338 episodes completed\n",
      "total reward 957\n",
      "loss 3.3922924995422363\n",
      "steps done 261110\n",
      "339 episodes completed\n",
      "total reward 1173\n",
      "loss 3.4578940868377686\n",
      "steps done 261860\n",
      "340 episodes completed\n",
      "total reward 1316\n",
      "loss 3.9815707206726074\n",
      "steps done 262790\n",
      "341 episodes completed\n",
      "total reward 1004\n",
      "loss 2.7371697425842285\n",
      "steps done 263750\n",
      "342 episodes completed\n",
      "total reward 898\n",
      "loss 2.363417148590088\n",
      "steps done 264680\n",
      "343 episodes completed\n",
      "total reward 1377\n",
      "loss 1.8002550601959229\n",
      "steps done 265620\n",
      "344 episodes completed\n",
      "total reward 1488\n",
      "loss 3.5341732501983643\n",
      "steps done 266430\n",
      "345 episodes completed\n",
      "total reward 757\n",
      "loss 3.50132155418396\n",
      "steps done 267160\n",
      "346 episodes completed\n",
      "total reward 1815\n",
      "loss 2.5548901557922363\n",
      "steps done 267810\n",
      "347 episodes completed\n",
      "total reward 992\n",
      "loss 3.536342144012451\n",
      "steps done 268730\n",
      "348 episodes completed\n",
      "total reward 1076\n",
      "loss 2.6051676273345947\n",
      "steps done 269520\n",
      "update target\n",
      "349 episodes completed\n",
      "total reward 261\n",
      "loss 4.420785903930664\n",
      "steps done 270330\n",
      "350 episodes completed\n",
      "total reward 788\n",
      "loss 2.4503068923950195\n",
      "steps done 271160\n",
      "351 episodes completed\n",
      "total reward 1114\n",
      "loss 4.27390718460083\n",
      "steps done 272130\n",
      "352 episodes completed\n",
      "total reward 1610\n",
      "loss 2.992520570755005\n",
      "steps done 272800\n",
      "353 episodes completed\n",
      "total reward 794\n",
      "loss 3.4180517196655273\n",
      "steps done 273930\n",
      "354 episodes completed\n",
      "total reward 1964\n",
      "loss 2.1683223247528076\n",
      "steps done 275180\n",
      "355 episodes completed\n",
      "total reward 1100\n",
      "loss 3.1305618286132812\n",
      "steps done 275760\n",
      "356 episodes completed\n",
      "total reward 250\n",
      "loss 2.129970073699951\n",
      "steps done 276620\n",
      "357 episodes completed\n",
      "total reward 614\n",
      "loss 2.2978250980377197\n",
      "steps done 277280\n",
      "358 episodes completed\n",
      "total reward 269\n",
      "loss 4.2343549728393555\n",
      "steps done 278140\n",
      "359 episodes completed\n",
      "total reward 1100\n",
      "loss 2.1896114349365234\n",
      "steps done 279000\n",
      "update target\n",
      "360 episodes completed\n",
      "total reward 1722\n",
      "loss 4.310250759124756\n",
      "steps done 280110\n",
      "361 episodes completed\n",
      "total reward 1851\n",
      "loss 2.86027193069458\n",
      "steps done 281290\n",
      "362 episodes completed\n",
      "total reward 1206\n",
      "loss 2.5401511192321777\n",
      "steps done 282040\n",
      "363 episodes completed\n",
      "total reward 126\n",
      "loss 2.513150691986084\n",
      "steps done 282600\n",
      "364 episodes completed\n",
      "total reward 924\n",
      "loss 2.907927989959717\n",
      "steps done 283220\n",
      "365 episodes completed\n",
      "total reward 1782\n",
      "loss 3.346787452697754\n",
      "steps done 283870\n",
      "366 episodes completed\n",
      "total reward 153\n",
      "loss 1.827697515487671\n",
      "steps done 284480\n",
      "367 episodes completed\n",
      "total reward 1444\n",
      "loss 2.9575231075286865\n",
      "steps done 285130\n",
      "368 episodes completed\n",
      "total reward 1052\n",
      "loss 3.088351249694824\n",
      "steps done 285910\n",
      "369 episodes completed\n",
      "total reward 611\n",
      "loss 2.8504388332366943\n",
      "steps done 286510\n",
      "370 episodes completed\n",
      "total reward 1315\n",
      "loss 3.213113307952881\n",
      "steps done 287170\n",
      "371 episodes completed\n",
      "total reward 1016\n",
      "loss 2.3921823501586914\n",
      "steps done 287760\n",
      "372 episodes completed\n",
      "total reward 646\n",
      "loss 1.704103946685791\n",
      "steps done 288720\n",
      "373 episodes completed\n",
      "total reward 255\n",
      "loss 3.6304163932800293\n",
      "steps done 289880\n",
      "update target\n",
      "374 episodes completed\n",
      "total reward 1341\n",
      "loss 2.3634033203125\n",
      "steps done 290730\n",
      "375 episodes completed\n",
      "total reward 1213\n",
      "loss 2.6864871978759766\n",
      "steps done 291290\n",
      "376 episodes completed\n",
      "total reward 1450\n",
      "loss 4.867936134338379\n",
      "steps done 292090\n",
      "377 episodes completed\n",
      "total reward 1873\n",
      "loss 3.423666000366211\n",
      "steps done 292730\n",
      "378 episodes completed\n",
      "total reward 1184\n",
      "loss 2.908100128173828\n",
      "steps done 293240\n",
      "379 episodes completed\n",
      "total reward 914\n",
      "loss 2.2280142307281494\n",
      "steps done 293860\n",
      "380 episodes completed\n",
      "total reward 745\n",
      "loss 2.3779120445251465\n",
      "steps done 294560\n",
      "381 episodes completed\n",
      "total reward 6\n",
      "loss 2.277083158493042\n",
      "steps done 295570\n",
      "382 episodes completed\n",
      "total reward 2334\n",
      "loss 3.661831855773926\n",
      "steps done 296570\n",
      "383 episodes completed\n",
      "total reward 1456\n",
      "loss 3.306440591812134\n",
      "steps done 297330\n",
      "384 episodes completed\n",
      "total reward 595\n",
      "loss 3.332280158996582\n",
      "steps done 298050\n",
      "385 episodes completed\n",
      "total reward 1463\n",
      "loss 2.356001853942871\n",
      "steps done 298820\n",
      "386 episodes completed\n",
      "total reward 954\n",
      "loss 3.367276191711426\n",
      "steps done 299390\n",
      "update target\n",
      "387 episodes completed\n",
      "total reward 1485\n",
      "loss 3.9939842224121094\n",
      "steps done 300070\n",
      "388 episodes completed\n",
      "total reward 1417\n",
      "loss 2.6485323905944824\n",
      "steps done 300900\n",
      "389 episodes completed\n",
      "total reward 400\n",
      "loss 4.712522983551025\n",
      "steps done 301490\n",
      "390 episodes completed\n",
      "total reward 1122\n",
      "loss 3.540572166442871\n",
      "steps done 302470\n",
      "391 episodes completed\n",
      "total reward 568\n",
      "loss 2.876946449279785\n",
      "steps done 303050\n",
      "392 episodes completed\n",
      "total reward 1131\n",
      "loss 3.894289493560791\n",
      "steps done 303690\n",
      "393 episodes completed\n",
      "total reward 1278\n",
      "loss 3.133734703063965\n",
      "steps done 304710\n",
      "394 episodes completed\n",
      "total reward 1076\n",
      "loss 4.148674488067627\n",
      "steps done 305550\n",
      "395 episodes completed\n",
      "total reward 30\n",
      "loss 3.200974702835083\n",
      "steps done 306590\n",
      "396 episodes completed\n",
      "total reward 702\n",
      "loss 2.884316921234131\n",
      "steps done 307300\n",
      "397 episodes completed\n",
      "total reward 1329\n",
      "loss 3.184077739715576\n",
      "steps done 308180\n",
      "398 episodes completed\n",
      "total reward 510\n",
      "loss 3.771749258041382\n",
      "steps done 308920\n",
      "399 episodes completed\n",
      "total reward 1613\n",
      "loss 1.86562979221344\n",
      "steps done 309870\n",
      "update target\n",
      "400 episodes completed\n",
      "total reward 130\n",
      "loss 3.1903157234191895\n",
      "steps done 310600\n",
      "401 episodes completed\n",
      "total reward 1443\n",
      "loss 2.4353694915771484\n",
      "steps done 311370\n",
      "402 episodes completed\n",
      "total reward 328\n",
      "loss 4.381031036376953\n",
      "steps done 311940\n",
      "403 episodes completed\n",
      "total reward 1870\n",
      "loss 3.046455144882202\n",
      "steps done 313120\n",
      "404 episodes completed\n",
      "total reward 471\n",
      "loss 3.153658390045166\n",
      "steps done 313980\n",
      "405 episodes completed\n",
      "total reward 883\n",
      "loss 2.3829774856567383\n",
      "steps done 314680\n",
      "406 episodes completed\n",
      "total reward 1309\n",
      "loss 2.418435573577881\n",
      "steps done 315350\n",
      "407 episodes completed\n",
      "total reward 1751\n",
      "loss 3.7380547523498535\n",
      "steps done 316660\n",
      "408 episodes completed\n",
      "total reward 820\n",
      "loss 3.5896241664886475\n",
      "steps done 317350\n",
      "409 episodes completed\n",
      "total reward 319\n",
      "loss 2.7263095378875732\n",
      "steps done 317930\n",
      "410 episodes completed\n",
      "total reward 1033\n",
      "loss 1.8075737953186035\n",
      "steps done 318470\n",
      "411 episodes completed\n",
      "total reward 1011\n",
      "loss 2.918135166168213\n",
      "steps done 319030\n",
      "412 episodes completed\n",
      "total reward 1445\n",
      "loss 3.5208992958068848\n",
      "steps done 319680\n",
      "update target\n",
      "413 episodes completed\n",
      "total reward 1003\n",
      "loss 2.995356559753418\n",
      "steps done 320290\n",
      "414 episodes completed\n",
      "total reward 2366\n",
      "loss 3.03366756439209\n",
      "steps done 321530\n",
      "415 episodes completed\n",
      "total reward 1395\n",
      "loss 2.1104743480682373\n",
      "steps done 322450\n",
      "416 episodes completed\n",
      "total reward 1107\n",
      "loss 3.157470941543579\n",
      "steps done 323230\n",
      "417 episodes completed\n",
      "total reward 1889\n",
      "loss 3.6172549724578857\n",
      "steps done 324530\n",
      "418 episodes completed\n",
      "total reward 907\n",
      "loss 3.6139631271362305\n",
      "steps done 325520\n",
      "419 episodes completed\n",
      "total reward 1519\n",
      "loss 3.096409320831299\n",
      "steps done 326200\n",
      "420 episodes completed\n",
      "total reward 150\n",
      "loss 2.7366080284118652\n",
      "steps done 326970\n",
      "421 episodes completed\n",
      "total reward 99\n",
      "loss 2.7186245918273926\n",
      "steps done 327480\n",
      "422 episodes completed\n",
      "total reward 1254\n",
      "loss 3.738349199295044\n",
      "steps done 328200\n",
      "423 episodes completed\n",
      "total reward 927\n",
      "loss 2.124202013015747\n",
      "steps done 328790\n",
      "424 episodes completed\n",
      "total reward 924\n",
      "loss 3.5008206367492676\n",
      "steps done 329790\n",
      "update target\n",
      "425 episodes completed\n",
      "total reward 607\n",
      "loss 2.844841718673706\n",
      "steps done 330350\n",
      "426 episodes completed\n",
      "total reward 1503\n",
      "loss 2.956031322479248\n",
      "steps done 331530\n",
      "427 episodes completed\n",
      "total reward 944\n",
      "loss 2.3642477989196777\n",
      "steps done 332140\n",
      "428 episodes completed\n",
      "total reward 988\n",
      "loss 2.517711639404297\n",
      "steps done 332670\n",
      "429 episodes completed\n",
      "total reward 1122\n",
      "loss 4.220832824707031\n",
      "steps done 333280\n",
      "430 episodes completed\n",
      "total reward 452\n",
      "loss 3.1325464248657227\n",
      "steps done 333890\n",
      "431 episodes completed\n",
      "total reward 2237\n",
      "loss 2.7653157711029053\n",
      "steps done 334800\n",
      "432 episodes completed\n",
      "total reward 1142\n",
      "loss 2.918607234954834\n",
      "steps done 335680\n",
      "433 episodes completed\n",
      "total reward 1948\n",
      "loss 3.606924295425415\n",
      "steps done 336520\n",
      "434 episodes completed\n",
      "total reward 1164\n",
      "loss 1.977910041809082\n",
      "steps done 337090\n",
      "435 episodes completed\n",
      "total reward 1079\n",
      "loss 3.2941081523895264\n",
      "steps done 337780\n",
      "436 episodes completed\n",
      "total reward 1171\n",
      "loss 2.9842770099639893\n",
      "steps done 338380\n",
      "437 episodes completed\n",
      "total reward 1177\n",
      "loss 3.917559862136841\n",
      "steps done 338990\n",
      "438 episodes completed\n",
      "total reward 1148\n",
      "loss 3.094571352005005\n",
      "steps done 339750\n",
      "update target\n",
      "439 episodes completed\n",
      "total reward 764\n",
      "loss 2.683401107788086\n",
      "steps done 340420\n",
      "440 episodes completed\n",
      "total reward 1259\n",
      "loss 4.5083327293396\n",
      "steps done 341110\n",
      "441 episodes completed\n",
      "total reward 266\n",
      "loss 3.6139235496520996\n",
      "steps done 341670\n",
      "442 episodes completed\n",
      "total reward 1182\n",
      "loss 3.1830763816833496\n",
      "steps done 342270\n",
      "443 episodes completed\n",
      "total reward 688\n",
      "loss 3.2668704986572266\n",
      "steps done 342980\n",
      "444 episodes completed\n",
      "total reward 77\n",
      "loss 2.9531493186950684\n",
      "steps done 343660\n",
      "445 episodes completed\n",
      "total reward 1332\n",
      "loss 3.461456060409546\n",
      "steps done 344570\n",
      "446 episodes completed\n",
      "total reward 864\n",
      "loss 3.730210304260254\n",
      "steps done 345120\n",
      "447 episodes completed\n",
      "total reward 1680\n",
      "loss 4.408374786376953\n",
      "steps done 345790\n",
      "448 episodes completed\n",
      "total reward 415\n",
      "loss 2.8782620429992676\n",
      "steps done 346460\n",
      "449 episodes completed\n",
      "total reward 1197\n",
      "loss 2.5944201946258545\n",
      "steps done 347130\n",
      "450 episodes completed\n",
      "total reward 245\n",
      "loss 3.111320972442627\n",
      "steps done 347660\n",
      "451 episodes completed\n",
      "total reward 2192\n",
      "loss 3.75319242477417\n",
      "steps done 348470\n",
      "452 episodes completed\n",
      "total reward 1100\n",
      "loss 3.3325653076171875\n",
      "steps done 349260\n",
      "453 episodes completed\n",
      "total reward 1435\n",
      "loss 3.2097277641296387\n",
      "steps done 349860\n",
      "update target\n",
      "454 episodes completed\n",
      "total reward 587\n",
      "loss 3.7794442176818848\n",
      "steps done 350470\n",
      "455 episodes completed\n",
      "total reward 1488\n",
      "loss 2.986365795135498\n",
      "steps done 351400\n",
      "456 episodes completed\n",
      "total reward 1104\n",
      "loss 3.39235258102417\n",
      "steps done 352090\n",
      "457 episodes completed\n",
      "total reward 1694\n",
      "loss 2.7099857330322266\n",
      "steps done 352740\n",
      "458 episodes completed\n",
      "total reward 1278\n",
      "loss 5.135112762451172\n",
      "steps done 353600\n",
      "459 episodes completed\n",
      "total reward 1875\n",
      "loss 2.9259629249572754\n",
      "steps done 354660\n",
      "460 episodes completed\n",
      "total reward 2000\n",
      "loss 4.838529109954834\n",
      "steps done 355730\n",
      "461 episodes completed\n",
      "total reward 1745\n",
      "loss 3.752174139022827\n",
      "steps done 356530\n",
      "462 episodes completed\n",
      "total reward 1585\n",
      "loss 3.297935962677002\n",
      "steps done 357740\n",
      "463 episodes completed\n",
      "total reward 1415\n",
      "loss 4.31613302230835\n",
      "steps done 358560\n",
      "464 episodes completed\n",
      "total reward 1140\n",
      "loss 2.6413474082946777\n",
      "steps done 359090\n",
      "465 episodes completed\n",
      "total reward 1127\n",
      "loss 2.621155261993408\n",
      "steps done 359580\n",
      "update target\n",
      "466 episodes completed\n",
      "total reward 2418\n",
      "loss 4.205198287963867\n",
      "steps done 360710\n",
      "467 episodes completed\n",
      "total reward 591\n",
      "loss 4.192561626434326\n",
      "steps done 361280\n",
      "468 episodes completed\n",
      "total reward 1818\n",
      "loss 4.128468990325928\n",
      "steps done 362220\n",
      "469 episodes completed\n",
      "total reward 1103\n",
      "loss 4.810029029846191\n",
      "steps done 363070\n",
      "470 episodes completed\n",
      "total reward 1164\n",
      "loss 5.167666435241699\n",
      "steps done 363770\n",
      "471 episodes completed\n",
      "total reward 1498\n",
      "loss 3.3693010807037354\n",
      "steps done 364650\n",
      "472 episodes completed\n",
      "total reward 967\n",
      "loss 3.738905429840088\n",
      "steps done 365290\n",
      "473 episodes completed\n",
      "total reward 1369\n",
      "loss 2.5039827823638916\n",
      "steps done 366050\n",
      "474 episodes completed\n",
      "total reward 1587\n",
      "loss 4.165310859680176\n",
      "steps done 366870\n",
      "475 episodes completed\n",
      "total reward 825\n",
      "loss 4.008708477020264\n",
      "steps done 367580\n",
      "476 episodes completed\n",
      "total reward 1121\n",
      "loss 3.976172924041748\n",
      "steps done 368180\n",
      "477 episodes completed\n",
      "total reward 1700\n",
      "loss 6.3454694747924805\n",
      "steps done 369110\n",
      "478 episodes completed\n",
      "total reward 1262\n",
      "loss 4.18187952041626\n",
      "steps done 369830\n",
      "update target\n",
      "479 episodes completed\n",
      "total reward 1367\n",
      "loss 4.3766374588012695\n",
      "steps done 370580\n",
      "480 episodes completed\n",
      "total reward 1378\n",
      "loss 5.145688056945801\n",
      "steps done 371210\n",
      "481 episodes completed\n",
      "total reward 1446\n",
      "loss 3.376255989074707\n",
      "steps done 371920\n",
      "482 episodes completed\n",
      "total reward 1267\n",
      "loss 3.7547223567962646\n",
      "steps done 372850\n",
      "483 episodes completed\n",
      "total reward 1258\n",
      "loss 3.060211181640625\n",
      "steps done 373570\n",
      "484 episodes completed\n",
      "total reward 1582\n",
      "loss 3.6477978229522705\n",
      "steps done 374140\n",
      "485 episodes completed\n",
      "total reward 1140\n",
      "loss 4.065158843994141\n",
      "steps done 374810\n",
      "486 episodes completed\n",
      "total reward 1202\n",
      "loss 5.489680290222168\n",
      "steps done 375450\n",
      "487 episodes completed\n",
      "total reward 1345\n",
      "loss 5.432726860046387\n",
      "steps done 376160\n",
      "488 episodes completed\n",
      "total reward 1768\n",
      "loss 4.246325492858887\n",
      "steps done 377140\n",
      "489 episodes completed\n",
      "total reward 1077\n",
      "loss 3.635758876800537\n",
      "steps done 377720\n",
      "490 episodes completed\n",
      "total reward 925\n",
      "loss 3.545964241027832\n",
      "steps done 378340\n",
      "491 episodes completed\n",
      "total reward 882\n",
      "loss 4.285946369171143\n",
      "steps done 378900\n",
      "492 episodes completed\n",
      "total reward 884\n",
      "loss 3.2944281101226807\n",
      "steps done 379520\n",
      "update target\n",
      "493 episodes completed\n",
      "total reward 1996\n",
      "loss 5.720526695251465\n",
      "steps done 380470\n",
      "494 episodes completed\n",
      "total reward 1211\n",
      "loss 4.003952503204346\n",
      "steps done 381270\n",
      "495 episodes completed\n",
      "total reward 986\n",
      "loss 4.3039164543151855\n",
      "steps done 381990\n",
      "496 episodes completed\n",
      "total reward 1223\n",
      "loss 5.334167957305908\n",
      "steps done 382870\n",
      "497 episodes completed\n",
      "total reward 1584\n",
      "loss 4.143963813781738\n",
      "steps done 383660\n",
      "498 episodes completed\n",
      "total reward 1990\n",
      "loss 3.861037254333496\n",
      "steps done 384460\n",
      "499 episodes completed\n",
      "total reward 1890\n",
      "loss 4.410979747772217\n",
      "steps done 385100\n",
      "500 episodes completed\n",
      "total reward 1936\n",
      "loss 4.033791542053223\n",
      "steps done 385810\n",
      "501 episodes completed\n",
      "total reward 1507\n",
      "loss 3.8029041290283203\n",
      "steps done 386550\n",
      "502 episodes completed\n",
      "total reward 1247\n",
      "loss 4.238052845001221\n",
      "steps done 387240\n",
      "503 episodes completed\n",
      "total reward 1320\n",
      "loss 3.789520502090454\n",
      "steps done 387920\n",
      "504 episodes completed\n",
      "total reward 1707\n",
      "loss 3.849174737930298\n",
      "steps done 388830\n",
      "505 episodes completed\n",
      "total reward 1971\n",
      "loss 4.463541030883789\n",
      "steps done 389750\n",
      "update target\n",
      "506 episodes completed\n",
      "total reward 2212\n",
      "loss 4.363386631011963\n",
      "steps done 390650\n",
      "507 episodes completed\n",
      "total reward 0\n",
      "loss 4.947521209716797\n",
      "steps done 391280\n",
      "508 episodes completed\n",
      "total reward 1785\n",
      "loss 2.7106151580810547\n",
      "steps done 391990\n",
      "509 episodes completed\n",
      "total reward 1257\n",
      "loss 4.5668792724609375\n",
      "steps done 392640\n",
      "510 episodes completed\n",
      "total reward 1242\n",
      "loss 3.535696506500244\n",
      "steps done 393260\n",
      "511 episodes completed\n",
      "total reward 1580\n",
      "loss 4.594539165496826\n",
      "steps done 393860\n",
      "512 episodes completed\n",
      "total reward 260\n",
      "loss 4.297021865844727\n",
      "steps done 394440\n",
      "513 episodes completed\n",
      "total reward 1169\n",
      "loss 4.423904895782471\n",
      "steps done 395040\n",
      "514 episodes completed\n",
      "total reward 597\n",
      "loss 3.8264007568359375\n",
      "steps done 395690\n",
      "515 episodes completed\n",
      "total reward 1668\n",
      "loss 5.5665388107299805\n",
      "steps done 396530\n",
      "516 episodes completed\n",
      "total reward 1033\n",
      "loss 3.354206085205078\n",
      "steps done 397120\n",
      "517 episodes completed\n",
      "total reward 1480\n",
      "loss 5.781126976013184\n",
      "steps done 397860\n",
      "518 episodes completed\n",
      "total reward 1870\n",
      "loss 6.023324966430664\n",
      "steps done 398710\n",
      "519 episodes completed\n",
      "total reward 687\n",
      "loss 5.6681108474731445\n",
      "steps done 399420\n",
      "update target\n",
      "520 episodes completed\n",
      "total reward 1516\n",
      "loss 5.627863883972168\n",
      "steps done 400080\n",
      "521 episodes completed\n",
      "total reward 888\n",
      "loss 4.172817707061768\n",
      "steps done 400760\n",
      "522 episodes completed\n",
      "total reward 1458\n",
      "loss 4.737661361694336\n",
      "steps done 401510\n",
      "523 episodes completed\n",
      "total reward 1093\n",
      "loss 5.813457012176514\n",
      "steps done 402120\n",
      "524 episodes completed\n",
      "total reward 1031\n",
      "loss 4.834155082702637\n",
      "steps done 402730\n",
      "525 episodes completed\n",
      "total reward 1444\n",
      "loss 4.21075963973999\n",
      "steps done 403450\n",
      "526 episodes completed\n",
      "total reward 1844\n",
      "loss 4.596188545227051\n",
      "steps done 404210\n",
      "527 episodes completed\n",
      "total reward 1631\n",
      "loss 5.45797872543335\n",
      "steps done 404760\n",
      "528 episodes completed\n",
      "total reward 1825\n",
      "loss 3.7218213081359863\n",
      "steps done 405410\n",
      "529 episodes completed\n",
      "total reward 2475\n",
      "loss 4.947666168212891\n",
      "steps done 406130\n",
      "530 episodes completed\n",
      "total reward 722\n",
      "loss 5.484651565551758\n",
      "steps done 406900\n",
      "531 episodes completed\n",
      "total reward 1544\n",
      "loss 3.6386353969573975\n",
      "steps done 407560\n",
      "532 episodes completed\n",
      "total reward 2160\n",
      "loss 3.6376097202301025\n",
      "steps done 408370\n",
      "533 episodes completed\n",
      "total reward 1187\n",
      "loss 3.677525758743286\n",
      "steps done 409460\n",
      "update target\n",
      "534 episodes completed\n",
      "total reward 1281\n",
      "loss 3.587127447128296\n",
      "steps done 410040\n",
      "535 episodes completed\n",
      "total reward 951\n",
      "loss 2.834644317626953\n",
      "steps done 410880\n",
      "536 episodes completed\n",
      "total reward 0\n",
      "loss 6.245916366577148\n",
      "steps done 411520\n",
      "537 episodes completed\n",
      "total reward 739\n",
      "loss 5.678101062774658\n",
      "steps done 412420\n",
      "538 episodes completed\n",
      "total reward 2066\n",
      "loss 5.875465393066406\n",
      "steps done 413240\n",
      "539 episodes completed\n",
      "total reward 1085\n",
      "loss 6.522477149963379\n",
      "steps done 413930\n",
      "540 episodes completed\n",
      "total reward 1772\n",
      "loss 4.8771772384643555\n",
      "steps done 414510\n",
      "541 episodes completed\n",
      "total reward 1086\n",
      "loss 5.2201056480407715\n",
      "steps done 415200\n",
      "542 episodes completed\n",
      "total reward 1829\n",
      "loss 4.24910306930542\n",
      "steps done 416190\n",
      "543 episodes completed\n",
      "total reward 2228\n",
      "loss 3.671800136566162\n",
      "steps done 416980\n",
      "544 episodes completed\n",
      "total reward 1387\n",
      "loss 4.887017250061035\n",
      "steps done 417580\n",
      "545 episodes completed\n",
      "total reward 1332\n",
      "loss 6.9139299392700195\n",
      "steps done 418170\n",
      "546 episodes completed\n",
      "total reward 1635\n",
      "loss 4.515586853027344\n",
      "steps done 418910\n",
      "547 episodes completed\n",
      "total reward 1537\n",
      "loss 5.216526985168457\n",
      "steps done 419520\n",
      "update target\n",
      "548 episodes completed\n",
      "total reward 693\n",
      "loss 5.295742034912109\n",
      "steps done 420260\n",
      "549 episodes completed\n",
      "total reward 1781\n",
      "loss 6.088706970214844\n",
      "steps done 420970\n",
      "550 episodes completed\n",
      "total reward 1627\n",
      "loss 5.97955322265625\n",
      "steps done 421720\n",
      "551 episodes completed\n",
      "total reward 1652\n",
      "loss 5.780515193939209\n",
      "steps done 422330\n",
      "552 episodes completed\n",
      "total reward 2334\n",
      "loss 6.1792731285095215\n",
      "steps done 423160\n",
      "553 episodes completed\n",
      "total reward 1\n",
      "loss 4.3638153076171875\n",
      "steps done 424210\n",
      "554 episodes completed\n",
      "total reward 1305\n",
      "loss 5.99154806137085\n",
      "steps done 425140\n",
      "555 episodes completed\n",
      "total reward 1390\n",
      "loss 4.170217037200928\n",
      "steps done 425770\n",
      "556 episodes completed\n",
      "total reward 1836\n",
      "loss 5.205204010009766\n",
      "steps done 426760\n",
      "557 episodes completed\n",
      "total reward 2055\n",
      "loss 4.7758259773254395\n",
      "steps done 427370\n",
      "558 episodes completed\n",
      "total reward 1839\n",
      "loss 5.035954475402832\n",
      "steps done 427930\n",
      "559 episodes completed\n",
      "total reward 897\n",
      "loss 4.347236633300781\n",
      "steps done 428470\n",
      "560 episodes completed\n",
      "total reward 1498\n",
      "loss 4.54196310043335\n",
      "steps done 429330\n",
      "update target\n",
      "561 episodes completed\n",
      "total reward 2685\n",
      "loss 5.378484725952148\n",
      "steps done 430550\n",
      "562 episodes completed\n",
      "total reward 1045\n",
      "loss 3.4931867122650146\n",
      "steps done 431320\n",
      "563 episodes completed\n",
      "total reward 1380\n",
      "loss 5.5726189613342285\n",
      "steps done 431990\n",
      "564 episodes completed\n",
      "total reward 2281\n",
      "loss 4.995153427124023\n",
      "steps done 432860\n",
      "565 episodes completed\n",
      "total reward 2109\n",
      "loss 5.531920909881592\n",
      "steps done 433680\n",
      "566 episodes completed\n",
      "total reward 2243\n",
      "loss 6.415962219238281\n",
      "steps done 434470\n",
      "567 episodes completed\n",
      "total reward 2186\n",
      "loss 5.888733863830566\n",
      "steps done 435450\n",
      "568 episodes completed\n",
      "total reward 2270\n",
      "loss 5.873093605041504\n",
      "steps done 436450\n",
      "569 episodes completed\n",
      "total reward 1623\n",
      "loss 4.38466739654541\n",
      "steps done 437230\n",
      "570 episodes completed\n",
      "total reward 3104\n",
      "loss 5.218996047973633\n",
      "steps done 438340\n",
      "571 episodes completed\n",
      "total reward 1697\n",
      "loss 5.492294788360596\n",
      "steps done 439020\n",
      "572 episodes completed\n",
      "total reward 1341\n",
      "loss 4.964765548706055\n",
      "steps done 439690\n",
      "update target\n",
      "573 episodes completed\n",
      "total reward 1218\n",
      "loss 6.104226112365723\n",
      "steps done 440140\n",
      "574 episodes completed\n",
      "total reward 1014\n",
      "loss 3.9940242767333984\n",
      "steps done 440920\n",
      "575 episodes completed\n",
      "total reward 2666\n",
      "loss 6.306802749633789\n",
      "steps done 442100\n",
      "576 episodes completed\n",
      "total reward 1540\n",
      "loss 6.28072452545166\n",
      "steps done 442780\n",
      "577 episodes completed\n",
      "total reward 1829\n",
      "loss 6.2176947593688965\n",
      "steps done 443490\n",
      "578 episodes completed\n",
      "total reward 2603\n",
      "loss 4.851255893707275\n",
      "steps done 444690\n",
      "579 episodes completed\n",
      "total reward 1618\n",
      "loss 5.9675798416137695\n",
      "steps done 445310\n",
      "580 episodes completed\n",
      "total reward 1571\n",
      "loss 6.216449737548828\n",
      "steps done 446020\n",
      "581 episodes completed\n",
      "total reward 1496\n",
      "loss 4.294366359710693\n",
      "steps done 446720\n",
      "582 episodes completed\n",
      "total reward 1126\n",
      "loss 4.94598913192749\n",
      "steps done 447210\n",
      "583 episodes completed\n",
      "total reward 1548\n",
      "loss 6.4156646728515625\n",
      "steps done 447830\n",
      "584 episodes completed\n",
      "total reward 1385\n",
      "loss 5.901510238647461\n",
      "steps done 448400\n",
      "585 episodes completed\n",
      "total reward 2330\n",
      "loss 4.491917133331299\n",
      "steps done 449270\n",
      "586 episodes completed\n",
      "total reward 810\n",
      "loss 4.267498970031738\n",
      "steps done 449680\n",
      "update target\n",
      "587 episodes completed\n",
      "total reward 1168\n",
      "loss 6.262923717498779\n",
      "steps done 450320\n",
      "588 episodes completed\n",
      "total reward 1179\n",
      "loss 7.34474515914917\n",
      "steps done 450650\n",
      "589 episodes completed\n",
      "total reward 1822\n",
      "loss 5.499155044555664\n",
      "steps done 451220\n",
      "590 episodes completed\n",
      "total reward 1937\n",
      "loss 5.750605583190918\n",
      "steps done 451950\n",
      "591 episodes completed\n",
      "total reward 1247\n",
      "loss 7.370182514190674\n",
      "steps done 452550\n",
      "592 episodes completed\n",
      "total reward 2014\n",
      "loss 5.911352634429932\n",
      "steps done 453290\n",
      "593 episodes completed\n",
      "total reward 1419\n",
      "loss 4.176421642303467\n",
      "steps done 454070\n",
      "594 episodes completed\n",
      "total reward 1541\n",
      "loss 5.967636585235596\n",
      "steps done 454920\n",
      "595 episodes completed\n",
      "total reward 2371\n",
      "loss 4.356048583984375\n",
      "steps done 456030\n",
      "596 episodes completed\n",
      "total reward 1851\n",
      "loss 4.955676555633545\n",
      "steps done 456760\n",
      "597 episodes completed\n",
      "total reward 1891\n",
      "loss 5.076032638549805\n",
      "steps done 457620\n",
      "598 episodes completed\n",
      "total reward 1221\n",
      "loss 4.561694145202637\n",
      "steps done 458360\n",
      "599 episodes completed\n",
      "total reward 1751\n",
      "loss 6.328641891479492\n",
      "steps done 459070\n",
      "600 episodes completed\n",
      "total reward 2216\n",
      "loss 6.0250959396362305\n",
      "steps done 459910\n",
      "update target\n",
      "601 episodes completed\n",
      "total reward 1867\n",
      "loss 7.877633094787598\n",
      "steps done 460760\n",
      "602 episodes completed\n",
      "total reward 1748\n",
      "loss 7.408472061157227\n",
      "steps done 461400\n",
      "603 episodes completed\n",
      "total reward 2518\n",
      "loss 7.389016151428223\n",
      "steps done 462510\n",
      "604 episodes completed\n",
      "total reward 1125\n",
      "loss 7.658205032348633\n",
      "steps done 463120\n",
      "605 episodes completed\n",
      "total reward 1821\n",
      "loss 7.497445106506348\n",
      "steps done 463820\n",
      "606 episodes completed\n",
      "total reward 1802\n",
      "loss 8.281852722167969\n",
      "steps done 464470\n",
      "607 episodes completed\n",
      "total reward 2763\n",
      "loss 7.019430160522461\n",
      "steps done 465480\n",
      "608 episodes completed\n",
      "total reward 1987\n",
      "loss 7.776425838470459\n",
      "steps done 466070\n",
      "609 episodes completed\n",
      "total reward 2032\n",
      "loss 6.086429119110107\n",
      "steps done 466670\n",
      "610 episodes completed\n",
      "total reward 2720\n",
      "loss 7.48707914352417\n",
      "steps done 467860\n",
      "611 episodes completed\n",
      "total reward 691\n",
      "loss 8.933603286743164\n",
      "steps done 468450\n",
      "612 episodes completed\n",
      "total reward 2005\n",
      "loss 6.087405204772949\n",
      "steps done 469150\n",
      "613 episodes completed\n",
      "total reward 754\n",
      "loss 6.301275730133057\n",
      "steps done 469910\n",
      "update target\n",
      "614 episodes completed\n",
      "total reward 1215\n",
      "loss 7.293931007385254\n",
      "steps done 470440\n",
      "615 episodes completed\n",
      "total reward 738\n",
      "loss 9.61155891418457\n",
      "steps done 471100\n",
      "616 episodes completed\n",
      "total reward 1421\n",
      "loss 9.69674015045166\n",
      "steps done 471670\n",
      "617 episodes completed\n",
      "total reward 743\n",
      "loss 7.6479034423828125\n",
      "steps done 472450\n",
      "618 episodes completed\n",
      "total reward 1416\n",
      "loss 7.4035773277282715\n",
      "steps done 473090\n",
      "619 episodes completed\n",
      "total reward 207\n",
      "loss 7.527774333953857\n",
      "steps done 473640\n",
      "620 episodes completed\n",
      "total reward 1149\n",
      "loss 7.0567779541015625\n",
      "steps done 474280\n",
      "621 episodes completed\n",
      "total reward 2309\n",
      "loss 7.817174911499023\n",
      "steps done 475460\n",
      "622 episodes completed\n",
      "total reward 1927\n",
      "loss 7.163468360900879\n",
      "steps done 476130\n",
      "623 episodes completed\n",
      "total reward 1405\n",
      "loss 5.226944446563721\n",
      "steps done 476770\n",
      "624 episodes completed\n",
      "total reward 2215\n",
      "loss 5.583097457885742\n",
      "steps done 477790\n",
      "625 episodes completed\n",
      "total reward 2226\n",
      "loss 6.559678554534912\n",
      "steps done 479060\n",
      "626 episodes completed\n",
      "total reward 2126\n",
      "loss 8.008021354675293\n",
      "steps done 479980\n",
      "update target\n",
      "627 episodes completed\n",
      "total reward 1428\n",
      "loss 5.544777870178223\n",
      "steps done 480780\n",
      "628 episodes completed\n",
      "total reward 1244\n",
      "loss 7.050539970397949\n",
      "steps done 481360\n",
      "629 episodes completed\n",
      "total reward 1027\n",
      "loss 7.0890889167785645\n",
      "steps done 482010\n",
      "630 episodes completed\n",
      "total reward 1968\n",
      "loss 9.329421043395996\n",
      "steps done 482620\n",
      "631 episodes completed\n",
      "total reward 1380\n",
      "loss 7.312236785888672\n",
      "steps done 483230\n",
      "632 episodes completed\n",
      "total reward 96\n",
      "loss 7.158041000366211\n",
      "steps done 484000\n",
      "633 episodes completed\n",
      "total reward 1658\n",
      "loss 7.630592346191406\n",
      "steps done 484680\n",
      "634 episodes completed\n",
      "total reward 2034\n",
      "loss 7.727272033691406\n",
      "steps done 485400\n",
      "635 episodes completed\n",
      "total reward 1958\n",
      "loss 6.218506336212158\n",
      "steps done 486020\n",
      "636 episodes completed\n",
      "total reward 2080\n",
      "loss 8.870294570922852\n",
      "steps done 486660\n",
      "637 episodes completed\n",
      "total reward 1397\n",
      "loss 5.396139621734619\n",
      "steps done 487340\n",
      "638 episodes completed\n",
      "total reward 2210\n",
      "loss 9.180648803710938\n",
      "steps done 488180\n",
      "639 episodes completed\n",
      "total reward 1723\n",
      "loss 6.443037033081055\n",
      "steps done 488830\n",
      "640 episodes completed\n",
      "total reward 2096\n",
      "loss 6.54448938369751\n",
      "steps done 489540\n",
      "update target\n",
      "641 episodes completed\n",
      "total reward 2181\n",
      "loss 8.80742073059082\n",
      "steps done 490730\n",
      "642 episodes completed\n",
      "total reward 513\n",
      "loss 8.447378158569336\n",
      "steps done 491420\n",
      "643 episodes completed\n",
      "total reward 2331\n",
      "loss 7.242926597595215\n",
      "steps done 492370\n",
      "644 episodes completed\n",
      "total reward 2208\n",
      "loss 6.750412464141846\n",
      "steps done 493320\n",
      "645 episodes completed\n",
      "total reward 2566\n",
      "loss 6.0820207595825195\n",
      "steps done 494090\n",
      "646 episodes completed\n",
      "total reward 1763\n",
      "loss 8.269838333129883\n",
      "steps done 494840\n",
      "647 episodes completed\n",
      "total reward 803\n",
      "loss 6.606900215148926\n",
      "steps done 495390\n",
      "648 episodes completed\n",
      "total reward 2460\n",
      "loss 6.632558822631836\n",
      "steps done 496180\n",
      "649 episodes completed\n",
      "total reward 0\n",
      "loss 7.9462890625\n",
      "steps done 496930\n",
      "650 episodes completed\n",
      "total reward 819\n",
      "loss 5.330585956573486\n",
      "steps done 497660\n",
      "651 episodes completed\n",
      "total reward 2037\n",
      "loss 8.286687850952148\n",
      "steps done 498300\n",
      "652 episodes completed\n",
      "total reward 1618\n",
      "loss 7.612852096557617\n",
      "steps done 498890\n",
      "653 episodes completed\n",
      "total reward 1753\n",
      "loss 7.394792556762695\n",
      "steps done 499620\n",
      "update target\n",
      "654 episodes completed\n",
      "total reward 1773\n",
      "loss 8.06047534942627\n",
      "steps done 500380\n",
      "655 episodes completed\n",
      "total reward 1218\n",
      "loss 7.312807083129883\n",
      "steps done 500920\n",
      "656 episodes completed\n",
      "total reward 1366\n",
      "loss 5.568106651306152\n",
      "steps done 501470\n",
      "657 episodes completed\n",
      "total reward 1935\n",
      "loss 5.361413955688477\n",
      "steps done 502120\n",
      "658 episodes completed\n",
      "total reward 756\n",
      "loss 6.7436323165893555\n",
      "steps done 502680\n",
      "659 episodes completed\n",
      "total reward 1852\n",
      "loss 6.411683082580566\n",
      "steps done 503360\n",
      "660 episodes completed\n",
      "total reward 1721\n",
      "loss 4.984119892120361\n",
      "steps done 503880\n",
      "661 episodes completed\n",
      "total reward 1538\n",
      "loss 8.101410865783691\n",
      "steps done 504470\n",
      "662 episodes completed\n",
      "total reward 1469\n",
      "loss 8.527364730834961\n",
      "steps done 505390\n",
      "663 episodes completed\n",
      "total reward 1629\n",
      "loss 7.602451801300049\n",
      "steps done 506070\n",
      "664 episodes completed\n",
      "total reward 1651\n",
      "loss 6.975277423858643\n",
      "steps done 506670\n",
      "665 episodes completed\n",
      "total reward 2416\n",
      "loss 6.889214515686035\n",
      "steps done 507520\n",
      "666 episodes completed\n",
      "total reward 2057\n",
      "loss 7.75160551071167\n",
      "steps done 508230\n",
      "667 episodes completed\n",
      "total reward 2402\n",
      "loss 7.222173690795898\n",
      "steps done 509100\n",
      "668 episodes completed\n",
      "total reward 2346\n",
      "loss 6.174326419830322\n",
      "steps done 509830\n",
      "update target\n",
      "669 episodes completed\n",
      "total reward 1872\n",
      "loss 5.658843994140625\n",
      "steps done 510440\n",
      "670 episodes completed\n",
      "total reward 1612\n",
      "loss 7.65716028213501\n",
      "steps done 511300\n",
      "671 episodes completed\n",
      "total reward 2600\n",
      "loss 7.100702285766602\n",
      "steps done 512560\n",
      "672 episodes completed\n",
      "total reward 1359\n",
      "loss 5.791858673095703\n",
      "steps done 513270\n",
      "673 episodes completed\n",
      "total reward 1395\n",
      "loss 8.488306045532227\n",
      "steps done 514010\n",
      "674 episodes completed\n",
      "total reward 1359\n",
      "loss 6.832588195800781\n",
      "steps done 514680\n",
      "675 episodes completed\n",
      "total reward 1640\n",
      "loss 6.344392776489258\n",
      "steps done 515220\n",
      "676 episodes completed\n",
      "total reward 2318\n",
      "loss 5.40446662902832\n",
      "steps done 515890\n",
      "677 episodes completed\n",
      "total reward 1435\n",
      "loss 7.597567081451416\n",
      "steps done 516630\n",
      "678 episodes completed\n",
      "total reward 1843\n",
      "loss 6.5531392097473145\n",
      "steps done 517390\n",
      "679 episodes completed\n",
      "total reward 2285\n",
      "loss 5.850450038909912\n",
      "steps done 518250\n",
      "680 episodes completed\n",
      "total reward 2336\n",
      "loss 6.707762718200684\n",
      "steps done 519000\n",
      "681 episodes completed\n",
      "total reward 814\n",
      "loss 6.173094749450684\n",
      "steps done 519510\n",
      "update target\n",
      "682 episodes completed\n",
      "total reward 2165\n",
      "loss 6.338799476623535\n",
      "steps done 520280\n",
      "683 episodes completed\n",
      "total reward 1396\n",
      "loss 5.685503005981445\n",
      "steps done 521120\n",
      "684 episodes completed\n",
      "total reward 1883\n",
      "loss 5.224287033081055\n",
      "steps done 521780\n",
      "685 episodes completed\n",
      "total reward 1932\n",
      "loss 6.69059419631958\n",
      "steps done 522560\n",
      "686 episodes completed\n",
      "total reward 854\n",
      "loss 7.278128147125244\n",
      "steps done 523360\n",
      "687 episodes completed\n",
      "total reward 2029\n",
      "loss 6.844524383544922\n",
      "steps done 524290\n",
      "688 episodes completed\n",
      "total reward 0\n",
      "loss 5.747036933898926\n",
      "steps done 524940\n",
      "689 episodes completed\n",
      "total reward 2707\n",
      "loss 6.57181453704834\n",
      "steps done 526180\n",
      "690 episodes completed\n",
      "total reward 2133\n",
      "loss 7.013578414916992\n",
      "steps done 526920\n",
      "691 episodes completed\n",
      "total reward 1241\n",
      "loss 7.308806419372559\n",
      "steps done 527780\n",
      "692 episodes completed\n",
      "total reward 474\n",
      "loss 9.187429428100586\n",
      "steps done 528400\n",
      "693 episodes completed\n",
      "total reward 1815\n",
      "loss 7.037923812866211\n",
      "steps done 529480\n",
      "update target\n",
      "694 episodes completed\n",
      "total reward 863\n",
      "loss 9.154284477233887\n",
      "steps done 530280\n",
      "695 episodes completed\n",
      "total reward 1424\n",
      "loss 7.555419921875\n",
      "steps done 531190\n",
      "696 episodes completed\n",
      "total reward 1421\n",
      "loss 6.423340797424316\n",
      "steps done 531900\n",
      "697 episodes completed\n",
      "total reward 1689\n",
      "loss 5.857882499694824\n",
      "steps done 532640\n",
      "698 episodes completed\n",
      "total reward 2146\n",
      "loss 5.452338218688965\n",
      "steps done 533360\n",
      "699 episodes completed\n",
      "total reward 2030\n",
      "loss 7.315819263458252\n",
      "steps done 534010\n",
      "700 episodes completed\n",
      "total reward 2212\n",
      "loss 6.674072265625\n",
      "steps done 534950\n",
      "701 episodes completed\n",
      "total reward 604\n",
      "loss 5.060723781585693\n",
      "steps done 535530\n",
      "702 episodes completed\n",
      "total reward 1229\n",
      "loss 7.733452320098877\n",
      "steps done 536120\n",
      "703 episodes completed\n",
      "total reward 1883\n",
      "loss 4.973697185516357\n",
      "steps done 537020\n",
      "704 episodes completed\n",
      "total reward 1548\n",
      "loss 7.2433624267578125\n",
      "steps done 537700\n",
      "705 episodes completed\n",
      "total reward 2303\n",
      "loss 4.807900428771973\n",
      "steps done 538470\n",
      "706 episodes completed\n",
      "total reward 1882\n",
      "loss 6.536194801330566\n",
      "steps done 539380\n",
      "update target\n",
      "707 episodes completed\n",
      "total reward 1604\n",
      "loss 6.57907772064209\n",
      "steps done 540040\n",
      "708 episodes completed\n",
      "total reward 1041\n",
      "loss 7.594337463378906\n",
      "steps done 540720\n",
      "709 episodes completed\n",
      "total reward 1790\n",
      "loss 4.805130481719971\n",
      "steps done 541930\n",
      "710 episodes completed\n",
      "total reward 1130\n",
      "loss 10.794059753417969\n",
      "steps done 542470\n",
      "711 episodes completed\n",
      "total reward 720\n",
      "loss 7.200270652770996\n",
      "steps done 543230\n",
      "712 episodes completed\n",
      "total reward 878\n",
      "loss 4.535318374633789\n",
      "steps done 544130\n",
      "713 episodes completed\n",
      "total reward 788\n",
      "loss 6.537713050842285\n",
      "steps done 544770\n",
      "714 episodes completed\n",
      "total reward 831\n",
      "loss 6.786314964294434\n",
      "steps done 545270\n",
      "715 episodes completed\n",
      "total reward 692\n",
      "loss 3.9816346168518066\n",
      "steps done 545990\n",
      "716 episodes completed\n",
      "total reward 697\n",
      "loss 4.520417213439941\n",
      "steps done 547010\n",
      "717 episodes completed\n",
      "total reward 1212\n",
      "loss 5.6811747550964355\n",
      "steps done 547580\n",
      "718 episodes completed\n",
      "total reward 855\n",
      "loss 5.922630310058594\n",
      "steps done 548220\n",
      "719 episodes completed\n",
      "total reward 1271\n",
      "loss 7.022173881530762\n",
      "steps done 549220\n",
      "720 episodes completed\n",
      "total reward 1458\n",
      "loss 6.907259941101074\n",
      "steps done 549810\n",
      "update target\n",
      "721 episodes completed\n",
      "total reward 1922\n",
      "loss 5.351276397705078\n",
      "steps done 550710\n",
      "722 episodes completed\n",
      "total reward 2106\n",
      "loss 6.5386505126953125\n",
      "steps done 551550\n",
      "723 episodes completed\n",
      "total reward 807\n",
      "loss 6.327475070953369\n",
      "steps done 552110\n",
      "724 episodes completed\n",
      "total reward 1741\n",
      "loss 5.137973785400391\n",
      "steps done 552750\n",
      "725 episodes completed\n",
      "total reward 1786\n",
      "loss 5.330811023712158\n",
      "steps done 553480\n",
      "726 episodes completed\n",
      "total reward 1014\n",
      "loss 5.926170349121094\n",
      "steps done 554150\n",
      "727 episodes completed\n",
      "total reward 1067\n",
      "loss 5.718585014343262\n",
      "steps done 554820\n",
      "728 episodes completed\n",
      "total reward 2146\n",
      "loss 6.847133159637451\n",
      "steps done 555670\n",
      "729 episodes completed\n",
      "total reward 2180\n",
      "loss 5.504002094268799\n",
      "steps done 556390\n",
      "730 episodes completed\n",
      "total reward 1692\n",
      "loss 6.744576454162598\n",
      "steps done 557050\n",
      "731 episodes completed\n",
      "total reward 2825\n",
      "loss 5.536444187164307\n",
      "steps done 558220\n",
      "732 episodes completed\n",
      "total reward 1291\n",
      "loss 5.987297534942627\n",
      "steps done 559080\n",
      "733 episodes completed\n",
      "total reward 1961\n",
      "loss 4.95464563369751\n",
      "steps done 559810\n",
      "update target\n",
      "734 episodes completed\n",
      "total reward 1855\n",
      "loss 7.197646617889404\n",
      "steps done 560410\n",
      "735 episodes completed\n",
      "total reward 2467\n",
      "loss 7.244945526123047\n",
      "steps done 561540\n",
      "736 episodes completed\n",
      "total reward 1583\n",
      "loss 7.265102863311768\n",
      "steps done 562150\n",
      "737 episodes completed\n",
      "total reward 2204\n",
      "loss 4.997842311859131\n",
      "steps done 563280\n",
      "738 episodes completed\n",
      "total reward 1789\n",
      "loss 4.221310615539551\n",
      "steps done 563940\n",
      "739 episodes completed\n",
      "total reward 543\n",
      "loss 4.500373840332031\n",
      "steps done 564510\n",
      "740 episodes completed\n",
      "total reward 1961\n",
      "loss 6.990694999694824\n",
      "steps done 565190\n",
      "741 episodes completed\n",
      "total reward 1768\n",
      "loss 7.286487579345703\n",
      "steps done 565880\n",
      "742 episodes completed\n",
      "total reward 2179\n",
      "loss 6.443955898284912\n",
      "steps done 566970\n",
      "743 episodes completed\n",
      "total reward 3153\n",
      "loss 6.190184593200684\n",
      "steps done 568090\n",
      "744 episodes completed\n",
      "total reward 2639\n",
      "loss 5.888613700866699\n",
      "steps done 569260\n",
      "update target\n",
      "745 episodes completed\n",
      "total reward 35\n",
      "loss 3.9644885063171387\n",
      "steps done 570570\n",
      "746 episodes completed\n",
      "total reward 1872\n",
      "loss 6.634404182434082\n",
      "steps done 571200\n",
      "747 episodes completed\n",
      "total reward 1689\n",
      "loss 5.435136318206787\n",
      "steps done 571870\n",
      "748 episodes completed\n",
      "total reward 2222\n",
      "loss 4.904331207275391\n",
      "steps done 572580\n",
      "749 episodes completed\n",
      "total reward 1912\n",
      "loss 5.764866828918457\n",
      "steps done 573260\n",
      "750 episodes completed\n",
      "total reward 2182\n",
      "loss 7.074078559875488\n",
      "steps done 573970\n",
      "751 episodes completed\n",
      "total reward 2021\n",
      "loss 6.979302406311035\n",
      "steps done 574700\n",
      "752 episodes completed\n",
      "total reward 1837\n",
      "loss 3.92177677154541\n",
      "steps done 575520\n",
      "753 episodes completed\n",
      "total reward 1608\n",
      "loss 4.4526472091674805\n",
      "steps done 576090\n",
      "754 episodes completed\n",
      "total reward 2243\n",
      "loss 7.276649475097656\n",
      "steps done 576940\n",
      "755 episodes completed\n",
      "total reward 657\n",
      "loss 6.663784980773926\n",
      "steps done 577570\n",
      "756 episodes completed\n",
      "total reward 2116\n",
      "loss 4.9766435623168945\n",
      "steps done 578270\n",
      "757 episodes completed\n",
      "total reward 1788\n",
      "loss 5.64115047454834\n",
      "steps done 579200\n",
      "update target\n",
      "758 episodes completed\n",
      "total reward 2559\n",
      "loss 8.74277114868164\n",
      "steps done 580140\n",
      "759 episodes completed\n",
      "total reward 1731\n",
      "loss 6.64283561706543\n",
      "steps done 580780\n",
      "760 episodes completed\n",
      "total reward 1495\n",
      "loss 5.027830600738525\n",
      "steps done 581410\n",
      "761 episodes completed\n",
      "total reward 2514\n",
      "loss 7.359846115112305\n",
      "steps done 582250\n",
      "762 episodes completed\n",
      "total reward 2134\n",
      "loss 5.762426853179932\n",
      "steps done 583150\n",
      "763 episodes completed\n",
      "total reward 1451\n",
      "loss 6.092743873596191\n",
      "steps done 583710\n",
      "764 episodes completed\n",
      "total reward 1600\n",
      "loss 5.486441135406494\n",
      "steps done 584330\n",
      "765 episodes completed\n",
      "total reward 1939\n",
      "loss 8.863929748535156\n",
      "steps done 584990\n",
      "766 episodes completed\n",
      "total reward 1761\n",
      "loss 5.392722129821777\n",
      "steps done 585590\n",
      "767 episodes completed\n",
      "total reward 171\n",
      "loss 6.830012321472168\n",
      "steps done 586280\n",
      "768 episodes completed\n",
      "total reward 1695\n",
      "loss 5.135079383850098\n",
      "steps done 586850\n",
      "769 episodes completed\n",
      "total reward 1585\n",
      "loss 7.121536731719971\n",
      "steps done 587350\n",
      "770 episodes completed\n",
      "total reward 1288\n",
      "loss 7.685048580169678\n",
      "steps done 588150\n",
      "771 episodes completed\n",
      "total reward 2231\n",
      "loss 6.578753471374512\n",
      "steps done 589100\n",
      "772 episodes completed\n",
      "total reward 2198\n",
      "loss 5.363323211669922\n",
      "steps done 589830\n",
      "update target\n",
      "773 episodes completed\n",
      "total reward 731\n",
      "loss 3.9684977531433105\n",
      "steps done 590690\n",
      "774 episodes completed\n",
      "total reward 1520\n",
      "loss 7.938449859619141\n",
      "steps done 591200\n",
      "775 episodes completed\n",
      "total reward 1678\n",
      "loss 6.723101615905762\n",
      "steps done 591800\n",
      "776 episodes completed\n",
      "total reward 307\n",
      "loss 6.885197162628174\n",
      "steps done 592570\n",
      "777 episodes completed\n",
      "total reward 14\n",
      "loss 4.881011962890625\n",
      "steps done 593270\n",
      "778 episodes completed\n",
      "total reward 2189\n",
      "loss 6.261801242828369\n",
      "steps done 594120\n",
      "779 episodes completed\n",
      "total reward 1360\n",
      "loss 6.029827117919922\n",
      "steps done 594910\n",
      "780 episodes completed\n",
      "total reward 1099\n",
      "loss 5.608086585998535\n",
      "steps done 595510\n",
      "781 episodes completed\n",
      "total reward 2604\n",
      "loss 3.90919828414917\n",
      "steps done 596500\n",
      "782 episodes completed\n",
      "total reward 1494\n",
      "loss 6.388179779052734\n",
      "steps done 597780\n",
      "783 episodes completed\n",
      "total reward 2274\n",
      "loss 6.422088623046875\n",
      "steps done 598740\n",
      "784 episodes completed\n",
      "total reward 1030\n",
      "loss 5.995449066162109\n",
      "steps done 599240\n",
      "785 episodes completed\n",
      "total reward 2053\n",
      "loss 5.539726257324219\n",
      "steps done 599990\n",
      "update target\n",
      "786 episodes completed\n",
      "total reward 1805\n",
      "loss 6.489116668701172\n",
      "steps done 600560\n",
      "787 episodes completed\n",
      "total reward 2039\n",
      "loss 7.870304107666016\n",
      "steps done 601410\n",
      "788 episodes completed\n",
      "total reward 279\n",
      "loss 8.921640396118164\n",
      "steps done 602410\n",
      "789 episodes completed\n",
      "total reward 2112\n",
      "loss 6.2573323249816895\n",
      "steps done 603210\n",
      "790 episodes completed\n",
      "total reward 1665\n",
      "loss 5.967887878417969\n",
      "steps done 603930\n",
      "791 episodes completed\n",
      "total reward 2259\n",
      "loss 5.457896709442139\n",
      "steps done 604660\n",
      "792 episodes completed\n",
      "total reward 659\n",
      "loss 6.593242645263672\n",
      "steps done 605290\n",
      "793 episodes completed\n",
      "total reward 1278\n",
      "loss 8.171529769897461\n",
      "steps done 605930\n",
      "794 episodes completed\n",
      "total reward 2385\n",
      "loss 6.267526626586914\n",
      "steps done 606730\n",
      "795 episodes completed\n",
      "total reward 2333\n",
      "loss 6.872480869293213\n",
      "steps done 607650\n",
      "796 episodes completed\n",
      "total reward 1635\n",
      "loss 5.089098930358887\n",
      "steps done 608340\n",
      "797 episodes completed\n",
      "total reward 1478\n",
      "loss 5.523892402648926\n",
      "steps done 609030\n",
      "798 episodes completed\n",
      "total reward 1767\n",
      "loss 6.945308685302734\n",
      "steps done 609770\n",
      "update target\n",
      "799 episodes completed\n",
      "total reward 1762\n",
      "loss 6.297223091125488\n",
      "steps done 610310\n",
      "800 episodes completed\n",
      "total reward 1965\n",
      "loss 6.877971649169922\n",
      "steps done 610970\n",
      "801 episodes completed\n",
      "total reward 2388\n",
      "loss 6.907772064208984\n",
      "steps done 611990\n",
      "802 episodes completed\n",
      "total reward 2070\n",
      "loss 5.50885009765625\n",
      "steps done 612790\n",
      "803 episodes completed\n",
      "total reward 1588\n",
      "loss 6.109238147735596\n",
      "steps done 613400\n",
      "804 episodes completed\n",
      "total reward 2297\n",
      "loss 9.360107421875\n",
      "steps done 614220\n",
      "805 episodes completed\n",
      "total reward 1496\n",
      "loss 6.820969581604004\n",
      "steps done 615060\n",
      "806 episodes completed\n",
      "total reward 1033\n",
      "loss 7.051991939544678\n",
      "steps done 615490\n",
      "807 episodes completed\n",
      "total reward 2212\n",
      "loss 5.161654949188232\n",
      "steps done 616390\n",
      "808 episodes completed\n",
      "total reward 3063\n",
      "loss 6.727194786071777\n",
      "steps done 617420\n",
      "809 episodes completed\n",
      "total reward 1484\n",
      "loss 7.4106903076171875\n",
      "steps done 618140\n",
      "810 episodes completed\n",
      "total reward 2628\n",
      "loss 6.1772308349609375\n",
      "steps done 619070\n",
      "811 episodes completed\n",
      "total reward 1685\n",
      "loss 7.496034622192383\n",
      "steps done 619880\n",
      "update target\n",
      "812 episodes completed\n",
      "total reward 2309\n",
      "loss 7.546180248260498\n",
      "steps done 620700\n",
      "813 episodes completed\n",
      "total reward 1797\n",
      "loss 8.20259952545166\n",
      "steps done 621450\n",
      "814 episodes completed\n",
      "total reward 2598\n",
      "loss 6.172149658203125\n",
      "steps done 622250\n",
      "815 episodes completed\n",
      "total reward 2771\n",
      "loss 6.946390151977539\n",
      "steps done 623030\n",
      "816 episodes completed\n",
      "total reward 50\n",
      "loss 7.832548141479492\n",
      "steps done 623820\n",
      "817 episodes completed\n",
      "total reward 2205\n",
      "loss 6.223407745361328\n",
      "steps done 624770\n",
      "818 episodes completed\n",
      "total reward 1586\n",
      "loss 6.003048896789551\n",
      "steps done 625470\n",
      "819 episodes completed\n",
      "total reward 1670\n",
      "loss 6.122459411621094\n",
      "steps done 626050\n",
      "820 episodes completed\n",
      "total reward 2127\n",
      "loss 5.248476982116699\n",
      "steps done 626740\n",
      "821 episodes completed\n",
      "total reward 2189\n",
      "loss 7.058437347412109\n",
      "steps done 627460\n",
      "822 episodes completed\n",
      "total reward 1490\n",
      "loss 5.875651836395264\n",
      "steps done 628090\n",
      "823 episodes completed\n",
      "total reward 1682\n",
      "loss 9.245223045349121\n",
      "steps done 628680\n",
      "824 episodes completed\n",
      "total reward 2121\n",
      "loss 4.524568557739258\n",
      "steps done 629340\n",
      "update target\n",
      "825 episodes completed\n",
      "total reward 2999\n",
      "loss 5.483485698699951\n",
      "steps done 630460\n",
      "826 episodes completed\n",
      "total reward 1620\n",
      "loss 7.599907875061035\n",
      "steps done 631420\n",
      "827 episodes completed\n",
      "total reward 2167\n",
      "loss 5.455925941467285\n",
      "steps done 632390\n",
      "828 episodes completed\n",
      "total reward 1572\n",
      "loss 5.803411483764648\n",
      "steps done 632960\n",
      "829 episodes completed\n",
      "total reward 1963\n",
      "loss 6.871157646179199\n",
      "steps done 633640\n",
      "830 episodes completed\n",
      "total reward 1645\n",
      "loss 6.294217586517334\n",
      "steps done 634280\n",
      "831 episodes completed\n",
      "total reward 2201\n",
      "loss 4.875540733337402\n",
      "steps done 634990\n",
      "832 episodes completed\n",
      "total reward 2468\n",
      "loss 4.68018913269043\n",
      "steps done 635830\n",
      "833 episodes completed\n",
      "total reward 2088\n",
      "loss 4.062254905700684\n",
      "steps done 636480\n",
      "834 episodes completed\n",
      "total reward 620\n",
      "loss 5.3013386726379395\n",
      "steps done 637250\n",
      "835 episodes completed\n",
      "total reward 2769\n",
      "loss 5.881562232971191\n",
      "steps done 638140\n",
      "836 episodes completed\n",
      "total reward 2323\n",
      "loss 6.561820983886719\n",
      "steps done 638820\n",
      "837 episodes completed\n",
      "total reward 2024\n",
      "loss 6.216610908508301\n",
      "steps done 639650\n",
      "update target\n",
      "838 episodes completed\n",
      "total reward 1960\n",
      "loss 5.445648193359375\n",
      "steps done 640330\n",
      "839 episodes completed\n",
      "total reward 1843\n",
      "loss 6.241512298583984\n",
      "steps done 641000\n",
      "840 episodes completed\n",
      "total reward 1611\n",
      "loss 5.404397964477539\n",
      "steps done 642300\n",
      "841 episodes completed\n",
      "total reward 2145\n",
      "loss 9.27402400970459\n",
      "steps done 642960\n",
      "842 episodes completed\n",
      "total reward 2514\n",
      "loss 5.0881757736206055\n",
      "steps done 643790\n",
      "843 episodes completed\n",
      "total reward 2150\n",
      "loss 5.109115123748779\n",
      "steps done 644480\n",
      "844 episodes completed\n",
      "total reward 2643\n",
      "loss 6.948128700256348\n",
      "steps done 645310\n",
      "845 episodes completed\n",
      "total reward 2634\n",
      "loss 6.827892303466797\n",
      "steps done 646180\n",
      "846 episodes completed\n",
      "total reward 2047\n",
      "loss 7.9648237228393555\n",
      "steps done 646750\n",
      "847 episodes completed\n",
      "total reward 2034\n",
      "loss 5.786661148071289\n",
      "steps done 647720\n",
      "848 episodes completed\n",
      "total reward 2171\n",
      "loss 6.728384494781494\n",
      "steps done 648640\n",
      "849 episodes completed\n",
      "total reward 2389\n",
      "loss 8.072186470031738\n",
      "steps done 649560\n",
      "update target\n",
      "850 episodes completed\n",
      "total reward 1295\n",
      "loss 7.0682783126831055\n",
      "steps done 650360\n",
      "851 episodes completed\n",
      "total reward 1532\n",
      "loss 6.99139928817749\n",
      "steps done 650910\n",
      "852 episodes completed\n",
      "total reward 1900\n",
      "loss 7.111988067626953\n",
      "steps done 651710\n",
      "853 episodes completed\n",
      "total reward 3092\n",
      "loss 6.048272609710693\n",
      "steps done 652890\n",
      "854 episodes completed\n",
      "total reward 1757\n",
      "loss 8.028815269470215\n",
      "steps done 653390\n",
      "855 episodes completed\n",
      "total reward 2022\n",
      "loss 7.374974250793457\n",
      "steps done 654060\n",
      "856 episodes completed\n",
      "total reward 2704\n",
      "loss 6.968528747558594\n",
      "steps done 655210\n",
      "857 episodes completed\n",
      "total reward 2070\n",
      "loss 7.222482681274414\n",
      "steps done 656030\n",
      "858 episodes completed\n",
      "total reward 1400\n",
      "loss 5.110512733459473\n",
      "steps done 656540\n",
      "859 episodes completed\n",
      "total reward 2404\n",
      "loss 7.044408798217773\n",
      "steps done 657480\n",
      "860 episodes completed\n",
      "total reward 1353\n",
      "loss 5.374414443969727\n",
      "steps done 658160\n",
      "861 episodes completed\n",
      "total reward 2398\n",
      "loss 5.548044204711914\n",
      "steps done 658910\n",
      "862 episodes completed\n",
      "total reward 1920\n",
      "loss 6.007084369659424\n",
      "steps done 659520\n",
      "update target\n",
      "863 episodes completed\n",
      "total reward 2482\n",
      "loss 7.26322603225708\n",
      "steps done 660400\n",
      "864 episodes completed\n",
      "total reward 1912\n",
      "loss 9.535894393920898\n",
      "steps done 661320\n",
      "865 episodes completed\n",
      "total reward 1374\n",
      "loss 6.955204486846924\n",
      "steps done 661870\n",
      "866 episodes completed\n",
      "total reward 2027\n",
      "loss 7.253157615661621\n",
      "steps done 662650\n",
      "867 episodes completed\n",
      "total reward 1990\n",
      "loss 6.013705253601074\n",
      "steps done 663420\n",
      "868 episodes completed\n",
      "total reward 1931\n",
      "loss 7.3526716232299805\n",
      "steps done 664320\n",
      "869 episodes completed\n",
      "total reward 1392\n",
      "loss 6.567912578582764\n",
      "steps done 664920\n",
      "870 episodes completed\n",
      "total reward 1928\n",
      "loss 6.799553871154785\n",
      "steps done 665550\n",
      "871 episodes completed\n",
      "total reward 1948\n",
      "loss 6.976688385009766\n",
      "steps done 666550\n",
      "872 episodes completed\n",
      "total reward 1771\n",
      "loss 9.400131225585938\n",
      "steps done 667190\n",
      "873 episodes completed\n",
      "total reward 2125\n",
      "loss 7.197243690490723\n",
      "steps done 667950\n",
      "874 episodes completed\n",
      "total reward 1883\n",
      "loss 6.825420379638672\n",
      "steps done 668530\n",
      "875 episodes completed\n",
      "total reward 2235\n",
      "loss 8.182500839233398\n",
      "steps done 669260\n",
      "update target\n",
      "876 episodes completed\n",
      "total reward 2220\n",
      "loss 6.655133247375488\n",
      "steps done 670070\n",
      "877 episodes completed\n",
      "total reward 2163\n",
      "loss 7.500242710113525\n",
      "steps done 670810\n",
      "878 episodes completed\n",
      "total reward 1972\n",
      "loss 6.142604827880859\n",
      "steps done 671490\n",
      "879 episodes completed\n",
      "total reward 2013\n",
      "loss 6.169356822967529\n",
      "steps done 672260\n",
      "880 episodes completed\n",
      "total reward 1926\n",
      "loss 7.266153335571289\n",
      "steps done 672880\n",
      "881 episodes completed\n",
      "total reward 1966\n",
      "loss 8.733016967773438\n",
      "steps done 673510\n",
      "882 episodes completed\n",
      "total reward 1669\n",
      "loss 6.011479377746582\n",
      "steps done 674040\n",
      "883 episodes completed\n",
      "total reward 2427\n",
      "loss 7.597297191619873\n",
      "steps done 674920\n",
      "884 episodes completed\n",
      "total reward 1771\n",
      "loss 6.968090534210205\n",
      "steps done 675670\n",
      "885 episodes completed\n",
      "total reward 2202\n",
      "loss 8.341583251953125\n",
      "steps done 676680\n",
      "886 episodes completed\n",
      "total reward 1927\n",
      "loss 7.4039764404296875\n",
      "steps done 677520\n",
      "887 episodes completed\n",
      "total reward 2042\n",
      "loss 5.861988544464111\n",
      "steps done 678300\n",
      "888 episodes completed\n",
      "total reward 2475\n",
      "loss 7.020998477935791\n",
      "steps done 679230\n",
      "889 episodes completed\n",
      "total reward 1593\n",
      "loss 6.541220664978027\n",
      "steps done 679810\n",
      "update target\n",
      "890 episodes completed\n",
      "total reward 2022\n",
      "loss 6.957719802856445\n",
      "steps done 680760\n",
      "891 episodes completed\n",
      "total reward 1824\n",
      "loss 6.803542137145996\n",
      "steps done 681260\n",
      "892 episodes completed\n",
      "total reward 1955\n",
      "loss 7.2178497314453125\n",
      "steps done 681910\n",
      "893 episodes completed\n",
      "total reward 1693\n",
      "loss 6.699726104736328\n",
      "steps done 683090\n",
      "894 episodes completed\n",
      "total reward 1607\n",
      "loss 7.687906265258789\n",
      "steps done 683670\n",
      "895 episodes completed\n",
      "total reward 1674\n",
      "loss 6.096915245056152\n",
      "steps done 684260\n",
      "896 episodes completed\n",
      "total reward 1137\n",
      "loss 8.032403945922852\n",
      "steps done 684830\n",
      "897 episodes completed\n",
      "total reward 2319\n",
      "loss 6.010072708129883\n",
      "steps done 685730\n",
      "898 episodes completed\n",
      "total reward 2641\n",
      "loss 4.636868476867676\n",
      "steps done 686660\n",
      "899 episodes completed\n",
      "total reward 2349\n",
      "loss 7.368353843688965\n",
      "steps done 687360\n",
      "900 episodes completed\n",
      "total reward 1277\n",
      "loss 7.080043315887451\n",
      "steps done 687930\n",
      "901 episodes completed\n",
      "total reward 1465\n",
      "loss 8.643457412719727\n",
      "steps done 688540\n",
      "902 episodes completed\n",
      "total reward 3175\n",
      "loss 6.300049781799316\n",
      "steps done 689640\n",
      "update target\n",
      "903 episodes completed\n",
      "total reward 1690\n",
      "loss 6.63482666015625\n",
      "steps done 690170\n",
      "904 episodes completed\n",
      "total reward 2041\n",
      "loss 6.599688529968262\n",
      "steps done 690990\n",
      "905 episodes completed\n",
      "total reward 2021\n",
      "loss 6.130300521850586\n",
      "steps done 691590\n",
      "906 episodes completed\n",
      "total reward 2145\n",
      "loss 7.565852642059326\n",
      "steps done 692260\n",
      "907 episodes completed\n",
      "total reward 1185\n",
      "loss 7.170684814453125\n",
      "steps done 692850\n",
      "908 episodes completed\n",
      "total reward 2388\n",
      "loss 9.068120002746582\n",
      "steps done 693710\n",
      "909 episodes completed\n",
      "total reward 1887\n",
      "loss 6.761224746704102\n",
      "steps done 694320\n",
      "910 episodes completed\n",
      "total reward 1868\n",
      "loss 6.266529083251953\n",
      "steps done 694910\n",
      "911 episodes completed\n",
      "total reward 2486\n",
      "loss 7.298006057739258\n",
      "steps done 695840\n",
      "912 episodes completed\n",
      "total reward 2027\n",
      "loss 5.1976518630981445\n",
      "steps done 696410\n",
      "913 episodes completed\n",
      "total reward 2249\n",
      "loss 6.379880905151367\n",
      "steps done 697270\n",
      "914 episodes completed\n",
      "total reward 1549\n",
      "loss 5.045915603637695\n",
      "steps done 697910\n",
      "915 episodes completed\n",
      "total reward 2072\n",
      "loss 6.3115434646606445\n",
      "steps done 698570\n",
      "916 episodes completed\n",
      "total reward 1801\n",
      "loss 5.453311920166016\n",
      "steps done 699110\n",
      "917 episodes completed\n",
      "total reward 1329\n",
      "loss 6.164059638977051\n",
      "steps done 699710\n",
      "update target\n",
      "918 episodes completed\n",
      "total reward 1730\n",
      "loss 7.381134033203125\n",
      "steps done 700310\n",
      "919 episodes completed\n",
      "total reward 1507\n",
      "loss 8.531320571899414\n",
      "steps done 701030\n",
      "920 episodes completed\n",
      "total reward 1896\n",
      "loss 8.464034080505371\n",
      "steps done 701700\n",
      "921 episodes completed\n",
      "total reward 0\n",
      "loss 5.280527114868164\n",
      "steps done 702130\n",
      "922 episodes completed\n",
      "total reward 2184\n",
      "loss 5.749558448791504\n",
      "steps done 702760\n",
      "923 episodes completed\n",
      "total reward 2724\n",
      "loss 5.861545562744141\n",
      "steps done 703930\n",
      "924 episodes completed\n",
      "total reward 2066\n",
      "loss 6.8441314697265625\n",
      "steps done 704720\n",
      "925 episodes completed\n",
      "total reward 2046\n",
      "loss 6.232962131500244\n",
      "steps done 705370\n",
      "926 episodes completed\n",
      "total reward 1698\n",
      "loss 5.388760566711426\n",
      "steps done 705950\n",
      "927 episodes completed\n",
      "total reward 2435\n",
      "loss 8.756576538085938\n",
      "steps done 706680\n",
      "928 episodes completed\n",
      "total reward 2712\n",
      "loss 7.973841190338135\n",
      "steps done 707770\n",
      "929 episodes completed\n",
      "total reward 1558\n",
      "loss 6.644653797149658\n",
      "steps done 708360\n",
      "930 episodes completed\n",
      "total reward 2009\n",
      "loss 8.475635528564453\n",
      "steps done 709130\n",
      "931 episodes completed\n",
      "total reward 405\n",
      "loss 4.945873260498047\n",
      "steps done 709710\n",
      "update target\n",
      "932 episodes completed\n",
      "total reward 1822\n",
      "loss 7.157806873321533\n",
      "steps done 710560\n",
      "933 episodes completed\n",
      "total reward 2051\n",
      "loss 6.771883487701416\n",
      "steps done 711230\n",
      "934 episodes completed\n",
      "total reward 1939\n",
      "loss 7.677847862243652\n",
      "steps done 711940\n",
      "935 episodes completed\n",
      "total reward 2290\n",
      "loss 6.636039733886719\n",
      "steps done 712700\n",
      "936 episodes completed\n",
      "total reward 1508\n",
      "loss 6.7151947021484375\n",
      "steps done 713250\n",
      "937 episodes completed\n",
      "total reward 2713\n",
      "loss 7.29432487487793\n",
      "steps done 714150\n",
      "938 episodes completed\n",
      "total reward 1416\n",
      "loss 8.382972717285156\n",
      "steps done 714890\n",
      "939 episodes completed\n",
      "total reward 2481\n",
      "loss 8.40463924407959\n",
      "steps done 716080\n",
      "940 episodes completed\n",
      "total reward 1402\n",
      "loss 6.945456504821777\n",
      "steps done 716630\n",
      "941 episodes completed\n",
      "total reward 1609\n",
      "loss 7.27340030670166\n",
      "steps done 717390\n",
      "942 episodes completed\n",
      "total reward 1711\n",
      "loss 5.229959487915039\n",
      "steps done 718050\n",
      "943 episodes completed\n",
      "total reward 2602\n",
      "loss 6.5233001708984375\n",
      "steps done 718940\n",
      "944 episodes completed\n",
      "total reward 1870\n",
      "loss 8.388708114624023\n",
      "steps done 719970\n",
      "update target\n",
      "945 episodes completed\n",
      "total reward 3030\n",
      "loss 9.630562782287598\n",
      "steps done 721030\n",
      "946 episodes completed\n",
      "total reward 2607\n",
      "loss 7.296924591064453\n",
      "steps done 722020\n",
      "947 episodes completed\n",
      "total reward 2258\n",
      "loss 7.9397382736206055\n",
      "steps done 722700\n",
      "948 episodes completed\n",
      "total reward 1387\n",
      "loss 5.590935230255127\n",
      "steps done 723310\n",
      "949 episodes completed\n",
      "total reward 1360\n",
      "loss 5.95685338973999\n",
      "steps done 723870\n",
      "950 episodes completed\n",
      "total reward 1359\n",
      "loss 5.578866958618164\n",
      "steps done 724480\n",
      "951 episodes completed\n",
      "total reward 1550\n",
      "loss 6.723116874694824\n",
      "steps done 725000\n",
      "952 episodes completed\n",
      "total reward 2926\n",
      "loss 7.541957855224609\n",
      "steps done 725950\n",
      "953 episodes completed\n",
      "total reward 1860\n",
      "loss 5.966108322143555\n",
      "steps done 726730\n",
      "954 episodes completed\n",
      "total reward 1681\n",
      "loss 7.148990154266357\n",
      "steps done 727380\n",
      "955 episodes completed\n",
      "total reward 1720\n",
      "loss 7.481637001037598\n",
      "steps done 727950\n",
      "956 episodes completed\n",
      "total reward 2460\n",
      "loss 7.588350296020508\n",
      "steps done 728800\n",
      "957 episodes completed\n",
      "total reward 2214\n",
      "loss 7.920623302459717\n",
      "steps done 729670\n",
      "update target\n",
      "958 episodes completed\n",
      "total reward 2014\n",
      "loss 6.47319221496582\n",
      "steps done 730360\n",
      "959 episodes completed\n",
      "total reward 1457\n",
      "loss 7.146214962005615\n",
      "steps done 731130\n",
      "960 episodes completed\n",
      "total reward 1887\n",
      "loss 7.110054016113281\n",
      "steps done 732200\n",
      "961 episodes completed\n",
      "total reward 2584\n",
      "loss 7.7187089920043945\n",
      "steps done 732990\n",
      "962 episodes completed\n",
      "total reward 2014\n",
      "loss 7.437397003173828\n",
      "steps done 733730\n",
      "963 episodes completed\n",
      "total reward 2160\n",
      "loss 8.160186767578125\n",
      "steps done 734780\n",
      "964 episodes completed\n",
      "total reward 1666\n",
      "loss 6.372420310974121\n",
      "steps done 735310\n",
      "965 episodes completed\n",
      "total reward 1545\n",
      "loss 6.99105167388916\n",
      "steps done 736040\n",
      "966 episodes completed\n",
      "total reward 1529\n",
      "loss 6.54438591003418\n",
      "steps done 736630\n",
      "967 episodes completed\n",
      "total reward 2239\n",
      "loss 6.56089973449707\n",
      "steps done 737400\n",
      "968 episodes completed\n",
      "total reward 2507\n",
      "loss 7.410218238830566\n",
      "steps done 738450\n",
      "969 episodes completed\n",
      "total reward 2011\n",
      "loss 8.412276268005371\n",
      "steps done 739070\n",
      "970 episodes completed\n",
      "total reward 2216\n",
      "loss 6.291359901428223\n",
      "steps done 739740\n",
      "update target\n",
      "971 episodes completed\n",
      "total reward 2336\n",
      "loss 6.394256114959717\n",
      "steps done 740560\n",
      "972 episodes completed\n",
      "total reward 2483\n",
      "loss 6.308612823486328\n",
      "steps done 741280\n",
      "973 episodes completed\n",
      "total reward 2026\n",
      "loss 7.482484340667725\n",
      "steps done 741930\n",
      "974 episodes completed\n",
      "total reward 2036\n",
      "loss 5.898807525634766\n",
      "steps done 742580\n",
      "975 episodes completed\n",
      "total reward 2334\n",
      "loss 6.7715864181518555\n",
      "steps done 743310\n",
      "976 episodes completed\n",
      "total reward 1040\n",
      "loss 6.8632378578186035\n",
      "steps done 744080\n",
      "977 episodes completed\n",
      "total reward 2256\n",
      "loss 4.404486179351807\n",
      "steps done 744780\n",
      "978 episodes completed\n",
      "total reward 1931\n",
      "loss 5.130863666534424\n",
      "steps done 745410\n",
      "979 episodes completed\n",
      "total reward 1990\n",
      "loss 5.05342960357666\n",
      "steps done 746080\n",
      "980 episodes completed\n",
      "total reward 1515\n",
      "loss 5.6603193283081055\n",
      "steps done 746610\n",
      "981 episodes completed\n",
      "total reward 2184\n",
      "loss 7.221227169036865\n",
      "steps done 747260\n",
      "982 episodes completed\n",
      "total reward 2319\n",
      "loss 7.895864486694336\n",
      "steps done 748260\n",
      "983 episodes completed\n",
      "total reward 2097\n",
      "loss 5.665825366973877\n",
      "steps done 748950\n",
      "984 episodes completed\n",
      "total reward 2118\n",
      "loss 4.484219551086426\n",
      "steps done 749750\n",
      "update target\n",
      "985 episodes completed\n",
      "total reward 1917\n",
      "loss 8.326581954956055\n",
      "steps done 750420\n",
      "986 episodes completed\n",
      "total reward 1867\n",
      "loss 7.621809959411621\n",
      "steps done 751160\n",
      "987 episodes completed\n",
      "total reward 2046\n",
      "loss 6.970806121826172\n",
      "steps done 751800\n",
      "988 episodes completed\n",
      "total reward 1921\n",
      "loss 6.641611099243164\n",
      "steps done 752510\n",
      "989 episodes completed\n",
      "total reward 1738\n",
      "loss 7.037172317504883\n",
      "steps done 753190\n",
      "990 episodes completed\n",
      "total reward 1938\n",
      "loss 6.737505912780762\n",
      "steps done 753800\n",
      "991 episodes completed\n",
      "total reward 2278\n",
      "loss 5.578235626220703\n",
      "steps done 754530\n",
      "992 episodes completed\n",
      "total reward 2063\n",
      "loss 5.307900905609131\n",
      "steps done 755300\n",
      "993 episodes completed\n",
      "total reward 1451\n",
      "loss 5.677687644958496\n",
      "steps done 755960\n",
      "994 episodes completed\n",
      "total reward 2098\n",
      "loss 5.404962062835693\n",
      "steps done 756680\n",
      "995 episodes completed\n",
      "total reward 291\n",
      "loss 5.762800216674805\n",
      "steps done 757550\n",
      "996 episodes completed\n",
      "total reward 1880\n",
      "loss 6.79632043838501\n",
      "steps done 758100\n",
      "997 episodes completed\n",
      "total reward 3696\n",
      "loss 5.814421653747559\n",
      "steps done 759330\n",
      "update target\n",
      "998 episodes completed\n",
      "total reward 2425\n",
      "loss 8.07725715637207\n",
      "steps done 760100\n",
      "999 episodes completed\n",
      "total reward 1838\n",
      "loss 5.250434875488281\n",
      "steps done 760840\n",
      "1000 episodes completed\n",
      "total reward 2066\n",
      "loss 7.821739196777344\n",
      "steps done 761520\n",
      "1001 episodes completed\n",
      "total reward 2288\n",
      "loss 7.204948425292969\n",
      "steps done 762270\n",
      "1002 episodes completed\n",
      "total reward 1196\n",
      "loss 8.030633926391602\n",
      "steps done 763230\n",
      "1003 episodes completed\n",
      "total reward 2596\n",
      "loss 7.1640214920043945\n",
      "steps done 764180\n",
      "1004 episodes completed\n",
      "total reward 1648\n",
      "loss 5.3472442626953125\n",
      "steps done 764810\n",
      "1005 episodes completed\n",
      "total reward 2980\n",
      "loss 7.876575469970703\n",
      "steps done 766080\n",
      "1006 episodes completed\n",
      "total reward 1862\n",
      "loss 6.048827648162842\n",
      "steps done 766700\n",
      "1007 episodes completed\n",
      "total reward 2252\n",
      "loss 6.777317523956299\n",
      "steps done 767350\n",
      "1008 episodes completed\n",
      "total reward 2167\n",
      "loss 7.388685703277588\n",
      "steps done 768170\n",
      "1009 episodes completed\n",
      "total reward 2623\n",
      "loss 5.298172950744629\n",
      "steps done 768990\n",
      "1010 episodes completed\n",
      "total reward 2251\n",
      "loss 8.495959281921387\n",
      "steps done 769650\n",
      "update target\n",
      "1011 episodes completed\n",
      "total reward 1254\n",
      "loss 7.292765140533447\n",
      "steps done 770240\n",
      "1012 episodes completed\n",
      "total reward 2344\n",
      "loss 7.793696880340576\n",
      "steps done 770990\n",
      "1013 episodes completed\n",
      "total reward 2552\n",
      "loss 6.971435070037842\n",
      "steps done 771990\n",
      "1014 episodes completed\n",
      "total reward 1704\n",
      "loss 7.510114669799805\n",
      "steps done 772790\n",
      "1015 episodes completed\n",
      "total reward 641\n",
      "loss 6.4446306228637695\n",
      "steps done 773350\n",
      "1016 episodes completed\n",
      "total reward 2099\n",
      "loss 8.774019241333008\n",
      "steps done 774040\n",
      "1017 episodes completed\n",
      "total reward 1558\n",
      "loss 5.490909576416016\n",
      "steps done 774610\n",
      "1018 episodes completed\n",
      "total reward 1688\n",
      "loss 5.3638153076171875\n",
      "steps done 775430\n",
      "1019 episodes completed\n",
      "total reward 3540\n",
      "loss 9.437173843383789\n",
      "steps done 776710\n",
      "1020 episodes completed\n",
      "total reward 1542\n",
      "loss 7.539926052093506\n",
      "steps done 777220\n",
      "1021 episodes completed\n",
      "total reward 2261\n",
      "loss 6.092541694641113\n",
      "steps done 777960\n",
      "1022 episodes completed\n",
      "total reward 1444\n",
      "loss 8.125051498413086\n",
      "steps done 778520\n",
      "1023 episodes completed\n",
      "total reward 1190\n",
      "loss 5.259642124176025\n",
      "steps done 779660\n",
      "update target\n",
      "1024 episodes completed\n",
      "total reward 1302\n",
      "loss 6.453724384307861\n",
      "steps done 780120\n",
      "1025 episodes completed\n",
      "total reward 1861\n",
      "loss 7.216357231140137\n",
      "steps done 780870\n",
      "1026 episodes completed\n",
      "total reward 1814\n",
      "loss 5.993284225463867\n",
      "steps done 781490\n",
      "1027 episodes completed\n",
      "total reward 2398\n",
      "loss 6.480339050292969\n",
      "steps done 782170\n",
      "1028 episodes completed\n",
      "total reward 2168\n",
      "loss 8.02828598022461\n",
      "steps done 782900\n",
      "1029 episodes completed\n",
      "total reward 1592\n",
      "loss 7.363994121551514\n",
      "steps done 783390\n",
      "1030 episodes completed\n",
      "total reward 1636\n",
      "loss 8.238297462463379\n",
      "steps done 784180\n",
      "1031 episodes completed\n",
      "total reward 2810\n",
      "loss 5.078948974609375\n",
      "steps done 784990\n",
      "1032 episodes completed\n",
      "total reward 2159\n",
      "loss 4.993804931640625\n",
      "steps done 785580\n",
      "1033 episodes completed\n",
      "total reward 2107\n",
      "loss 6.5143961906433105\n",
      "steps done 786160\n",
      "1034 episodes completed\n",
      "total reward 2130\n",
      "loss 7.166256904602051\n",
      "steps done 786850\n",
      "1035 episodes completed\n",
      "total reward 1926\n",
      "loss 8.943889617919922\n",
      "steps done 787730\n",
      "1036 episodes completed\n",
      "total reward 2661\n",
      "loss 7.20430326461792\n",
      "steps done 788750\n",
      "1037 episodes completed\n",
      "total reward 2617\n",
      "loss 5.76893424987793\n",
      "steps done 789540\n",
      "update target\n",
      "1038 episodes completed\n",
      "total reward 2426\n",
      "loss 6.903482913970947\n",
      "steps done 790350\n",
      "1039 episodes completed\n",
      "total reward 2017\n",
      "loss 7.582367897033691\n",
      "steps done 791110\n",
      "1040 episodes completed\n",
      "total reward 2471\n",
      "loss 6.260165214538574\n",
      "steps done 791870\n",
      "1041 episodes completed\n",
      "total reward 1819\n",
      "loss 6.356575965881348\n",
      "steps done 792590\n",
      "1042 episodes completed\n",
      "total reward 2826\n",
      "loss 5.248331546783447\n",
      "steps done 793420\n",
      "1043 episodes completed\n",
      "total reward 2227\n",
      "loss 7.186464309692383\n",
      "steps done 794200\n",
      "1044 episodes completed\n",
      "total reward 1961\n",
      "loss 5.296133518218994\n",
      "steps done 794790\n",
      "1045 episodes completed\n",
      "total reward 2525\n",
      "loss 6.128064155578613\n",
      "steps done 795630\n",
      "1046 episodes completed\n",
      "total reward 1839\n",
      "loss 7.241718292236328\n",
      "steps done 796190\n",
      "1047 episodes completed\n",
      "total reward 2006\n",
      "loss 5.772572040557861\n",
      "steps done 796820\n",
      "1048 episodes completed\n",
      "total reward 2930\n",
      "loss 4.968573570251465\n",
      "steps done 797700\n",
      "1049 episodes completed\n",
      "total reward 2507\n",
      "loss 7.581569671630859\n",
      "steps done 798560\n",
      "1050 episodes completed\n",
      "total reward 2036\n",
      "loss 5.365669250488281\n",
      "steps done 799200\n",
      "1051 episodes completed\n",
      "total reward 1040\n",
      "loss 5.682215213775635\n",
      "steps done 799690\n",
      "update target\n",
      "1052 episodes completed\n",
      "total reward 1817\n",
      "loss 6.772515296936035\n",
      "steps done 800410\n",
      "1053 episodes completed\n",
      "total reward 2139\n",
      "loss 8.221923828125\n",
      "steps done 801140\n",
      "1054 episodes completed\n",
      "total reward 1543\n",
      "loss 7.703575611114502\n",
      "steps done 801790\n",
      "1055 episodes completed\n",
      "total reward 2013\n",
      "loss 6.856421947479248\n",
      "steps done 802490\n",
      "1056 episodes completed\n",
      "total reward 2260\n",
      "loss 7.728403091430664\n",
      "steps done 803170\n",
      "1057 episodes completed\n",
      "total reward 1877\n",
      "loss 9.100406646728516\n",
      "steps done 803830\n",
      "1058 episodes completed\n",
      "total reward 2067\n",
      "loss 7.240843772888184\n",
      "steps done 804570\n",
      "1059 episodes completed\n",
      "total reward 1892\n",
      "loss 6.099954605102539\n",
      "steps done 805220\n",
      "1060 episodes completed\n",
      "total reward 1482\n",
      "loss 7.831226348876953\n",
      "steps done 805800\n",
      "1061 episodes completed\n",
      "total reward 2544\n",
      "loss 6.854625701904297\n",
      "steps done 806570\n",
      "1062 episodes completed\n",
      "total reward 2817\n",
      "loss 8.210843086242676\n",
      "steps done 807560\n",
      "1063 episodes completed\n",
      "total reward 2046\n",
      "loss 6.685498237609863\n",
      "steps done 808270\n",
      "1064 episodes completed\n",
      "total reward 2665\n",
      "loss 5.835021018981934\n",
      "steps done 809500\n",
      "update target\n",
      "1065 episodes completed\n",
      "total reward 1890\n",
      "loss 5.084650993347168\n",
      "steps done 810150\n",
      "1066 episodes completed\n",
      "total reward 1830\n",
      "loss 7.130268573760986\n",
      "steps done 810790\n",
      "1067 episodes completed\n",
      "total reward 2790\n",
      "loss 8.524542808532715\n",
      "steps done 811910\n",
      "1068 episodes completed\n",
      "total reward 2596\n",
      "loss 9.24038314819336\n",
      "steps done 812670\n",
      "1069 episodes completed\n",
      "total reward 2409\n",
      "loss 8.482637405395508\n",
      "steps done 813550\n",
      "1070 episodes completed\n",
      "total reward 1587\n",
      "loss 8.722660064697266\n",
      "steps done 814150\n",
      "1071 episodes completed\n",
      "total reward 1875\n",
      "loss 6.54782772064209\n",
      "steps done 814750\n",
      "1072 episodes completed\n",
      "total reward 1676\n",
      "loss 5.812755584716797\n",
      "steps done 815400\n",
      "1073 episodes completed\n",
      "total reward 1367\n",
      "loss 7.508138656616211\n",
      "steps done 815870\n",
      "1074 episodes completed\n",
      "total reward 2487\n",
      "loss 7.801992893218994\n",
      "steps done 816640\n",
      "1075 episodes completed\n",
      "total reward 2808\n",
      "loss 6.806235313415527\n",
      "steps done 817480\n",
      "1076 episodes completed\n",
      "total reward 2328\n",
      "loss 8.276345252990723\n",
      "steps done 818270\n",
      "1077 episodes completed\n",
      "total reward 1267\n",
      "loss 7.165019989013672\n",
      "steps done 818800\n",
      "1078 episodes completed\n",
      "total reward 2454\n",
      "loss 7.795803070068359\n",
      "steps done 819620\n",
      "update target\n",
      "1079 episodes completed\n",
      "total reward 2188\n",
      "loss 8.075485229492188\n",
      "steps done 820480\n",
      "1080 episodes completed\n",
      "total reward 3453\n",
      "loss 10.346284866333008\n",
      "steps done 821780\n",
      "1081 episodes completed\n",
      "total reward 2906\n",
      "loss 9.197683334350586\n",
      "steps done 822610\n",
      "1082 episodes completed\n",
      "total reward 2803\n",
      "loss 7.302153587341309\n",
      "steps done 823430\n",
      "1083 episodes completed\n",
      "total reward 1786\n",
      "loss 7.583888053894043\n",
      "steps done 824080\n",
      "1084 episodes completed\n",
      "total reward 1877\n",
      "loss 5.872067451477051\n",
      "steps done 824660\n",
      "1085 episodes completed\n",
      "total reward 2094\n",
      "loss 7.419005870819092\n",
      "steps done 825290\n",
      "1086 episodes completed\n",
      "total reward 1581\n",
      "loss 8.387901306152344\n",
      "steps done 826010\n",
      "1087 episodes completed\n",
      "total reward 1844\n",
      "loss 5.776785850524902\n",
      "steps done 826630\n",
      "1088 episodes completed\n",
      "total reward 2019\n",
      "loss 7.872797012329102\n",
      "steps done 827270\n",
      "1089 episodes completed\n",
      "total reward 2646\n",
      "loss 6.081691741943359\n",
      "steps done 828110\n",
      "1090 episodes completed\n",
      "total reward 1550\n",
      "loss 7.897495269775391\n",
      "steps done 828670\n",
      "1091 episodes completed\n",
      "total reward 2168\n",
      "loss 5.625605583190918\n",
      "steps done 829350\n",
      "update target\n",
      "1092 episodes completed\n",
      "total reward 1970\n",
      "loss 6.3190507888793945\n",
      "steps done 830010\n",
      "1093 episodes completed\n",
      "total reward 2226\n",
      "loss 8.565006256103516\n",
      "steps done 830790\n",
      "1094 episodes completed\n",
      "total reward 2073\n",
      "loss 6.668212890625\n",
      "steps done 831530\n",
      "1095 episodes completed\n",
      "total reward 2151\n",
      "loss 7.040287017822266\n",
      "steps done 832220\n",
      "1096 episodes completed\n",
      "total reward 1992\n",
      "loss 6.646474838256836\n",
      "steps done 832860\n",
      "1097 episodes completed\n",
      "total reward 3003\n",
      "loss 7.545419692993164\n",
      "steps done 834200\n",
      "1098 episodes completed\n",
      "total reward 2337\n",
      "loss 7.11550235748291\n",
      "steps done 835070\n",
      "1099 episodes completed\n",
      "total reward 2948\n",
      "loss 7.270403861999512\n",
      "steps done 835960\n",
      "1100 episodes completed\n",
      "total reward 2103\n",
      "loss 6.81820011138916\n",
      "steps done 836640\n",
      "1101 episodes completed\n",
      "total reward 2299\n",
      "loss 7.48613166809082\n",
      "steps done 837270\n",
      "1102 episodes completed\n",
      "total reward 1888\n",
      "loss 7.817740440368652\n",
      "steps done 837900\n",
      "1103 episodes completed\n",
      "total reward 2951\n",
      "loss 6.618491172790527\n",
      "steps done 838910\n",
      "1104 episodes completed\n",
      "total reward 1729\n",
      "loss 6.911838531494141\n",
      "steps done 839540\n",
      "update target\n",
      "1105 episodes completed\n",
      "total reward 3645\n",
      "loss 5.910005569458008\n",
      "steps done 840830\n",
      "1106 episodes completed\n",
      "total reward 1971\n",
      "loss 10.592514991760254\n",
      "steps done 841420\n",
      "1107 episodes completed\n",
      "total reward 1849\n",
      "loss 8.767066955566406\n",
      "steps done 842100\n",
      "1108 episodes completed\n",
      "total reward 2207\n",
      "loss 7.822002410888672\n",
      "steps done 842780\n",
      "1109 episodes completed\n",
      "total reward 830\n",
      "loss 7.002353191375732\n",
      "steps done 843790\n",
      "1110 episodes completed\n",
      "total reward 2119\n",
      "loss 6.849214553833008\n",
      "steps done 844410\n",
      "1111 episodes completed\n",
      "total reward 2942\n",
      "loss 4.598796844482422\n",
      "steps done 845490\n",
      "1112 episodes completed\n",
      "total reward 435\n",
      "loss 7.433147430419922\n",
      "steps done 846030\n",
      "1113 episodes completed\n",
      "total reward 1723\n",
      "loss 6.849057197570801\n",
      "steps done 846620\n",
      "1114 episodes completed\n",
      "total reward 1289\n",
      "loss 5.660813331604004\n",
      "steps done 847160\n",
      "1115 episodes completed\n",
      "total reward 1950\n",
      "loss 6.389984130859375\n",
      "steps done 847710\n",
      "1116 episodes completed\n",
      "total reward 2369\n",
      "loss 6.890777587890625\n",
      "steps done 848490\n",
      "1117 episodes completed\n",
      "total reward 2329\n",
      "loss 6.325227737426758\n",
      "steps done 849310\n",
      "1118 episodes completed\n",
      "total reward 1710\n",
      "loss 6.492348670959473\n",
      "steps done 849870\n",
      "update target\n",
      "1119 episodes completed\n",
      "total reward 1861\n",
      "loss 8.181900024414062\n",
      "steps done 850440\n",
      "1120 episodes completed\n",
      "total reward 2060\n",
      "loss 8.336864471435547\n",
      "steps done 851160\n",
      "1121 episodes completed\n",
      "total reward 2101\n",
      "loss 7.693727493286133\n",
      "steps done 851830\n",
      "1122 episodes completed\n",
      "total reward 2201\n",
      "loss 6.567566871643066\n",
      "steps done 852710\n",
      "1123 episodes completed\n",
      "total reward 2042\n",
      "loss 7.222951889038086\n",
      "steps done 853390\n",
      "1124 episodes completed\n",
      "total reward 1940\n",
      "loss 6.721406936645508\n",
      "steps done 854010\n",
      "1125 episodes completed\n",
      "total reward 2122\n",
      "loss 7.37308406829834\n",
      "steps done 854700\n",
      "1126 episodes completed\n",
      "total reward 2435\n",
      "loss 7.504478931427002\n",
      "steps done 855350\n",
      "1127 episodes completed\n",
      "total reward 2151\n",
      "loss 6.122129440307617\n",
      "steps done 856110\n",
      "1128 episodes completed\n",
      "total reward 2865\n",
      "loss 8.683923721313477\n",
      "steps done 856930\n",
      "1129 episodes completed\n",
      "total reward 2586\n",
      "loss 7.850387096405029\n",
      "steps done 857750\n",
      "1130 episodes completed\n",
      "total reward 1755\n",
      "loss 6.352260589599609\n",
      "steps done 858520\n",
      "1131 episodes completed\n",
      "total reward 1859\n",
      "loss 5.509265899658203\n",
      "steps done 859140\n",
      "update target\n",
      "1132 episodes completed\n",
      "total reward 2627\n",
      "loss 6.265293121337891\n",
      "steps done 860000\n",
      "1133 episodes completed\n",
      "total reward 2341\n",
      "loss 7.432941913604736\n",
      "steps done 860720\n",
      "1134 episodes completed\n",
      "total reward 1910\n",
      "loss 4.895390033721924\n",
      "steps done 861300\n",
      "1135 episodes completed\n",
      "total reward 2326\n",
      "loss 5.882340908050537\n",
      "steps done 862000\n",
      "1136 episodes completed\n",
      "total reward 784\n",
      "loss 7.67665958404541\n",
      "steps done 862620\n",
      "1137 episodes completed\n",
      "total reward 1604\n",
      "loss 6.147675037384033\n",
      "steps done 863320\n",
      "1138 episodes completed\n",
      "total reward 1880\n",
      "loss 6.690712928771973\n",
      "steps done 863960\n",
      "1139 episodes completed\n",
      "total reward 1969\n",
      "loss 6.264715671539307\n",
      "steps done 864480\n",
      "1140 episodes completed\n",
      "total reward 2447\n",
      "loss 6.527261734008789\n",
      "steps done 865260\n",
      "1141 episodes completed\n",
      "total reward 1496\n",
      "loss 6.397950172424316\n",
      "steps done 865850\n",
      "1142 episodes completed\n",
      "total reward 2076\n",
      "loss 9.567317008972168\n",
      "steps done 866670\n",
      "1143 episodes completed\n",
      "total reward 2312\n",
      "loss 6.537466526031494\n",
      "steps done 867430\n",
      "1144 episodes completed\n",
      "total reward 2957\n",
      "loss 9.418627738952637\n",
      "steps done 868550\n",
      "1145 episodes completed\n",
      "total reward 3168\n",
      "loss 7.467558860778809\n",
      "steps done 869680\n",
      "update target\n",
      "1146 episodes completed\n",
      "total reward 2373\n",
      "loss 7.640927791595459\n",
      "steps done 870430\n",
      "1147 episodes completed\n",
      "total reward 1894\n",
      "loss 7.406164646148682\n",
      "steps done 871070\n",
      "1148 episodes completed\n",
      "total reward 3335\n",
      "loss 8.460930824279785\n",
      "steps done 872310\n",
      "1149 episodes completed\n",
      "total reward 1924\n",
      "loss 6.044739723205566\n",
      "steps done 873040\n",
      "1150 episodes completed\n",
      "total reward 2786\n",
      "loss 7.75770902633667\n",
      "steps done 873880\n",
      "1151 episodes completed\n",
      "total reward 1926\n",
      "loss 4.784719467163086\n",
      "steps done 874660\n",
      "1152 episodes completed\n",
      "total reward 2913\n",
      "loss 6.797354698181152\n",
      "steps done 875690\n",
      "1153 episodes completed\n",
      "total reward 2533\n",
      "loss 5.909628868103027\n",
      "steps done 876750\n",
      "1154 episodes completed\n",
      "total reward 2872\n",
      "loss 6.946415424346924\n",
      "steps done 877780\n",
      "1155 episodes completed\n",
      "total reward 2253\n",
      "loss 6.615530967712402\n",
      "steps done 878510\n",
      "1156 episodes completed\n",
      "total reward 2370\n",
      "loss 7.265186309814453\n",
      "steps done 879350\n",
      "1157 episodes completed\n",
      "total reward 1457\n",
      "loss 7.042121410369873\n",
      "steps done 879850\n",
      "update target\n",
      "1158 episodes completed\n",
      "total reward 2231\n",
      "loss 7.222301483154297\n",
      "steps done 880590\n",
      "1159 episodes completed\n",
      "total reward 1880\n",
      "loss 8.295478820800781\n",
      "steps done 881310\n",
      "1160 episodes completed\n",
      "total reward 252\n",
      "loss 8.105500221252441\n",
      "steps done 882020\n",
      "1161 episodes completed\n",
      "total reward 3107\n",
      "loss 7.269282341003418\n",
      "steps done 883040\n",
      "1162 episodes completed\n",
      "total reward 1890\n",
      "loss 8.541768074035645\n",
      "steps done 883680\n",
      "1163 episodes completed\n",
      "total reward 2053\n",
      "loss 6.610328674316406\n",
      "steps done 884330\n",
      "1164 episodes completed\n",
      "total reward 1398\n",
      "loss 7.191624164581299\n",
      "steps done 885140\n",
      "1165 episodes completed\n",
      "total reward 1714\n",
      "loss 7.788459777832031\n",
      "steps done 885700\n",
      "1166 episodes completed\n",
      "total reward 2470\n",
      "loss 5.67780065536499\n",
      "steps done 886530\n",
      "1167 episodes completed\n",
      "total reward 1759\n",
      "loss 6.382558822631836\n",
      "steps done 887110\n",
      "1168 episodes completed\n",
      "total reward 2671\n",
      "loss 5.49110221862793\n",
      "steps done 887960\n",
      "1169 episodes completed\n",
      "total reward 3400\n",
      "loss 7.260040283203125\n",
      "steps done 889100\n",
      "1170 episodes completed\n",
      "total reward 2144\n",
      "loss 6.783316135406494\n",
      "steps done 889800\n",
      "update target\n",
      "1171 episodes completed\n",
      "total reward 1799\n",
      "loss 7.730376243591309\n",
      "steps done 890220\n",
      "1172 episodes completed\n",
      "total reward 2052\n",
      "loss 7.0573649406433105\n",
      "steps done 891020\n",
      "1173 episodes completed\n",
      "total reward 1975\n",
      "loss 6.768306732177734\n",
      "steps done 891620\n",
      "1174 episodes completed\n",
      "total reward 2441\n",
      "loss 6.394668102264404\n",
      "steps done 892650\n",
      "1175 episodes completed\n",
      "total reward 2089\n",
      "loss 7.115852355957031\n",
      "steps done 893350\n",
      "1176 episodes completed\n",
      "total reward 1598\n",
      "loss 4.866238117218018\n",
      "steps done 893930\n",
      "1177 episodes completed\n",
      "total reward 2216\n",
      "loss 6.543862819671631\n",
      "steps done 894660\n",
      "1178 episodes completed\n",
      "total reward 2456\n",
      "loss 7.85658073425293\n",
      "steps done 895440\n",
      "1179 episodes completed\n",
      "total reward 1625\n",
      "loss 7.547407627105713\n",
      "steps done 896040\n",
      "1180 episodes completed\n",
      "total reward 2365\n",
      "loss 7.926912784576416\n",
      "steps done 896920\n",
      "1181 episodes completed\n",
      "total reward 1425\n",
      "loss 4.976302146911621\n",
      "steps done 897500\n",
      "1182 episodes completed\n",
      "total reward 1823\n",
      "loss 7.435907363891602\n",
      "steps done 898050\n",
      "1183 episodes completed\n",
      "total reward 2280\n",
      "loss 6.877383708953857\n",
      "steps done 899060\n",
      "1184 episodes completed\n",
      "total reward 2255\n",
      "loss 8.363422393798828\n",
      "steps done 899780\n",
      "update target\n",
      "1185 episodes completed\n",
      "total reward 1451\n",
      "loss 6.944004058837891\n",
      "steps done 900410\n",
      "1186 episodes completed\n",
      "total reward 1855\n",
      "loss 8.42416763305664\n",
      "steps done 901010\n",
      "1187 episodes completed\n",
      "total reward 1462\n",
      "loss 6.986346244812012\n",
      "steps done 901520\n",
      "1188 episodes completed\n",
      "total reward 2037\n",
      "loss 6.030698299407959\n",
      "steps done 902250\n",
      "1189 episodes completed\n",
      "total reward 1809\n",
      "loss 5.414176940917969\n",
      "steps done 902900\n",
      "1190 episodes completed\n",
      "total reward 1708\n",
      "loss 6.667003631591797\n",
      "steps done 903600\n",
      "1191 episodes completed\n",
      "total reward 1867\n",
      "loss 6.5615668296813965\n",
      "steps done 904220\n",
      "1192 episodes completed\n",
      "total reward 1330\n",
      "loss 6.292463779449463\n",
      "steps done 904700\n",
      "1193 episodes completed\n",
      "total reward 1793\n",
      "loss 6.970665454864502\n",
      "steps done 905180\n",
      "1194 episodes completed\n",
      "total reward 2449\n",
      "loss 7.01816987991333\n",
      "steps done 905890\n",
      "1195 episodes completed\n",
      "total reward 1892\n",
      "loss 7.122725963592529\n",
      "steps done 906580\n",
      "1196 episodes completed\n",
      "total reward 2989\n",
      "loss 6.300474166870117\n",
      "steps done 907950\n",
      "1197 episodes completed\n",
      "total reward 2359\n",
      "loss 4.730695724487305\n",
      "steps done 908760\n",
      "1198 episodes completed\n",
      "total reward 1746\n",
      "loss 7.454240798950195\n",
      "steps done 909410\n",
      "update target\n",
      "1199 episodes completed\n",
      "total reward 2119\n",
      "loss 5.831944942474365\n",
      "steps done 910090\n",
      "1200 episodes completed\n",
      "total reward 1729\n",
      "loss 8.84394645690918\n",
      "steps done 910610\n",
      "1201 episodes completed\n",
      "total reward 1963\n",
      "loss 7.021335601806641\n",
      "steps done 911260\n",
      "1202 episodes completed\n",
      "total reward 2399\n",
      "loss 7.623497486114502\n",
      "steps done 911990\n",
      "1203 episodes completed\n",
      "total reward 2242\n",
      "loss 6.9699578285217285\n",
      "steps done 912650\n",
      "1204 episodes completed\n",
      "total reward 1813\n",
      "loss 9.088057518005371\n",
      "steps done 913210\n",
      "1205 episodes completed\n",
      "total reward 1940\n",
      "loss 6.876744270324707\n",
      "steps done 914080\n",
      "1206 episodes completed\n",
      "total reward 2733\n",
      "loss 7.519368648529053\n",
      "steps done 914970\n",
      "1207 episodes completed\n",
      "total reward 1536\n",
      "loss 6.3560075759887695\n",
      "steps done 915460\n",
      "1208 episodes completed\n",
      "total reward 1904\n",
      "loss 7.631810188293457\n",
      "steps done 916230\n",
      "1209 episodes completed\n",
      "total reward 2069\n",
      "loss 6.319242477416992\n",
      "steps done 917510\n",
      "1210 episodes completed\n",
      "total reward 3181\n",
      "loss 5.664798736572266\n",
      "steps done 918700\n",
      "1211 episodes completed\n",
      "total reward 2501\n",
      "loss 6.861728668212891\n",
      "steps done 919440\n",
      "update target\n",
      "1212 episodes completed\n",
      "total reward 2451\n",
      "loss 7.860916614532471\n",
      "steps done 920360\n",
      "1213 episodes completed\n",
      "total reward 2994\n",
      "loss 9.178436279296875\n",
      "steps done 921320\n",
      "1214 episodes completed\n",
      "total reward 1791\n",
      "loss 10.966214179992676\n",
      "steps done 922040\n",
      "1215 episodes completed\n",
      "total reward 2116\n",
      "loss 4.5635199546813965\n",
      "steps done 922730\n",
      "1216 episodes completed\n",
      "total reward 2418\n",
      "loss 5.170270919799805\n",
      "steps done 923520\n",
      "1217 episodes completed\n",
      "total reward 2519\n",
      "loss 10.12109375\n",
      "steps done 924460\n",
      "1218 episodes completed\n",
      "total reward 2396\n",
      "loss 7.488170623779297\n",
      "steps done 925270\n",
      "1219 episodes completed\n",
      "total reward 2215\n",
      "loss 6.637650489807129\n",
      "steps done 925960\n",
      "1220 episodes completed\n",
      "total reward 2380\n",
      "loss 8.158510208129883\n",
      "steps done 926690\n",
      "1221 episodes completed\n",
      "total reward 2145\n",
      "loss 8.326383590698242\n",
      "steps done 927450\n",
      "1222 episodes completed\n",
      "total reward 2033\n",
      "loss 6.811188697814941\n",
      "steps done 928030\n",
      "1223 episodes completed\n",
      "total reward 1668\n",
      "loss 7.293956279754639\n",
      "steps done 928560\n",
      "1224 episodes completed\n",
      "total reward 3353\n",
      "loss 6.464885711669922\n",
      "steps done 929810\n",
      "update target\n",
      "1225 episodes completed\n",
      "total reward 1882\n",
      "loss 6.437905788421631\n",
      "steps done 930510\n",
      "1226 episodes completed\n",
      "total reward 2043\n",
      "loss 6.302664279937744\n",
      "steps done 931240\n",
      "1227 episodes completed\n",
      "total reward 1681\n",
      "loss 7.5855207443237305\n",
      "steps done 931780\n",
      "1228 episodes completed\n",
      "total reward 1505\n",
      "loss 5.903387546539307\n",
      "steps done 932320\n",
      "1229 episodes completed\n",
      "total reward 2565\n",
      "loss 7.826029300689697\n",
      "steps done 933040\n",
      "1230 episodes completed\n",
      "total reward 1957\n",
      "loss 8.412032127380371\n",
      "steps done 933720\n",
      "1231 episodes completed\n",
      "total reward 2238\n",
      "loss 7.334989547729492\n",
      "steps done 934390\n",
      "1232 episodes completed\n",
      "total reward 1811\n",
      "loss 7.564798831939697\n",
      "steps done 935110\n",
      "1233 episodes completed\n",
      "total reward 2067\n",
      "loss 6.566281318664551\n",
      "steps done 935820\n",
      "1234 episodes completed\n",
      "total reward 1894\n",
      "loss 6.642576217651367\n",
      "steps done 936410\n",
      "1235 episodes completed\n",
      "total reward 2162\n",
      "loss 6.566410541534424\n",
      "steps done 937130\n",
      "1236 episodes completed\n",
      "total reward 1385\n",
      "loss 7.346198081970215\n",
      "steps done 937660\n",
      "1237 episodes completed\n",
      "total reward 2549\n",
      "loss 6.878066062927246\n",
      "steps done 938290\n",
      "1238 episodes completed\n",
      "total reward 2169\n",
      "loss 6.808940410614014\n",
      "steps done 939120\n",
      "1239 episodes completed\n",
      "total reward 1959\n",
      "loss 5.595250129699707\n",
      "steps done 939740\n",
      "update target\n",
      "1240 episodes completed\n",
      "total reward 3444\n",
      "loss 7.73796272277832\n",
      "steps done 940920\n",
      "1241 episodes completed\n",
      "total reward 3298\n",
      "loss 7.072321891784668\n",
      "steps done 941990\n",
      "1242 episodes completed\n",
      "total reward 1382\n",
      "loss 6.6651153564453125\n",
      "steps done 942510\n",
      "1243 episodes completed\n",
      "total reward 1752\n",
      "loss 8.090108871459961\n",
      "steps done 943100\n",
      "1244 episodes completed\n",
      "total reward 2901\n",
      "loss 6.026605606079102\n",
      "steps done 944040\n",
      "1245 episodes completed\n",
      "total reward 1189\n",
      "loss 6.83966064453125\n",
      "steps done 944610\n",
      "1246 episodes completed\n",
      "total reward 2250\n",
      "loss 4.884446144104004\n",
      "steps done 945210\n",
      "1247 episodes completed\n",
      "total reward 1933\n",
      "loss 8.105527877807617\n",
      "steps done 945930\n",
      "1248 episodes completed\n",
      "total reward 2146\n",
      "loss 8.84144401550293\n",
      "steps done 946700\n",
      "1249 episodes completed\n",
      "total reward 1419\n",
      "loss 6.893153190612793\n",
      "steps done 947380\n",
      "1250 episodes completed\n",
      "total reward 1994\n",
      "loss 7.1469526290893555\n",
      "steps done 948000\n",
      "1251 episodes completed\n",
      "total reward 2048\n",
      "loss 8.815753936767578\n",
      "steps done 948700\n",
      "1252 episodes completed\n",
      "total reward 2923\n",
      "loss 6.090924263000488\n",
      "steps done 949650\n",
      "update target\n",
      "1253 episodes completed\n",
      "total reward 2304\n",
      "loss 7.912467956542969\n",
      "steps done 950530\n",
      "1254 episodes completed\n",
      "total reward 1645\n",
      "loss 5.940408706665039\n",
      "steps done 951230\n",
      "1255 episodes completed\n",
      "total reward 1722\n",
      "loss 8.605073928833008\n",
      "steps done 951950\n",
      "1256 episodes completed\n",
      "total reward 2251\n",
      "loss 8.484480857849121\n",
      "steps done 952630\n",
      "1257 episodes completed\n",
      "total reward 2101\n",
      "loss 5.176357269287109\n",
      "steps done 953360\n",
      "1258 episodes completed\n",
      "total reward 883\n",
      "loss 7.279764175415039\n",
      "steps done 954200\n",
      "1259 episodes completed\n",
      "total reward 2280\n",
      "loss 6.736346244812012\n",
      "steps done 955020\n",
      "1260 episodes completed\n",
      "total reward 9\n",
      "loss 8.929182052612305\n",
      "steps done 955720\n",
      "1261 episodes completed\n",
      "total reward 2340\n",
      "loss 7.7027997970581055\n",
      "steps done 956510\n",
      "1262 episodes completed\n",
      "total reward 1619\n",
      "loss 6.586050033569336\n",
      "steps done 957720\n",
      "1263 episodes completed\n",
      "total reward 1722\n",
      "loss 8.056610107421875\n",
      "steps done 958300\n",
      "1264 episodes completed\n",
      "total reward 2793\n",
      "loss 5.62066650390625\n",
      "steps done 959240\n",
      "update target\n",
      "1265 episodes completed\n",
      "total reward 3513\n",
      "loss 6.588092803955078\n",
      "steps done 960350\n",
      "1266 episodes completed\n",
      "total reward 1991\n",
      "loss 6.994455814361572\n",
      "steps done 961130\n",
      "1267 episodes completed\n",
      "total reward 2019\n",
      "loss 7.184304237365723\n",
      "steps done 961780\n",
      "1268 episodes completed\n",
      "total reward 2737\n",
      "loss 6.453296184539795\n",
      "steps done 962660\n",
      "1269 episodes completed\n",
      "total reward 2388\n",
      "loss 8.029899597167969\n",
      "steps done 963490\n",
      "1270 episodes completed\n",
      "total reward 1526\n",
      "loss 6.4542927742004395\n",
      "steps done 964020\n",
      "1271 episodes completed\n",
      "total reward 2213\n",
      "loss 7.987814903259277\n",
      "steps done 965000\n",
      "1272 episodes completed\n",
      "total reward 2162\n",
      "loss 7.350887298583984\n",
      "steps done 965640\n",
      "1273 episodes completed\n",
      "total reward 2411\n",
      "loss 5.600054740905762\n",
      "steps done 966350\n",
      "1274 episodes completed\n",
      "total reward 1766\n",
      "loss 6.61061954498291\n",
      "steps done 967000\n",
      "1275 episodes completed\n",
      "total reward 1542\n",
      "loss 5.024313926696777\n",
      "steps done 967640\n",
      "1276 episodes completed\n",
      "total reward 3139\n",
      "loss 7.04555082321167\n",
      "steps done 968880\n",
      "1277 episodes completed\n",
      "total reward 0\n",
      "loss 9.7157621383667\n",
      "steps done 969800\n",
      "update target\n",
      "1278 episodes completed\n",
      "total reward 1823\n",
      "loss 8.152349472045898\n",
      "steps done 970500\n",
      "1279 episodes completed\n",
      "total reward 2107\n",
      "loss 7.289736747741699\n",
      "steps done 971240\n",
      "1280 episodes completed\n",
      "total reward 2150\n",
      "loss 6.993940353393555\n",
      "steps done 972020\n",
      "1281 episodes completed\n",
      "total reward 1819\n",
      "loss 8.204747200012207\n",
      "steps done 972590\n",
      "1282 episodes completed\n",
      "total reward 1553\n",
      "loss 6.10752010345459\n",
      "steps done 973010\n",
      "1283 episodes completed\n",
      "total reward 2652\n",
      "loss 6.805193901062012\n",
      "steps done 974230\n",
      "1284 episodes completed\n",
      "total reward 3328\n",
      "loss 6.22620964050293\n",
      "steps done 975360\n",
      "1285 episodes completed\n",
      "total reward 2399\n",
      "loss 6.0530900955200195\n",
      "steps done 976080\n",
      "1286 episodes completed\n",
      "total reward 1647\n",
      "loss 7.3772382736206055\n",
      "steps done 976950\n",
      "1287 episodes completed\n",
      "total reward 2702\n",
      "loss 8.012612342834473\n",
      "steps done 977830\n",
      "1288 episodes completed\n",
      "total reward 0\n",
      "loss 5.202666282653809\n",
      "steps done 978660\n",
      "1289 episodes completed\n",
      "total reward 1963\n",
      "loss 6.562798500061035\n",
      "steps done 979390\n",
      "update target\n",
      "1290 episodes completed\n",
      "total reward 2199\n",
      "loss 5.414226531982422\n",
      "steps done 980070\n",
      "1291 episodes completed\n",
      "total reward 2593\n",
      "loss 5.5864152908325195\n",
      "steps done 981260\n",
      "1292 episodes completed\n",
      "total reward 1749\n",
      "loss 6.3197922706604\n",
      "steps done 981990\n",
      "1293 episodes completed\n",
      "total reward 1232\n",
      "loss 6.384317398071289\n",
      "steps done 982510\n",
      "1294 episodes completed\n",
      "total reward 1778\n",
      "loss 7.031520843505859\n",
      "steps done 983280\n",
      "1295 episodes completed\n",
      "total reward 2200\n",
      "loss 4.658597946166992\n",
      "steps done 984240\n",
      "1296 episodes completed\n",
      "total reward 1283\n",
      "loss 5.988223075866699\n",
      "steps done 985110\n",
      "1297 episodes completed\n",
      "total reward 410\n",
      "loss 5.104922771453857\n",
      "steps done 985760\n",
      "1298 episodes completed\n",
      "total reward 1176\n",
      "loss 6.519835472106934\n",
      "steps done 986750\n",
      "1299 episodes completed\n",
      "total reward 1743\n",
      "loss 6.815623760223389\n",
      "steps done 987300\n",
      "1300 episodes completed\n",
      "total reward 1655\n",
      "loss 7.483040809631348\n",
      "steps done 987890\n",
      "1301 episodes completed\n",
      "total reward 1761\n",
      "loss 7.425192832946777\n",
      "steps done 988540\n",
      "1302 episodes completed\n",
      "total reward 2191\n",
      "loss 4.524853706359863\n",
      "steps done 989500\n",
      "update target\n",
      "1303 episodes completed\n",
      "total reward 1852\n",
      "loss 6.72227668762207\n",
      "steps done 990100\n",
      "1304 episodes completed\n",
      "total reward 2087\n",
      "loss 6.927469253540039\n",
      "steps done 990820\n",
      "1305 episodes completed\n",
      "total reward 2071\n",
      "loss 6.777935028076172\n",
      "steps done 991410\n",
      "1306 episodes completed\n",
      "total reward 2891\n",
      "loss 7.243951797485352\n",
      "steps done 992290\n",
      "1307 episodes completed\n",
      "total reward 127\n",
      "loss 4.8057861328125\n",
      "steps done 992790\n",
      "1308 episodes completed\n",
      "total reward 2433\n",
      "loss 4.572614669799805\n",
      "steps done 993660\n",
      "1309 episodes completed\n",
      "total reward 1476\n",
      "loss 6.162622451782227\n",
      "steps done 994460\n",
      "1310 episodes completed\n",
      "total reward 1909\n",
      "loss 10.346755027770996\n",
      "steps done 995180\n",
      "1311 episodes completed\n",
      "total reward 2058\n",
      "loss 6.39628267288208\n",
      "steps done 995900\n",
      "1312 episodes completed\n",
      "total reward 1863\n",
      "loss 7.423983573913574\n",
      "steps done 996710\n",
      "1313 episodes completed\n",
      "total reward 2082\n",
      "loss 6.594809055328369\n",
      "steps done 997560\n",
      "1314 episodes completed\n",
      "total reward 1191\n",
      "loss 5.898581027984619\n",
      "steps done 998270\n",
      "1315 episodes completed\n",
      "total reward 2096\n",
      "loss 7.1389946937561035\n",
      "steps done 998970\n",
      "1316 episodes completed\n",
      "total reward 2352\n",
      "loss 6.06348991394043\n",
      "steps done 999880\n",
      "update target\n",
      "1317 episodes completed\n",
      "total reward 1328\n",
      "loss 5.390772819519043\n",
      "steps done 1000490\n",
      "1318 episodes completed\n",
      "total reward 1972\n",
      "loss 6.015988349914551\n",
      "steps done 1001160\n",
      "1319 episodes completed\n",
      "total reward 1446\n",
      "loss 5.39649772644043\n",
      "steps done 1001650\n",
      "1320 episodes completed\n",
      "total reward 2003\n",
      "loss 5.333406925201416\n",
      "steps done 1002550\n",
      "1321 episodes completed\n",
      "total reward 1855\n",
      "loss 5.874577045440674\n",
      "steps done 1003120\n",
      "1322 episodes completed\n",
      "total reward 2063\n",
      "loss 6.890468597412109\n",
      "steps done 1003780\n",
      "1323 episodes completed\n",
      "total reward 1494\n",
      "loss 6.217396259307861\n",
      "steps done 1004380\n",
      "1324 episodes completed\n",
      "total reward 1834\n",
      "loss 5.803338050842285\n",
      "steps done 1004990\n",
      "1325 episodes completed\n",
      "total reward 1734\n",
      "loss 5.052823066711426\n",
      "steps done 1005510\n",
      "1326 episodes completed\n",
      "total reward 1631\n",
      "loss 7.704493999481201\n",
      "steps done 1006090\n",
      "1327 episodes completed\n",
      "total reward 2391\n",
      "loss 5.273135185241699\n",
      "steps done 1006870\n",
      "1328 episodes completed\n",
      "total reward 2411\n",
      "loss 6.262977600097656\n",
      "steps done 1007630\n",
      "1329 episodes completed\n",
      "total reward 2102\n",
      "loss 7.081146240234375\n",
      "steps done 1008330\n",
      "1330 episodes completed\n",
      "total reward 2491\n",
      "loss 6.022412300109863\n",
      "steps done 1009150\n",
      "1331 episodes completed\n",
      "total reward 2828\n",
      "loss 7.252265453338623\n",
      "steps done 1009990\n",
      "update target\n",
      "1332 episodes completed\n",
      "total reward 1755\n",
      "loss 6.910857677459717\n",
      "steps done 1010580\n",
      "1333 episodes completed\n",
      "total reward 3039\n",
      "loss 8.6968994140625\n",
      "steps done 1011620\n",
      "1334 episodes completed\n",
      "total reward 1991\n",
      "loss 6.456897258758545\n",
      "steps done 1012440\n",
      "1335 episodes completed\n",
      "total reward 2777\n",
      "loss 6.162400245666504\n",
      "steps done 1013420\n",
      "1336 episodes completed\n",
      "total reward 2096\n",
      "loss 8.14772891998291\n",
      "steps done 1014220\n",
      "1337 episodes completed\n",
      "total reward 1975\n",
      "loss 6.716014862060547\n",
      "steps done 1014910\n",
      "1338 episodes completed\n",
      "total reward 599\n",
      "loss 6.482027053833008\n",
      "steps done 1015960\n",
      "1339 episodes completed\n",
      "total reward 1928\n",
      "loss 5.415078639984131\n",
      "steps done 1016520\n",
      "1340 episodes completed\n",
      "total reward 2407\n",
      "loss 5.914356231689453\n",
      "steps done 1017420\n",
      "1341 episodes completed\n",
      "total reward 2082\n",
      "loss 5.749251365661621\n",
      "steps done 1018080\n",
      "1342 episodes completed\n",
      "total reward 2162\n",
      "loss 7.296954154968262\n",
      "steps done 1018760\n",
      "1343 episodes completed\n",
      "total reward 2182\n",
      "loss 6.4642157554626465\n",
      "steps done 1019470\n",
      "update target\n",
      "1344 episodes completed\n",
      "total reward 1984\n",
      "loss 7.978984832763672\n",
      "steps done 1020120\n",
      "1345 episodes completed\n",
      "total reward 2506\n",
      "loss 5.804121971130371\n",
      "steps done 1020910\n",
      "1346 episodes completed\n",
      "total reward 2323\n",
      "loss 6.324753284454346\n",
      "steps done 1021760\n",
      "1347 episodes completed\n",
      "total reward 1790\n",
      "loss 6.81539249420166\n",
      "steps done 1022360\n",
      "1348 episodes completed\n",
      "total reward 1630\n",
      "loss 6.1187849044799805\n",
      "steps done 1023030\n",
      "1349 episodes completed\n",
      "total reward 3458\n",
      "loss 7.085466384887695\n",
      "steps done 1024270\n",
      "1350 episodes completed\n",
      "total reward 1663\n",
      "loss 5.6067423820495605\n",
      "steps done 1025120\n",
      "1351 episodes completed\n",
      "total reward 4218\n",
      "loss 9.35315990447998\n",
      "steps done 1026500\n",
      "1352 episodes completed\n",
      "total reward 2138\n",
      "loss 5.963785171508789\n",
      "steps done 1027080\n",
      "1353 episodes completed\n",
      "total reward 2167\n",
      "loss 5.772502422332764\n",
      "steps done 1027730\n",
      "1354 episodes completed\n",
      "total reward 2101\n",
      "loss 7.423140525817871\n",
      "steps done 1028570\n",
      "1355 episodes completed\n",
      "total reward 1731\n",
      "loss 5.964507102966309\n",
      "steps done 1029140\n",
      "update target\n",
      "1356 episodes completed\n",
      "total reward 2250\n",
      "loss 8.309272766113281\n",
      "steps done 1030060\n",
      "1357 episodes completed\n",
      "total reward 2339\n",
      "loss 6.528894424438477\n",
      "steps done 1030940\n",
      "1358 episodes completed\n",
      "total reward 1829\n",
      "loss 9.516684532165527\n",
      "steps done 1031690\n",
      "1359 episodes completed\n",
      "total reward 1860\n",
      "loss 5.743389129638672\n",
      "steps done 1032290\n",
      "1360 episodes completed\n",
      "total reward 1966\n",
      "loss 7.451876163482666\n",
      "steps done 1033000\n",
      "1361 episodes completed\n",
      "total reward 1239\n",
      "loss 7.916347980499268\n",
      "steps done 1033760\n",
      "1362 episodes completed\n",
      "total reward 1735\n",
      "loss 7.0389251708984375\n",
      "steps done 1034380\n",
      "1363 episodes completed\n",
      "total reward 1259\n",
      "loss 6.960729598999023\n",
      "steps done 1034970\n",
      "1364 episodes completed\n",
      "total reward 3352\n",
      "loss 6.475461959838867\n",
      "steps done 1036190\n",
      "1365 episodes completed\n",
      "total reward 2349\n",
      "loss 6.116063594818115\n",
      "steps done 1036980\n",
      "1366 episodes completed\n",
      "total reward 2003\n",
      "loss 6.467190742492676\n",
      "steps done 1037600\n",
      "1367 episodes completed\n",
      "total reward 1408\n",
      "loss 5.619175910949707\n",
      "steps done 1038060\n",
      "1368 episodes completed\n",
      "total reward 2248\n",
      "loss 5.171175003051758\n",
      "steps done 1038750\n",
      "1369 episodes completed\n",
      "total reward 1853\n",
      "loss 6.752445220947266\n",
      "steps done 1039540\n",
      "update target\n",
      "1370 episodes completed\n",
      "total reward 2121\n",
      "loss 8.63824462890625\n",
      "steps done 1040200\n",
      "1371 episodes completed\n",
      "total reward 2376\n",
      "loss 7.057363986968994\n",
      "steps done 1041040\n",
      "1372 episodes completed\n",
      "total reward 1079\n",
      "loss 7.288225173950195\n",
      "steps done 1041430\n",
      "1373 episodes completed\n",
      "total reward 2029\n",
      "loss 7.317659378051758\n",
      "steps done 1042130\n",
      "1374 episodes completed\n",
      "total reward 3284\n",
      "loss 7.275585174560547\n",
      "steps done 1043360\n",
      "1375 episodes completed\n",
      "total reward 2462\n",
      "loss 6.760636806488037\n",
      "steps done 1044230\n",
      "1376 episodes completed\n",
      "total reward 2461\n",
      "loss 10.538618087768555\n",
      "steps done 1044990\n",
      "1377 episodes completed\n",
      "total reward 2506\n",
      "loss 5.736310958862305\n",
      "steps done 1046000\n",
      "1378 episodes completed\n",
      "total reward 1857\n",
      "loss 7.012842655181885\n",
      "steps done 1046740\n",
      "1379 episodes completed\n",
      "total reward 1920\n",
      "loss 6.871819496154785\n",
      "steps done 1047390\n",
      "1380 episodes completed\n",
      "total reward 1504\n",
      "loss 7.439764022827148\n",
      "steps done 1048100\n",
      "1381 episodes completed\n",
      "total reward 1265\n",
      "loss 8.27765941619873\n",
      "steps done 1049060\n",
      "1382 episodes completed\n",
      "total reward 1536\n",
      "loss 8.536993026733398\n",
      "steps done 1049610\n",
      "update target\n",
      "1383 episodes completed\n",
      "total reward 2141\n",
      "loss 6.1417741775512695\n",
      "steps done 1050420\n",
      "1384 episodes completed\n",
      "total reward 1534\n",
      "loss 6.634445667266846\n",
      "steps done 1050940\n",
      "1385 episodes completed\n",
      "total reward 1846\n",
      "loss 9.03856086730957\n",
      "steps done 1051550\n",
      "1386 episodes completed\n",
      "total reward 1997\n",
      "loss 5.693770408630371\n",
      "steps done 1052220\n",
      "1387 episodes completed\n",
      "total reward 2049\n",
      "loss 6.332199573516846\n",
      "steps done 1052800\n",
      "1388 episodes completed\n",
      "total reward 1941\n",
      "loss 5.37984037399292\n",
      "steps done 1053350\n",
      "1389 episodes completed\n",
      "total reward 2584\n",
      "loss 7.273104667663574\n",
      "steps done 1054300\n",
      "1390 episodes completed\n",
      "total reward 1880\n",
      "loss 7.263566017150879\n",
      "steps done 1054960\n",
      "1391 episodes completed\n",
      "total reward 1795\n",
      "loss 8.27392578125\n",
      "steps done 1055570\n",
      "1392 episodes completed\n",
      "total reward 1926\n",
      "loss 8.441858291625977\n",
      "steps done 1056290\n",
      "1393 episodes completed\n",
      "total reward 2441\n",
      "loss 7.410603046417236\n",
      "steps done 1057070\n",
      "1394 episodes completed\n",
      "total reward 2005\n",
      "loss 7.293208122253418\n",
      "steps done 1057710\n",
      "1395 episodes completed\n",
      "total reward 1893\n",
      "loss 5.470460891723633\n",
      "steps done 1058470\n",
      "1396 episodes completed\n",
      "total reward 1876\n",
      "loss 6.318226337432861\n",
      "steps done 1059120\n",
      "update target\n",
      "1397 episodes completed\n",
      "total reward 2665\n",
      "loss 9.265857696533203\n",
      "steps done 1060080\n",
      "1398 episodes completed\n",
      "total reward 2046\n",
      "loss 8.97603988647461\n",
      "steps done 1060740\n",
      "1399 episodes completed\n",
      "total reward 2154\n",
      "loss 6.556737899780273\n",
      "steps done 1061450\n",
      "1400 episodes completed\n",
      "total reward 2837\n",
      "loss 6.8997416496276855\n",
      "steps done 1062490\n",
      "1401 episodes completed\n",
      "total reward 2341\n",
      "loss 7.462026119232178\n",
      "steps done 1063660\n",
      "1402 episodes completed\n",
      "total reward 2637\n",
      "loss 7.69727897644043\n",
      "steps done 1064630\n",
      "1403 episodes completed\n",
      "total reward 1875\n",
      "loss 5.005936622619629\n",
      "steps done 1065290\n",
      "1404 episodes completed\n",
      "total reward 2098\n",
      "loss 5.212857246398926\n",
      "steps done 1066100\n",
      "1405 episodes completed\n",
      "total reward 3081\n",
      "loss 5.917314529418945\n",
      "steps done 1067140\n",
      "1406 episodes completed\n",
      "total reward 2129\n",
      "loss 6.9801225662231445\n",
      "steps done 1067800\n",
      "1407 episodes completed\n",
      "total reward 1675\n",
      "loss 6.242042541503906\n",
      "steps done 1068410\n",
      "1408 episodes completed\n",
      "total reward 2300\n",
      "loss 9.920190811157227\n",
      "steps done 1069120\n",
      "1409 episodes completed\n",
      "total reward 2780\n",
      "loss 6.604305267333984\n",
      "steps done 1069970\n",
      "update target\n",
      "1410 episodes completed\n",
      "total reward 2539\n",
      "loss 9.257222175598145\n",
      "steps done 1070630\n",
      "1411 episodes completed\n",
      "total reward 2250\n",
      "loss 5.793386459350586\n",
      "steps done 1071320\n",
      "1412 episodes completed\n",
      "total reward 2089\n",
      "loss 6.875444412231445\n",
      "steps done 1071960\n",
      "1413 episodes completed\n",
      "total reward 2382\n",
      "loss 6.666391372680664\n",
      "steps done 1072680\n",
      "1414 episodes completed\n",
      "total reward 2490\n",
      "loss 7.354951858520508\n",
      "steps done 1073720\n",
      "1415 episodes completed\n",
      "total reward 3669\n",
      "loss 8.42721939086914\n",
      "steps done 1074800\n",
      "1416 episodes completed\n",
      "total reward 2039\n",
      "loss 6.2425761222839355\n",
      "steps done 1075380\n",
      "1417 episodes completed\n",
      "total reward 1928\n",
      "loss 8.325190544128418\n",
      "steps done 1075930\n",
      "1418 episodes completed\n",
      "total reward 3949\n",
      "loss 8.06023120880127\n",
      "steps done 1077210\n",
      "1419 episodes completed\n",
      "total reward 3400\n",
      "loss 6.85223388671875\n",
      "steps done 1078460\n",
      "1420 episodes completed\n",
      "total reward 2085\n",
      "loss 6.8731689453125\n",
      "steps done 1079190\n",
      "1421 episodes completed\n",
      "total reward 2107\n",
      "loss 9.05211067199707\n",
      "steps done 1079890\n",
      "update target\n",
      "1422 episodes completed\n",
      "total reward 2066\n",
      "loss 10.364100456237793\n",
      "steps done 1080720\n",
      "1423 episodes completed\n",
      "total reward 2263\n",
      "loss 9.528526306152344\n",
      "steps done 1081320\n",
      "1424 episodes completed\n",
      "total reward 1911\n",
      "loss 7.088034629821777\n",
      "steps done 1082000\n",
      "1425 episodes completed\n",
      "total reward 1985\n",
      "loss 9.632455825805664\n",
      "steps done 1082600\n",
      "1426 episodes completed\n",
      "total reward 2494\n",
      "loss 7.992161750793457\n",
      "steps done 1083440\n",
      "1427 episodes completed\n",
      "total reward 3168\n",
      "loss 8.19049072265625\n",
      "steps done 1084800\n",
      "1428 episodes completed\n",
      "total reward 1876\n",
      "loss 6.702192306518555\n",
      "steps done 1085340\n",
      "1429 episodes completed\n",
      "total reward 2634\n",
      "loss 6.821781158447266\n",
      "steps done 1086160\n",
      "1430 episodes completed\n",
      "total reward 2342\n",
      "loss 7.275730609893799\n",
      "steps done 1086920\n",
      "1431 episodes completed\n",
      "total reward 1501\n",
      "loss 8.057050704956055\n",
      "steps done 1087360\n",
      "1432 episodes completed\n",
      "total reward 1867\n",
      "loss 6.567507743835449\n",
      "steps done 1088020\n",
      "1433 episodes completed\n",
      "total reward 1688\n",
      "loss 5.842582702636719\n",
      "steps done 1088610\n",
      "1434 episodes completed\n",
      "total reward 1677\n",
      "loss 6.122241973876953\n",
      "steps done 1089110\n",
      "1435 episodes completed\n",
      "total reward 1992\n",
      "loss 6.668051719665527\n",
      "steps done 1089720\n",
      "update target\n",
      "1436 episodes completed\n",
      "total reward 2095\n",
      "loss 8.94523811340332\n",
      "steps done 1090530\n",
      "1437 episodes completed\n",
      "total reward 3677\n",
      "loss 6.294612407684326\n",
      "steps done 1091680\n",
      "1438 episodes completed\n",
      "total reward 1862\n",
      "loss 7.013877868652344\n",
      "steps done 1092330\n",
      "1439 episodes completed\n",
      "total reward 3132\n",
      "loss 7.756795883178711\n",
      "steps done 1093290\n",
      "1440 episodes completed\n",
      "total reward 2636\n",
      "loss 8.663963317871094\n",
      "steps done 1094150\n",
      "1441 episodes completed\n",
      "total reward 2777\n",
      "loss 6.605191230773926\n",
      "steps done 1095040\n",
      "1442 episodes completed\n",
      "total reward 2517\n",
      "loss 7.993181228637695\n",
      "steps done 1096020\n",
      "1443 episodes completed\n",
      "total reward 1933\n",
      "loss 7.163972854614258\n",
      "steps done 1096650\n",
      "1444 episodes completed\n",
      "total reward 1926\n",
      "loss 9.33853530883789\n",
      "steps done 1097310\n",
      "1445 episodes completed\n",
      "total reward 1801\n",
      "loss 8.34961986541748\n",
      "steps done 1097960\n",
      "1446 episodes completed\n",
      "total reward 2369\n",
      "loss 9.383277893066406\n",
      "steps done 1098710\n",
      "1447 episodes completed\n",
      "total reward 2624\n",
      "loss 7.252719879150391\n",
      "steps done 1099660\n",
      "update target\n",
      "1448 episodes completed\n",
      "total reward 2135\n",
      "loss 7.628878116607666\n",
      "steps done 1100380\n",
      "1449 episodes completed\n",
      "total reward 2800\n",
      "loss 8.513174057006836\n",
      "steps done 1101370\n",
      "1450 episodes completed\n",
      "total reward 3271\n",
      "loss 8.58173942565918\n",
      "steps done 1102280\n",
      "1451 episodes completed\n",
      "total reward 1770\n",
      "loss 7.096531867980957\n",
      "steps done 1102810\n",
      "1452 episodes completed\n",
      "total reward 2196\n",
      "loss 7.896172046661377\n",
      "steps done 1103530\n",
      "1453 episodes completed\n",
      "total reward 2491\n",
      "loss 7.460515975952148\n",
      "steps done 1104370\n",
      "1454 episodes completed\n",
      "total reward 2519\n",
      "loss 7.104389667510986\n",
      "steps done 1105210\n",
      "1455 episodes completed\n",
      "total reward 1224\n",
      "loss 8.962343215942383\n",
      "steps done 1105690\n",
      "1456 episodes completed\n",
      "total reward 2959\n",
      "loss 6.625763416290283\n",
      "steps done 1106790\n",
      "1457 episodes completed\n",
      "total reward 3500\n",
      "loss 7.322286605834961\n",
      "steps done 1107990\n",
      "1458 episodes completed\n",
      "total reward 2159\n",
      "loss 9.619169235229492\n",
      "steps done 1108780\n",
      "1459 episodes completed\n",
      "total reward 2241\n",
      "loss 7.101106643676758\n",
      "steps done 1109440\n",
      "update target\n",
      "1460 episodes completed\n",
      "total reward 2312\n",
      "loss 8.869773864746094\n",
      "steps done 1110270\n",
      "1461 episodes completed\n",
      "total reward 1992\n",
      "loss 7.88620662689209\n",
      "steps done 1110820\n",
      "1462 episodes completed\n",
      "total reward 2418\n",
      "loss 7.968339920043945\n",
      "steps done 1111610\n",
      "1463 episodes completed\n",
      "total reward 2212\n",
      "loss 9.419228553771973\n",
      "steps done 1112530\n",
      "1464 episodes completed\n",
      "total reward 2165\n",
      "loss 8.446949005126953\n",
      "steps done 1113380\n",
      "1465 episodes completed\n",
      "total reward 2485\n",
      "loss 8.403854370117188\n",
      "steps done 1114240\n",
      "1466 episodes completed\n",
      "total reward 2090\n",
      "loss 9.003597259521484\n",
      "steps done 1114850\n",
      "1467 episodes completed\n",
      "total reward 2045\n",
      "loss 9.60323715209961\n",
      "steps done 1115460\n",
      "1468 episodes completed\n",
      "total reward 2074\n",
      "loss 9.335020065307617\n",
      "steps done 1116250\n",
      "1469 episodes completed\n",
      "total reward 2079\n",
      "loss 6.647435188293457\n",
      "steps done 1116970\n",
      "1470 episodes completed\n",
      "total reward 1878\n",
      "loss 7.677112102508545\n",
      "steps done 1117630\n",
      "1471 episodes completed\n",
      "total reward 2386\n",
      "loss 9.810494422912598\n",
      "steps done 1118440\n",
      "1472 episodes completed\n",
      "total reward 1830\n",
      "loss 8.516788482666016\n",
      "steps done 1119040\n",
      "1473 episodes completed\n",
      "total reward 2186\n",
      "loss 8.363615036010742\n",
      "steps done 1119940\n",
      "update target\n",
      "1474 episodes completed\n",
      "total reward 2501\n",
      "loss 7.958278179168701\n",
      "steps done 1120760\n",
      "1475 episodes completed\n",
      "total reward 2644\n",
      "loss 6.49202823638916\n",
      "steps done 1121610\n",
      "1476 episodes completed\n",
      "total reward 2696\n",
      "loss 7.205434799194336\n",
      "steps done 1122640\n",
      "1477 episodes completed\n",
      "total reward 2383\n",
      "loss 6.300678253173828\n",
      "steps done 1123380\n",
      "1478 episodes completed\n",
      "total reward 2207\n",
      "loss 7.4210004806518555\n",
      "steps done 1124100\n",
      "1479 episodes completed\n",
      "total reward 2921\n",
      "loss 8.465778350830078\n",
      "steps done 1125000\n",
      "1480 episodes completed\n",
      "total reward 2115\n",
      "loss 8.51435661315918\n",
      "steps done 1125650\n",
      "1481 episodes completed\n",
      "total reward 2257\n",
      "loss 5.918586254119873\n",
      "steps done 1126520\n",
      "1482 episodes completed\n",
      "total reward 2302\n",
      "loss 8.315717697143555\n",
      "steps done 1127220\n",
      "1483 episodes completed\n",
      "total reward 1812\n",
      "loss 7.336299896240234\n",
      "steps done 1127860\n",
      "1484 episodes completed\n",
      "total reward 1995\n",
      "loss 8.73996639251709\n",
      "steps done 1128430\n",
      "1485 episodes completed\n",
      "total reward 2063\n",
      "loss 8.572376251220703\n",
      "steps done 1129170\n",
      "update target\n",
      "1486 episodes completed\n",
      "total reward 2268\n",
      "loss 7.937096118927002\n",
      "steps done 1130080\n",
      "1487 episodes completed\n",
      "total reward 2379\n",
      "loss 8.236639022827148\n",
      "steps done 1130970\n",
      "1488 episodes completed\n",
      "total reward 2021\n",
      "loss 8.003045082092285\n",
      "steps done 1131720\n",
      "1489 episodes completed\n",
      "total reward 2353\n",
      "loss 7.442337989807129\n",
      "steps done 1133030\n",
      "1490 episodes completed\n",
      "total reward 1958\n",
      "loss 9.544513702392578\n",
      "steps done 1133740\n",
      "1491 episodes completed\n",
      "total reward 2349\n",
      "loss 9.37938117980957\n",
      "steps done 1134550\n",
      "1492 episodes completed\n",
      "total reward 1428\n",
      "loss 7.38771915435791\n",
      "steps done 1135110\n",
      "1493 episodes completed\n",
      "total reward 2396\n",
      "loss 8.480598449707031\n",
      "steps done 1136130\n",
      "1494 episodes completed\n",
      "total reward 2229\n",
      "loss 7.279120445251465\n",
      "steps done 1136890\n",
      "1495 episodes completed\n",
      "total reward 2265\n",
      "loss 6.564178466796875\n",
      "steps done 1137600\n",
      "1496 episodes completed\n",
      "total reward 2735\n",
      "loss 8.294013977050781\n",
      "steps done 1138720\n",
      "1497 episodes completed\n",
      "total reward 2401\n",
      "loss 6.434512615203857\n",
      "steps done 1139530\n",
      "update target\n",
      "1498 episodes completed\n",
      "total reward 2305\n",
      "loss 12.479722023010254\n",
      "steps done 1140370\n",
      "1499 episodes completed\n",
      "total reward 1940\n",
      "loss 7.504452705383301\n",
      "steps done 1140950\n",
      "1500 episodes completed\n",
      "total reward 1812\n",
      "loss 6.782504558563232\n",
      "steps done 1141830\n",
      "1501 episodes completed\n",
      "total reward 1734\n",
      "loss 7.627805233001709\n",
      "steps done 1142470\n",
      "1502 episodes completed\n",
      "total reward 2472\n",
      "loss 8.34101390838623\n",
      "steps done 1143230\n",
      "1503 episodes completed\n",
      "total reward 3160\n",
      "loss 7.361303329467773\n",
      "steps done 1144300\n",
      "1504 episodes completed\n",
      "total reward 2420\n",
      "loss 7.300839424133301\n",
      "steps done 1145270\n",
      "1505 episodes completed\n",
      "total reward 1319\n",
      "loss 8.101694107055664\n",
      "steps done 1145750\n",
      "1506 episodes completed\n",
      "total reward 2641\n",
      "loss 9.037322998046875\n",
      "steps done 1146630\n",
      "1507 episodes completed\n",
      "total reward 3167\n",
      "loss 8.169191360473633\n",
      "steps done 1147640\n",
      "1508 episodes completed\n",
      "total reward 1960\n",
      "loss 8.028244018554688\n",
      "steps done 1148230\n",
      "1509 episodes completed\n",
      "total reward 1756\n",
      "loss 7.97756814956665\n",
      "steps done 1148830\n",
      "1510 episodes completed\n",
      "total reward 1926\n",
      "loss 7.943633556365967\n",
      "steps done 1149380\n",
      "update target\n",
      "1511 episodes completed\n",
      "total reward 1799\n",
      "loss 10.221389770507812\n",
      "steps done 1150010\n",
      "1512 episodes completed\n",
      "total reward 2287\n",
      "loss 7.821122169494629\n",
      "steps done 1150840\n",
      "1513 episodes completed\n",
      "total reward 2278\n",
      "loss 7.730053424835205\n",
      "steps done 1151700\n",
      "1514 episodes completed\n",
      "total reward 2288\n",
      "loss 8.527430534362793\n",
      "steps done 1152440\n",
      "1515 episodes completed\n",
      "total reward 1996\n",
      "loss 7.19930362701416\n",
      "steps done 1153040\n",
      "1516 episodes completed\n",
      "total reward 2677\n",
      "loss 8.079599380493164\n",
      "steps done 1153860\n",
      "1517 episodes completed\n",
      "total reward 2732\n",
      "loss 7.4596638679504395\n",
      "steps done 1154700\n",
      "1518 episodes completed\n",
      "total reward 1796\n",
      "loss 6.648033618927002\n",
      "steps done 1155200\n",
      "1519 episodes completed\n",
      "total reward 3152\n",
      "loss 7.519290447235107\n",
      "steps done 1156360\n",
      "1520 episodes completed\n",
      "total reward 2484\n",
      "loss 8.055002212524414\n",
      "steps done 1156980\n",
      "1521 episodes completed\n",
      "total reward 2109\n",
      "loss 9.141592979431152\n",
      "steps done 1157610\n",
      "1522 episodes completed\n",
      "total reward 1815\n",
      "loss 7.525001525878906\n",
      "steps done 1158190\n",
      "1523 episodes completed\n",
      "total reward 1755\n",
      "loss 6.353660583496094\n",
      "steps done 1158830\n",
      "1524 episodes completed\n",
      "total reward 1318\n",
      "loss 9.561234474182129\n",
      "steps done 1159400\n",
      "update target\n",
      "1525 episodes completed\n",
      "total reward 1828\n",
      "loss 8.773117065429688\n",
      "steps done 1160080\n",
      "1526 episodes completed\n",
      "total reward 2356\n",
      "loss 8.998125076293945\n",
      "steps done 1160870\n",
      "1527 episodes completed\n",
      "total reward 2170\n",
      "loss 8.273822784423828\n",
      "steps done 1161520\n",
      "1528 episodes completed\n",
      "total reward 1617\n",
      "loss 8.327625274658203\n",
      "steps done 1162090\n",
      "1529 episodes completed\n",
      "total reward 2173\n",
      "loss 9.546371459960938\n",
      "steps done 1162790\n",
      "1530 episodes completed\n",
      "total reward 1634\n",
      "loss 6.505690574645996\n",
      "steps done 1163270\n",
      "1531 episodes completed\n",
      "total reward 1707\n",
      "loss 7.6463727951049805\n",
      "steps done 1163920\n",
      "1532 episodes completed\n",
      "total reward 2265\n",
      "loss 7.067448139190674\n",
      "steps done 1164590\n",
      "1533 episodes completed\n",
      "total reward 2285\n",
      "loss 8.456563949584961\n",
      "steps done 1165270\n",
      "1534 episodes completed\n",
      "total reward 2125\n",
      "loss 7.829566955566406\n",
      "steps done 1166250\n",
      "1535 episodes completed\n",
      "total reward 2234\n",
      "loss 7.172483444213867\n",
      "steps done 1166960\n",
      "1536 episodes completed\n",
      "total reward 1957\n",
      "loss 9.411788940429688\n",
      "steps done 1167600\n",
      "1537 episodes completed\n",
      "total reward 1913\n",
      "loss 8.031855583190918\n",
      "steps done 1168330\n",
      "1538 episodes completed\n",
      "total reward 1907\n",
      "loss 8.683917999267578\n",
      "steps done 1168980\n",
      "1539 episodes completed\n",
      "total reward 2215\n",
      "loss 8.609195709228516\n",
      "steps done 1169620\n",
      "update target\n",
      "1540 episodes completed\n",
      "total reward 2318\n",
      "loss 7.387733459472656\n",
      "steps done 1170290\n",
      "1541 episodes completed\n",
      "total reward 3073\n",
      "loss 8.767763137817383\n",
      "steps done 1171220\n",
      "1542 episodes completed\n",
      "total reward 2395\n",
      "loss 8.040709495544434\n",
      "steps done 1171980\n",
      "1543 episodes completed\n",
      "total reward 2399\n",
      "loss 7.801537990570068\n",
      "steps done 1172730\n",
      "1544 episodes completed\n",
      "total reward 2015\n",
      "loss 7.33496618270874\n",
      "steps done 1173310\n",
      "1545 episodes completed\n",
      "total reward 1782\n",
      "loss 6.839447975158691\n",
      "steps done 1174050\n",
      "1546 episodes completed\n",
      "total reward 2177\n",
      "loss 7.431066513061523\n",
      "steps done 1174810\n",
      "1547 episodes completed\n",
      "total reward 2300\n",
      "loss 8.834155082702637\n",
      "steps done 1175530\n",
      "1548 episodes completed\n",
      "total reward 2374\n",
      "loss 8.915972709655762\n",
      "steps done 1176170\n",
      "1549 episodes completed\n",
      "total reward 2492\n",
      "loss 7.810112476348877\n",
      "steps done 1176960\n",
      "1550 episodes completed\n",
      "total reward 2661\n",
      "loss 6.748939037322998\n",
      "steps done 1177860\n",
      "1551 episodes completed\n",
      "total reward 2422\n",
      "loss 7.159965515136719\n",
      "steps done 1178700\n",
      "1552 episodes completed\n",
      "total reward 2383\n",
      "loss 7.948821067810059\n",
      "steps done 1179630\n",
      "update target\n",
      "1553 episodes completed\n",
      "total reward 1532\n",
      "loss 8.520875930786133\n",
      "steps done 1180300\n",
      "1554 episodes completed\n",
      "total reward 1947\n",
      "loss 7.325962066650391\n",
      "steps done 1180890\n",
      "1555 episodes completed\n",
      "total reward 2020\n",
      "loss 8.107223510742188\n",
      "steps done 1181670\n",
      "1556 episodes completed\n",
      "total reward 1845\n",
      "loss 9.375495910644531\n",
      "steps done 1182320\n",
      "1557 episodes completed\n",
      "total reward 2976\n",
      "loss 7.246706485748291\n",
      "steps done 1183270\n",
      "1558 episodes completed\n",
      "total reward 2155\n",
      "loss 7.3017778396606445\n",
      "steps done 1184150\n",
      "1559 episodes completed\n",
      "total reward 1668\n",
      "loss 8.58807373046875\n",
      "steps done 1184690\n",
      "1560 episodes completed\n",
      "total reward 1799\n",
      "loss 6.288459777832031\n",
      "steps done 1185210\n",
      "1561 episodes completed\n",
      "total reward 2178\n",
      "loss 8.787362098693848\n",
      "steps done 1185840\n",
      "1562 episodes completed\n",
      "total reward 2151\n",
      "loss 8.072782516479492\n",
      "steps done 1186460\n",
      "1563 episodes completed\n",
      "total reward 2681\n",
      "loss 7.666146755218506\n",
      "steps done 1187190\n",
      "1564 episodes completed\n",
      "total reward 2128\n",
      "loss 9.28316879272461\n",
      "steps done 1187780\n",
      "1565 episodes completed\n",
      "total reward 2509\n",
      "loss 7.682221412658691\n",
      "steps done 1188550\n",
      "1566 episodes completed\n",
      "total reward 2063\n",
      "loss 6.295531749725342\n",
      "steps done 1189330\n",
      "update target\n",
      "1567 episodes completed\n",
      "total reward 2213\n",
      "loss 8.107688903808594\n",
      "steps done 1190020\n",
      "1568 episodes completed\n",
      "total reward 1498\n",
      "loss 9.389861106872559\n",
      "steps done 1190500\n",
      "1569 episodes completed\n",
      "total reward 1645\n",
      "loss 8.982344627380371\n",
      "steps done 1191090\n",
      "1570 episodes completed\n",
      "total reward 2071\n",
      "loss 7.949686527252197\n",
      "steps done 1191780\n",
      "1571 episodes completed\n",
      "total reward 2226\n",
      "loss 6.004884243011475\n",
      "steps done 1192540\n",
      "1572 episodes completed\n",
      "total reward 3594\n",
      "loss 8.475242614746094\n",
      "steps done 1193640\n",
      "1573 episodes completed\n",
      "total reward 2871\n",
      "loss 6.529296875\n",
      "steps done 1194590\n",
      "1574 episodes completed\n",
      "total reward 2339\n",
      "loss 8.358787536621094\n",
      "steps done 1195200\n",
      "1575 episodes completed\n",
      "total reward 2179\n",
      "loss 7.563990116119385\n",
      "steps done 1195820\n",
      "1576 episodes completed\n",
      "total reward 2194\n",
      "loss 8.131930351257324\n",
      "steps done 1196490\n",
      "1577 episodes completed\n",
      "total reward 1665\n",
      "loss 7.188239097595215\n",
      "steps done 1196960\n",
      "1578 episodes completed\n",
      "total reward 2365\n",
      "loss 6.565479755401611\n",
      "steps done 1197660\n",
      "1579 episodes completed\n",
      "total reward 2049\n",
      "loss 7.935760974884033\n",
      "steps done 1198240\n",
      "1580 episodes completed\n",
      "total reward 2025\n",
      "loss 7.610179424285889\n",
      "steps done 1198790\n",
      "1581 episodes completed\n",
      "total reward 2586\n",
      "loss 5.9826154708862305\n",
      "steps done 1199670\n",
      "update target\n",
      "1582 episodes completed\n",
      "total reward 2779\n",
      "loss 10.264845848083496\n",
      "steps done 1200650\n",
      "1583 episodes completed\n",
      "total reward 2140\n",
      "loss 7.383709907531738\n",
      "steps done 1201280\n",
      "1584 episodes completed\n",
      "total reward 2867\n",
      "loss 7.466132164001465\n",
      "steps done 1202350\n",
      "1585 episodes completed\n",
      "total reward 2492\n",
      "loss 6.150033950805664\n",
      "steps done 1203160\n",
      "1586 episodes completed\n",
      "total reward 2140\n",
      "loss 6.576931953430176\n",
      "steps done 1203720\n",
      "1587 episodes completed\n",
      "total reward 2295\n",
      "loss 7.778445720672607\n",
      "steps done 1204390\n",
      "1588 episodes completed\n",
      "total reward 1532\n",
      "loss 8.193000793457031\n",
      "steps done 1204940\n",
      "1589 episodes completed\n",
      "total reward 2130\n",
      "loss 7.479297637939453\n",
      "steps done 1205610\n",
      "1590 episodes completed\n",
      "total reward 2256\n",
      "loss 7.272617340087891\n",
      "steps done 1206350\n",
      "1591 episodes completed\n",
      "total reward 2452\n",
      "loss 6.037295341491699\n",
      "steps done 1207110\n",
      "1592 episodes completed\n",
      "total reward 2564\n",
      "loss 5.93043851852417\n",
      "steps done 1207990\n",
      "1593 episodes completed\n",
      "total reward 1912\n",
      "loss 6.991300106048584\n",
      "steps done 1208790\n",
      "1594 episodes completed\n",
      "total reward 2504\n",
      "loss 8.45566177368164\n",
      "steps done 1209520\n",
      "update target\n",
      "1595 episodes completed\n",
      "total reward 2215\n",
      "loss 8.554075241088867\n",
      "steps done 1210210\n",
      "1596 episodes completed\n",
      "total reward 2686\n",
      "loss 8.736966133117676\n",
      "steps done 1211160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_dqn.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   \u001b[39m# Initialize the environment and state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   \u001b[39m#env.reset()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   observation \u001b[39m=\u001b[39m fireEnv\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   state_vector_1 \u001b[39m=\u001b[39m dronesEnv\u001b[39m.\u001b[39mdrones[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstate\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   map_1 \u001b[39m=\u001b[39m dronesEnv\u001b[39m.\u001b[39mdrones[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mobservation\n",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_dqn.ipynb Cell 13\u001b[0m in \u001b[0;36mAbstractFireEnv.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_observation()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation\n",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_dqn.ipynb Cell 13\u001b[0m in \u001b[0;36mProbabilisticFireEnv.next_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m           pnm \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mpnmkl)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m       pmn \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m pnm\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m       probability_map[row, col] \u001b[39m=\u001b[39m pmn\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation[probability_map \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(HEIGHT,WIDTH)]  \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_dqn.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(HEIGHT, WIDTH)\n",
    "dronesEnv = DronesEnv(HEIGHT, WIDTH, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "seed, observation = fireEnv.reset()\n",
    "dronesEnv.reset(seed, observation)\n",
    "\n",
    "total_reward = 0 \n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "\n",
    "  observation = fireEnv.step()\n",
    "\n",
    "  state_vector_1 = dronesEnv.drones[0].state\n",
    "  map_1 = dronesEnv.drones[0].observation\n",
    "  state_vector_1 = torch.tensor(state_vector_1, dtype=torch.float)\n",
    "  map_1 = torch.tensor(map_1, dtype=torch.float)\n",
    "\n",
    "  state_vector_2 = dronesEnv.drones[1].state\n",
    "  map_2 = dronesEnv.drones[1].observation\n",
    "  state_vector_2 = torch.tensor(state_vector_2, dtype=torch.float)\n",
    "  map_2 = torch.tensor(map_2, dtype=torch.float)\n",
    "\n",
    "  for i in range(int(DT/DTI)):\n",
    "    action1 = select_action(map_1, state_vector_1, steps)\n",
    "    action2 = select_action(map_2, state_vector_2, steps)\n",
    "    steps += 2\n",
    "    rewards = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "    next_state_vector_1 = dronesEnv.drones[0].state\n",
    "    next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "    next_state_vector_1 = torch.tensor(next_state_vector_1, dtype=torch.float)\n",
    "    next_map_1 = torch.tensor(next_map_1, dtype=torch.float)\n",
    "\n",
    "    next_state_vector_2 = dronesEnv.drones[1].state\n",
    "    next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "    next_state_vector_2 = torch.tensor(next_state_vector_2, dtype=torch.float)\n",
    "    next_map_2 = torch.tensor(next_map_2, dtype=torch.float)\n",
    "\n",
    "    total_reward += sum(rewards)\n",
    "\n",
    "    reward_1 = torch.tensor([rewards[0]])\n",
    "    reward_2 = torch.tensor([rewards[1]])  \n",
    "\n",
    "    memory.push(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1)\n",
    "    memory.push(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2)\n",
    "\n",
    "    state_vector_1 = next_state_vector_1\n",
    "    state_vector_2 = next_state_vector_2\n",
    "\n",
    "    map_1 = next_map_1\n",
    "    map_2 = next_map_2\n",
    "\n",
    "  if not fireEnv.fire_in_range(6):\n",
    "\n",
    "    print(f'{i_episode} episodes completed')\n",
    "    print(f'total reward {total_reward}')\n",
    "    print(f'loss {loss}')\n",
    "    print(f'steps done {steps}')\n",
    "\n",
    "    seed, observation = fireEnv.reset()\n",
    "    dronesEnv.reset(seed, observation)\n",
    "    i_episode +=1\n",
    "    total_reward = 0\n",
    "\n",
    "    if i_episode % SAVE_NETWORKS == 0:\n",
    "      torch.save(policy_net.state_dict(), policy_file_path)\n",
    "      torch.save(target_net.state_dict(), target_file_path)\n",
    "\n",
    "  if steps>=INIT_SIZE and steps % STEPS_TILL_UPDATE==0:\n",
    "    loss = optimize_model(memory.sample(BATCH_SIZE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
