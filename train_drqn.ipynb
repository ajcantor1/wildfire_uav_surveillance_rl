{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T02:07:51.891825Z",
     "iopub.status.busy": "2023-01-16T02:07:51.891379Z",
     "iopub.status.idle": "2023-01-16T02:07:54.806349Z",
     "shell.execute_reply": "2023-01-16T02:07:54.805358Z",
     "shell.execute_reply.started": "2023-01-16T02:07:51.891746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgpath2mpl in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T02:07:54.808376Z",
     "iopub.status.busy": "2023-01-16T02:07:54.808094Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "SEQ_LENGTH = 30\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 25000\n",
    "INIT_SIZE = 70\n",
    "TARGET_UPDATE  = 1000\n",
    "SAVE_NETWORKS = 100\n",
    "EPISODE_LENGTH = 360\n",
    "TAU = 0.005\n",
    "HEIGHT = WIDTH = 100\n",
    "CHANNELS = 2\n",
    "N_ACTIONS = 2\n",
    "STATE_VECTOR_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import  deque, namedtuple\n",
    "import random\n",
    "Transition = namedtuple('Transition',('belief_map', 'state_vector', 'action', 'next_belief_map', 'next_state_vector', 'reward'))\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "  def __init__(self, capacity):\n",
    "    self.sequences = deque([], maxlen=capacity)\n",
    "\n",
    "  def push(self, sequence):\n",
    "\n",
    "    self.sequences.append([Transition(**transition._asdict()) for transition in sequence])\n",
    "\n",
    "  def save(self):\n",
    "\n",
    "\n",
    "    belief_maps = np.zeros((len(self.sequences), SEQ_LENGTH, CHANNELS, HEIGHT, WIDTH), dtype=np.float32)\n",
    "    state_vectors = np.zeros((len(self.sequences), SEQ_LENGTH, STATE_VECTOR_SIZE), dtype=np.float32)\n",
    "    actions = np.zeros((len(self.sequences), SEQ_LENGTH), dtype=np.int64)\n",
    "    next_belief_maps = np.zeros((len(self.sequences), SEQ_LENGTH, CHANNELS, HEIGHT, WIDTH), dtype=np.float32)\n",
    "    next_state_vectors = np.zeros((len(self.sequences), SEQ_LENGTH, STATE_VECTOR_SIZE), dtype=np.float32)\n",
    "    rewards = np.zeros((len(self.sequences), SEQ_LENGTH), dtype=np.int64)\n",
    "\n",
    "    for idx, sequence in enumerate(self.sequences):\n",
    "\n",
    "      transitions = Transition(*zip(*sequence))\n",
    "\n",
    "      belief_maps[idx, :] = transitions.belief_map\n",
    "      state_vectors[idx, :] = transitions.state_vector\n",
    "      actions[idx, :] = transitions.action\n",
    "      next_belief_maps[idx, :] = transitions.next_belief_map\n",
    "      next_state_vectors[idx, :] = transitions.next_state_vector\n",
    "      rewards[idx, :] = transitions.reward\n",
    "\n",
    "    np.savez_compressed('./drqn_replay_memory',\n",
    "      belief_maps=belief_maps,\n",
    "      state_vectors=state_vectors,\n",
    "      actions=actions,\n",
    "      next_belief_maps=next_belief_maps,\n",
    "      next_state_vectors=next_state_vectors,\n",
    "      rewards=rewards\n",
    "    )\n",
    "\n",
    "  def load(self):\n",
    "\n",
    "    data = np.load('./drqn_replay_memory')\n",
    "\n",
    "    total_length = data['belief_maps'].shape[0]\n",
    "    sequence_length = data['belief_maps'].shape[1]\n",
    "\n",
    "    Transition = namedtuple('Transition',('belief_map', 'state_vector', 'action', 'next_belief_map', 'next_state_vector', 'reward'))\n",
    "    for i in range(total_length):\n",
    "      sequence = []\n",
    "      belief_maps = data['belief_maps']\n",
    "      state_vectors = data['state_vectors']\n",
    "      actions = data['actions']\n",
    "      next_belief_maps = data['next_belief_maps']\n",
    "      next_state_vectors = data['next_state_vectors']\n",
    "      rewards = data['rewards']\n",
    "\n",
    "      for j in range(sequence_length):\n",
    "        sequence.append(Transition(belief_maps[i, j, :], state_vectors[i, j, :], actions[i,j], next_belief_maps[i, j, :], next_state_vectors[i, j, :], rewards[i, j, :]))\n",
    "      self.sequences.append(sequence)\n",
    "      \n",
    "\n",
    "  def sample(self, batch_size):\n",
    "\n",
    "    belief_maps = np.zeros((batch_size, SEQ_LENGTH, CHANNELS, HEIGHT, WIDTH), dtype=np.float32)\n",
    "    state_vectors = np.zeros((batch_size, SEQ_LENGTH, STATE_VECTOR_SIZE), dtype=np.float32)\n",
    "    actions = np.zeros((batch_size, SEQ_LENGTH), dtype=np.int64)\n",
    "    next_belief_maps = np.zeros((batch_size, SEQ_LENGTH, CHANNELS, HEIGHT, WIDTH), dtype=np.float32)\n",
    "    next_state_vectors = np.zeros((batch_size, SEQ_LENGTH, STATE_VECTOR_SIZE), dtype=np.float32)\n",
    "    rewards = np.zeros((batch_size, SEQ_LENGTH), dtype=np.int64)\n",
    "    \n",
    "    sampled_sequences = random.sample(self.sequences, batch_size)\n",
    "\n",
    "    for idx, sequence in enumerate(sampled_sequences):\n",
    "\n",
    "      transitions = Transition(*zip(*sequence))\n",
    "\n",
    "      belief_maps[idx, :] = transitions.belief_map\n",
    "      state_vectors[idx, :] = transitions.state_vector\n",
    "      actions[idx, :] = transitions.action\n",
    "      next_belief_maps[idx, :] = transitions.next_belief_map\n",
    "      next_state_vectors[idx, :] = transitions.next_state_vector\n",
    "      rewards[idx, :] = transitions.reward\n",
    "\n",
    "    return torch.tensor(belief_maps),\\\n",
    "      torch.tensor(state_vectors),\\\n",
    "      torch.tensor(actions),\\\n",
    "      torch.tensor(next_belief_maps),\\\n",
    "      torch.tensor(next_state_vectors),\\\n",
    "      torch.tensor(rewards)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "class AbstractFireEnv(metaclass = ABCMeta):\n",
    "\n",
    "  def __init__(self, _height, _width):\n",
    "    self._height = _height\n",
    "    self._width  = _width\n",
    "    self._time_steps = 0\n",
    "    self._observation = None\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "  \n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @property \n",
    "  def time_steps(self):\n",
    "    return self._time_steps\n",
    "\n",
    "  @time_steps.setter\n",
    "  def time_steps(self, _time_steps):\n",
    "    self._time_steps = _time_steps\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return self._observation\n",
    "\n",
    "  @observation.setter\n",
    "  def observation(self, _observation):\n",
    "    self._observation = _observation\n",
    "\n",
    "  def step(self):\n",
    "    self._time_steps += 1\n",
    "    self.observation = self.next_observation()\n",
    "    return self.observation\n",
    "\n",
    "  def plot_heat_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)  \n",
    "    heat_map_plot = ax.imshow(self.observation, cmap='hot')\n",
    "    return heat_map_plot\n",
    "\n",
    "  def reset(self):\n",
    "    self._time_steps = 0\n",
    "    self.observation = self.reset_observation()\n",
    "    seed = self.observation.copy()\n",
    "    for _ in range(30):\n",
    "      self.step()\n",
    "    return seed, self.observation\n",
    "\n",
    "  def fire_in_range(self,margin=2):\n",
    "    burnX, burnY = np.where(self.observation==1)\n",
    "    return min(burnX)>=margin and min(burnY)>=margin and max(burnX)<=99-margin and max(burnY)<=99-margin\n",
    "\n",
    "  @abstractmethod\n",
    "  def next_observation(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def reset_observation(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "D = 2\n",
    "K = 0.05\n",
    "\n",
    "def getNeighbors(point):\n",
    "    neighbors = []\n",
    "    min_x = max(0, point[1]-D)\n",
    "    max_x = min(99, point[1]+D)\n",
    "    min_y = max(0, point[0]-D)\n",
    "    max_y = min(99, point[0]+D)\n",
    "\n",
    "    for y in range(min_y, max_y): \n",
    "      for x in range(min_x, max_x):\n",
    "        neighbors.append((y, x))\n",
    "    return neighbors\n",
    "\n",
    "class ProbabilisticFireEnv(AbstractFireEnv):\n",
    "\n",
    "  def next_observation(self):\n",
    "\n",
    "    probability_map = np.zeros(shape=(HEIGHT,WIDTH), dtype=float)\n",
    "    for row in range(self.height):\n",
    "      for col in range(self.width):\n",
    "        if self.observation[row,col] == 1:\n",
    "          if self.fuel[row, col] > 0:\n",
    "            self.fuel[row, col] -= 1\n",
    "          else:\n",
    "            self.observation[row,col] = 0\n",
    "\n",
    "        elif self.observation[row,col] == 0 and self.fuel[row, col] > 0:\n",
    "          neighboring_cells = getNeighbors((row, col))\n",
    "          pnm = 1\n",
    "          for neighboring_cell in neighboring_cells:\n",
    "            if self.observation[neighboring_cell] == 1:\n",
    "              dnmkl = np.array([a-b for a, b in zip(neighboring_cell, (row,col))])\n",
    "              norm = np.sum(dnmkl**2)\n",
    "              pnmkl0 = K/norm\n",
    "              pnmklw = K*(dnmkl @ self.wind)/norm \n",
    "              pnmkl  = max(0, min(1, (pnmkl0+pnmklw)))\n",
    "              pnm *= (1-pnmkl)\n",
    "          pmn = 1 - pnm\n",
    "          probability_map[row, col] = pmn\n",
    "\n",
    "    self.observation[probability_map > np.random.rand(HEIGHT,WIDTH)]  = 1\n",
    "\n",
    "    return self.observation\n",
    "\n",
    "  def reset_observation(self):\n",
    "    center = [49, 49]\n",
    "    self.observation = np.zeros(shape=(self.height, self.width), dtype=int)\n",
    "    self.observation[center[0]-2:center[0]+2, center[1]-2:center[1]+2] = 1\n",
    "    self.fuel = np.random.randint(low=15, high=20, size=(self.height, self.width))\n",
    "    self.wind = np.random.uniform(low=-0.25, high=0.25, size=2)\n",
    "    return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import random\n",
    "\n",
    "HEIGHT = WIDTH = 100\n",
    "C = 50\n",
    "Cx = 50\n",
    "Cy = 50\n",
    "\n",
    "VELOCITY = 2\n",
    "GRAVITY  = 0.981\n",
    "MINRANGE = 15   # Minimium initial distance from wildfire seed\n",
    "MAXRANGE = 30   # Maximum initial distance from wildfire seed\n",
    "BANK_ANGLE_DELTA  = 5\n",
    "\n",
    "plane_marker = parse_path('M 11.640625 15.0625 L 9.304688 13.015625 L 9.300781 9.621094 L 15.125 11.511719 L 15.117188 10.109375 L 9.257812 5.535156 L 9.25 2.851562 L 9.25 1.296875 C 9.253906 1.019531 9.140625 0.777344 8.960938 0.585938 C 8.738281 0.324219 8.410156 0.15625 8.039062 0.160156 C 8.027344 0.160156 8.011719 0.164062 8 0.164062 C 7.988281 0.164062 7.972656 0.160156 7.960938 0.160156 C 7.589844 0.15625 7.257812 0.324219 7.035156 0.585938 C 6.859375 0.777344 6.746094 1.019531 6.746094 1.296875 L 6.746094 2.851562 L 6.742188 5.535156 L 0.882812 10.109375 L 0.875 11.511719 L 6.699219 9.621094 L 6.691406 13.011719 L 4.359375 15.0625 L 4.355469 15.761719 L 4.628906 15.695312 L 4.628906 15.839844 L 7.511719 14.992188 L 8 14.875 L 8.484375 14.992188 L 11.371094 15.839844 L 11.375 15.695312 L 11.644531 15.761719 Z M 11.640625 15.0625 ')\n",
    "plane_marker.vertices -= plane_marker.vertices.mean(axis=0)\n",
    "#plane_marker = plane_marker.transformed(matplotlib.transforms.Affine2D().rotate_deg(180))\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "  return math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "\n",
    "def shift_matrix(matrix, x, y, padding_value=0):\n",
    "  deltaX = Cx-x\n",
    "  deltaY = Cy-y\n",
    "\n",
    "\n",
    "  if deltaX==0 and deltaY==0:\n",
    "    return matrix\n",
    "\n",
    "  return shift(matrix, (deltaY, deltaX), cval = padding_value)\n",
    "  \n",
    "\n",
    "class Drone:\n",
    "\n",
    "  def __init__(self, _droneEnv, _dt, _dti):\n",
    "    self._bank_angle = 0\n",
    "    self._droneEnv = _droneEnv\n",
    "    self._trajectory = []\n",
    "    self._otherDrone = None\n",
    "    self.dt = _dt\n",
    "    self.dti = _dti\n",
    "\n",
    "  def reset(self):\n",
    "    radius = random.random()*(MAXRANGE-MINRANGE) + MINRANGE\n",
    "    angle = (random.random()-0.5)*2*np.pi\n",
    "    self._x = radius*np.cos(angle) + 50\n",
    "    self._y = radius*np.sin(angle) + 50\n",
    "    self._bank_angle = 0\n",
    "    self._trajectory = [(self.x, self.y)]\n",
    "    self._heading_angle = (random.random()-0.5)*2*np.pi\n",
    "\n",
    "  @property\n",
    "  def otherDrone(self):\n",
    "    return self._otherDrone\n",
    "\n",
    "  @otherDrone.setter\n",
    "  def otherDrone(self, _otherDrone):\n",
    "    self._otherDrone = _otherDrone\n",
    "\n",
    "  @property\n",
    "  def trajectory(self):\n",
    "    return self._trajectory\n",
    "\n",
    "  @trajectory.setter\n",
    "  def trajectory(self, _trajectory):\n",
    "    self._trajectory = _trajectory\n",
    "\n",
    "  @property\n",
    "  def x(self):\n",
    "    return self._x\n",
    "\n",
    "  @x.setter\n",
    "  def x(self, _x):\n",
    "    self._x = _x\n",
    "\n",
    "  @property\n",
    "  def y(self):\n",
    "    return self._y\n",
    "\n",
    "  @y.setter\n",
    "  def y(self, _y):\n",
    "    self._y = _y\n",
    "\n",
    "  @property\n",
    "  def mask(self):\n",
    "    Y, X = np.ogrid[:HEIGHT, :WIDTH]\n",
    "    dist_from_center = np.sqrt((X - self.x)**2 + (Y-self.y)**2)\n",
    "    mask = dist_from_center <= self._droneEnv.scan_radius\n",
    "    return mask\n",
    "\n",
    "\n",
    "  @property\n",
    "  def bank_angle(self):\n",
    "    return self._bank_angle\n",
    "\n",
    "  @bank_angle.setter\n",
    "  def bank_angle(self, _bank_angle):\n",
    "    self._bank_angle = _bank_angle\n",
    "\n",
    "  @property\n",
    "  def heading_angle(self):\n",
    "    return self._heading_angle\n",
    "\n",
    "  @heading_angle.setter\n",
    "  def heading_angle(self, _heading_angle):\n",
    "    self._heading_angle = _heading_angle\n",
    "  \n",
    "  @property\n",
    "  def rho(self):\n",
    "    return euclidean_distance(self.x, self.y, self.otherDrone.x, self.otherDrone.y)\n",
    "\n",
    "  @property\n",
    "  def theta(self):\n",
    "    _theta = np.arctan2((self.otherDrone.y-self.y),(self.otherDrone.x-self.x)) - self.heading_angle\n",
    "    \n",
    "    if (_theta > math.pi):\n",
    "      _theta -= 2*math.pi\n",
    "    elif (_theta<-math.pi):\n",
    "      _theta+= 2*math.pi\n",
    "\n",
    "    return _theta\n",
    "\n",
    "  @property\n",
    "  def psi(self):\n",
    "    _psi = self.otherDrone.heading_angle - self.heading_angle\n",
    "\n",
    "    if (_psi > math.pi):\n",
    "      _psi -= 2*math.pi\n",
    "    elif (_psi<-math.pi):\n",
    "      _psi += 2*math.pi\n",
    "\n",
    "    return _psi\n",
    "    \n",
    "  @property\n",
    "  def state(self):\n",
    "    return np.array([\n",
    "        self.bank_angle, \n",
    "        self.rho,\n",
    "        self.theta,\n",
    "        self.psi,\n",
    "        self.otherDrone.bank_angle\n",
    "    ])\n",
    "\n",
    "  @property\n",
    "  def belief_map(self):\n",
    "    return self._transform_map(self._droneEnv.belief_map_channel.copy())\n",
    "  \n",
    "  @property\n",
    "  def time_elasped_map(self):\n",
    "    return self._transform_map(self._droneEnv.time_map_channel.copy(), 250.0)/250.0\n",
    "\n",
    "  def _transform_map(self, map, padding_value=0):\n",
    "    return rotate(shift_matrix(map, self.x, self.y, padding_value), angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value)\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return np.stack((self.time_elasped_map, self.belief_map), axis=0)\n",
    "    \n",
    "  def step(self, input):\n",
    "\n",
    "    self.x +=  VELOCITY*math.cos(self.heading_angle)\n",
    "    self.y +=  VELOCITY*math.sin(self.heading_angle)\n",
    "    self.trajectory.append((self.x, self.y))  \n",
    "    self.heading_angle += GRAVITY*np.tan(self.bank_angle)/(VELOCITY)\n",
    "\n",
    "    if (self.heading_angle>np.pi):\n",
    "      self.heading_angle-=2*np.pi\n",
    "    elif (self.heading_angle<-math.pi):\n",
    "      self.heading_angle+=2*np.pi\n",
    "\n",
    "    action =  5.0*np.pi/180.0 if input==1 else -5.0*np.pi/180.0\n",
    "    self.bank_angle += action\n",
    "    \n",
    "\n",
    "    if self.bank_angle >  50.0*np.pi/180.0 or self.bank_angle < -50.0*np.pi/180.0:\n",
    "      self.bank_angle -= action\n",
    "\n",
    "  @property\n",
    "  def reward(self):\n",
    "\n",
    "    return self._reward1()+self._reward2()+self._reward3()+self._reward4()\n",
    "      \n",
    "\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self.time_elasped_map*250.0, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot \n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self.belief_map, cmap='gray_r', vmin=0, vmax=1)  \n",
    "    return belief_map_plot\n",
    "\n",
    "\n",
    "class DronesEnv:\n",
    "  def __init__(self, _height, _width, _dt, _dti, _scan_radius=10):\n",
    "    self._drones = [Drone(self, _dt, _dti), Drone(self, _dt, _dti)]\n",
    "    self._drones[0].otherDrone = self._drones[1]\n",
    "    self._drones[1].otherDrone = self._drones[0]\n",
    "    self._height = _height \n",
    "    self._width  = _width\n",
    "    self._scan_radius = _scan_radius\n",
    "\n",
    "  @property \n",
    "  def scan_radius(self):\n",
    "    return self._scan_radius\n",
    "\n",
    "  @scan_radius.setter\n",
    "  def scan_radius(self, _scan_radius):\n",
    "    self._scan_radius = _scan_radius\n",
    "        \n",
    "  def reset(self, seed, fireMap):\n",
    "\n",
    "\n",
    "    self._drones[0].reset()\n",
    "    self._drones[1].reset()\n",
    "\n",
    "    self._belief_map_channel = seed\n",
    "    self._time_elapsed_channel = np.full(shape=(self._height, self._width), fill_value=250)\n",
    "    self._scan(fireMap)\n",
    "\n",
    "\n",
    "  def _reward(self, drone, fireMap):\n",
    "    return np.count_nonzero(drone.mask & (self._belief_map_channel==0) & (fireMap==1))\n",
    "\n",
    "  def _scan(self, fireMap):\n",
    "\n",
    "    mask = self.drones[0].mask | self.drones[1].mask\n",
    "\n",
    "    self._belief_map_channel[mask] = fireMap[mask]\n",
    "    self._time_elapsed_channel[mask] = 0\n",
    "    self._time_elapsed_channel[~mask & (self._time_elapsed_channel < 250)] += 1\n",
    "\n",
    "\n",
    "  @property \n",
    "  def belief_map_channel(self):\n",
    "    return self._belief_map_channel\n",
    "\n",
    "  @belief_map_channel.setter\n",
    "  def belief_map_channel(self, _belief_map_channel):\n",
    "    self._belief_map_channel = _belief_map_channel\n",
    "\n",
    "  @property \n",
    "  def time_map_channel(self):\n",
    "    return self._time_elapsed_channel\n",
    "\n",
    "  @time_map_channel.setter\n",
    "  def time_map_channel(self, _time_elapsed_channel):\n",
    "    self._time_elapsed_channel = _time_elapsed_channel\n",
    "\n",
    "  @property\n",
    "  def drones(self):\n",
    "    return self._drones\n",
    "\n",
    "  def step(self, input, fireMap):\n",
    "    \n",
    "    rewards = []\n",
    "\n",
    "    for move, drone in zip(input,self.drones):\n",
    "      drone.step(move) \n",
    "      rewards.append(self._reward(drone, fireMap))\n",
    "\n",
    "    self._scan(fireMap)\n",
    "    return rewards\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self._time_elapsed_channel, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot\n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self._belief_map_channel, cmap='gray_r', vmin=0, vmax=1)\n",
    "    return belief_map_plot   \n",
    "\n",
    "  def plot_drones(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self.drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self.drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x1 = self._drones[0].x + np.cos(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "    y1 = self._drones[0].y + np.sin(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x2 = self._drones[1].x + np.cos(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    y2 = self._drones[1].y + np.sin(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    \n",
    "    ax.plot(x1, y1, '--')\n",
    "    ax.plot(x2, y2, '--')\n",
    "\n",
    "  def plot_trajectory(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self._drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self._drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    x1, y1 = zip(*self.drones[0].trajectory)\n",
    "    x2, y2 = zip(*self.drones[1].trajectory)\n",
    "\n",
    "    ax.plot(x1, y1, '.')\n",
    "    ax.plot(x2, y2, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class BaseDQN(nn.Module):\n",
    "\n",
    "  def _get_conv_out(self):\n",
    "    o = self.conv(torch.zeros((1, self.channels, self.height, self.width)))\n",
    "    return int(np.prod(o.size()))\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "\n",
    "  @height.setter\n",
    "  def height(self, _height):\n",
    "    self._height = _height\n",
    "\n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @width.setter\n",
    "  def width(self, _width):\n",
    "    self._width = _width\n",
    "\n",
    "  @property\n",
    "  def channels(self):\n",
    "    return self._channels\n",
    "\n",
    "  @channels.setter\n",
    "  def channels(self, _channels):\n",
    "    self._channels = _channels\n",
    "\n",
    "  @property\n",
    "  def outputs(self):\n",
    "    return self._outputs\n",
    "\n",
    "  @outputs.setter\n",
    "  def outputs(self, _outputs):\n",
    "    self._outputs = _outputs\n",
    "    \n",
    "  def __init__(self, _channels, _height, _width, _outputs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.channels = _channels\n",
    "    self.height = _height\n",
    "    self.width = _width\n",
    "    self.outputs = _outputs\n",
    "\n",
    "  def forward(self, belief_map, state_vector):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class DRQN(BaseDQN):\n",
    "\n",
    "  def __init__(self, channels, height, width, outputs):\n",
    "    super().__init__(channels, height, width, outputs)\n",
    "\n",
    "\n",
    "    self.fc1  = nn.Sequential(\n",
    "      nn.Linear(5, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "      nn.Conv2d(2, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "  \n",
    "    conv_out_size = self._get_conv_out()\n",
    "\n",
    "    self.fc2 = nn.Sequential(\n",
    "      nn.Linear(conv_out_size, 500),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(500, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "    self.fc3 = nn.Sequential(\n",
    "      nn.Linear(200, 200),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(200, 200),\n",
    "    )\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = 200, hidden_size = 200, batch_first=True)\n",
    "    \n",
    "    self.fc4 = nn.Linear(200, outputs)\n",
    "\n",
    "  def forward(self, belief_map, state_vector, hidden = None, training=False):\n",
    "    \n",
    "    if training:\n",
    "      state_vector = state_vector.view(-1, 5)\n",
    "      belief_map   = belief_map.view(-1, 2, 100, 100)\n",
    "  \n",
    "    fc1_out = self.fc1(state_vector)\n",
    "    conv_out = self.conv(belief_map)\n",
    "    flatten_out = torch.flatten(conv_out, 1)\n",
    "    fc2_out = self.fc2(flatten_out)\n",
    "    concatenated = torch.cat((fc1_out, fc2_out), dim=1)\n",
    "    \n",
    "    fc3_out = self.fc3(concatenated)\n",
    "    \n",
    "    if training:\n",
    "      fc3_out = fc3_out.view(BATCH_SIZE, SEQ_LENGTH, 200)\n",
    "    else: \n",
    "      fc3_out = fc3_out.view(1, 1, 200)\n",
    "\n",
    "\n",
    "    if hidden is None:\n",
    "      lstm_out, hidden_out = self.lstm(fc3_out)\n",
    "    else:\n",
    "      lstm_out, hidden_out = self.lstm(fc3_out, hidden)\n",
    "    \n",
    "\n",
    "    return self.fc4(lstm_out), hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DRQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "policy_net.load_state_dict(torch.load('./rnn_policy_weights.pt'))\n",
    "\n",
    "policy_net.train()\n",
    "steps = 0\n",
    "\n",
    "memory = ReplayMemory(4000)\n",
    "target_net = DRQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "#target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.load_state_dict(torch.load('./rnn_target_weights.pt'))\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(belief_map, state_vector, steps, hidden):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output, hidden_out = policy_net(belief_map.cuda(), state_vector.cuda(), hidden)\n",
    "  if sample > eps_threshold:\n",
    "    \n",
    "    return torch.tensor([torch.argmax(output).cpu()]), hidden_out\n",
    "  else:\n",
    "    \n",
    "    return  torch.tensor([random.randrange(2)]), hidden_out \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "    \n",
    "    belief_maps, state_vectors, actions, next_belief_maps, next_state_vectors, rewards = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    policy_output, _ = policy_net(belief_maps.cuda(), state_vectors.cuda(), training=True)\n",
    "    \n",
    "    \n",
    "    one_hot_actions = F.one_hot(torch.LongTensor(actions), 2).to(device)\n",
    "    \n",
    "    state_action_values = torch.sum(policy_output * one_hot_actions, -1)\n",
    "    \n",
    "    target_out, _ = target_net(next_belief_maps.cuda(), next_state_vectors.cuda(), training=True)\n",
    "\n",
    "    next_state_values = target_out.max(2)[0].view(BATCH_SIZE, SEQ_LENGTH).detach()\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + rewards.cuda()\n",
    "\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    target_net_state_dict = target_net.state_dict()\n",
    "    policy_net_state_dict = policy_net.state_dict()\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "    target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episodes completed\n",
      "total reward 402\n",
      "loss None\n",
      "steps done 360\n",
      "1 episodes completed\n",
      "total reward 1527\n",
      "loss None\n",
      "steps done 720\n",
      "2 episodes completed\n",
      "total reward 71\n",
      "loss 7.82175350189209\n",
      "steps done 1080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_drqn.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m map_2 \u001b[39m=\u001b[39m dronesEnv\u001b[39m.\u001b[39mdrones[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mobservation\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(DT\u001b[39m/\u001b[39mDTI)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   action1, hidden_1 \u001b[39m=\u001b[39m select_action(torch\u001b[39m.\u001b[39;49mtensor(map_1, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mtensor(state_vector_1, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), steps, hidden_1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   action2, hidden_2 \u001b[39m=\u001b[39m select_action(torch\u001b[39m.\u001b[39mtensor(map_2, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mtensor(state_vector_2, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), steps, hidden_2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EPISODE = 3000\n",
    "DT          = 0.5          \n",
    "DTI         = 0.1  \n",
    "fireEnv = ProbabilisticFireEnv(HEIGHT, WIDTH)\n",
    "dronesEnv = DronesEnv(HEIGHT, WIDTH, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 0\n",
    "\n",
    "total_reward = 0\n",
    "seed, observation = fireEnv.reset()\n",
    "dronesEnv.reset(seed, observation)\n",
    "\n",
    "sequence_1 = []\n",
    "sequence_2  = []\n",
    "sequence_i = 0\n",
    "hidden_1 = None\n",
    "hidden_2 = None\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "\n",
    "  state_vector_1 = dronesEnv.drones[0].state\n",
    "  map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "\n",
    "  state_vector_2 = dronesEnv.drones[1].state\n",
    "  map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "\n",
    "  for i in range(int(DT/DTI)):\n",
    "\n",
    "    action1, hidden_1 = select_action(torch.tensor(map_1, dtype=torch.float).unsqueeze(0), torch.tensor(state_vector_1, dtype=torch.float).unsqueeze(0), steps, hidden_1)\n",
    "    action2, hidden_2 = select_action(torch.tensor(map_2, dtype=torch.float).unsqueeze(0), torch.tensor(state_vector_2, dtype=torch.float).unsqueeze(0), steps, hidden_2)\n",
    "\n",
    "    steps += 1\n",
    "    sequence_i += 1\n",
    "    rewards = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "    next_state_vector_1 = dronesEnv.drones[0].state\n",
    "    next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "\n",
    "    next_state_vector_2 = dronesEnv.drones[1].state\n",
    "    next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "\n",
    "    total_reward += sum(rewards)\n",
    "\n",
    "    sequence_1.append(Transition(map_1, state_vector_1, action1.item(), next_map_1, next_state_vector_1, rewards[0]))\n",
    "    sequence_2.append(Transition(map_2, state_vector_2, action2.item(), next_map_2, next_state_vector_2, rewards[1]))\n",
    "\n",
    "    state_vector_1 = next_state_vector_1\n",
    "    map_1 = next_map_1\n",
    "\n",
    "    state_vector_2 = next_state_vector_2\n",
    "    map_2 = next_map_2\n",
    "\n",
    "  observation = fireEnv.step()\n",
    "\n",
    "\n",
    "\n",
    "  if len(sequence_1) == SEQ_LENGTH:\n",
    "    memory.push(sequence_1)\n",
    "    memory.push(sequence_2)\n",
    "    sequence_1 = []\n",
    "    sequence_2 = []\n",
    "    \n",
    "    if len(memory)>INIT_SIZE:\n",
    "      loss = optimize_model()\n",
    "\n",
    "\n",
    "  if steps % EPISODE_LENGTH == 0:\n",
    "    # done\n",
    "    hidden_1 = None\n",
    "    hidden_2 = None\n",
    "\n",
    "    print(f'{i_episode} episodes completed')\n",
    "    print(f'total reward {total_reward}')\n",
    "    print(f'loss {loss}')\n",
    "    print(f'steps done {steps}')\n",
    "\n",
    "    i_episode +=1\n",
    "    total_reward = 0\n",
    "\n",
    "    if i_episode % SAVE_NETWORKS == 0:\n",
    "      policy_file_path = f'./rnn_policy_weights.pt'\n",
    "      target_file_path = f'./rnn_target_weights.pt'\n",
    "      #torch.save(policy_net.state_dict(), policy_file_path)\n",
    "      #torch.save(target_net.state_dict(), target_file_path)\n",
    "\n",
    "    if i_episode > MAX_EPISODE:\n",
    "      break\n",
    "        \n",
    "    seed, observation = fireEnv.reset()\n",
    "    dronesEnv.reset(seed, observation)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
