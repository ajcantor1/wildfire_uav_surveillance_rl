{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T02:07:51.891825Z",
     "iopub.status.busy": "2023-01-16T02:07:51.891379Z",
     "iopub.status.idle": "2023-01-16T02:07:54.806349Z",
     "shell.execute_reply": "2023-01-16T02:07:54.805358Z",
     "shell.execute_reply.started": "2023-01-16T02:07:51.891746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgpath2mpl in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T02:07:54.808376Z",
     "iopub.status.busy": "2023-01-16T02:07:54.808094Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "SEQ_LENGTH = 100\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 20000\n",
    "INIT_SIZE = 200\n",
    "TARGET_UPDATE  = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import  deque, namedtuple\n",
    "import random\n",
    "Transition = namedtuple('Transition',('belief_map', 'state_vector', 'action', 'next_belief_map', 'next_state_vector', 'reward'))\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "  def __init__(self, capacity):\n",
    "    self.episodes = deque([], maxlen=capacity)\n",
    "\n",
    "  def push(self, episode):\n",
    "    self.episodes.append([Transition(**transition._asdict()) for transition in episode])\n",
    "\n",
    "  def sample(self, batch_size, sequence_length):\n",
    "    \n",
    "    belief_maps = []\n",
    "    state_vectors = []\n",
    "    actions = []\n",
    "    next_belief_maps = []\n",
    "    next_state_vectors = []\n",
    "    rewards = []\n",
    "    \n",
    "    episodes = random.sample(self.episodes, batch_size)\n",
    "\n",
    "    for episode in episodes:\n",
    "      start_idx = random.randint(0, len(episode)-SEQ_LENGTH)\n",
    "      episode_sequence = Transition(*zip(*episode[start_idx:start_idx+SEQ_LENGTH]))\n",
    "\n",
    "      belief_maps.append(np.array(episode_sequence.belief_map))\n",
    "      state_vectors.append(np.array(episode_sequence.state_vector))\n",
    "      actions.append(np.array(episode_sequence.action))\n",
    "      next_belief_maps.append(np.array(episode_sequence.next_belief_map))\n",
    "      next_state_vectors.append(np.array(episode_sequence.next_state_vector))\n",
    "      rewards.append(np.array(episode_sequence.reward)) \n",
    "\n",
    "    return torch.tensor(np.asarray(belief_maps), dtype=torch.float), torch.tensor(np.asarray(state_vectors), dtype=torch.float), torch.tensor(np.asarray(actions)), torch.tensor(np.asarray(next_belief_maps), dtype=torch.float), torch.tensor(np.asarray(next_state_vectors), dtype=torch.float), torch.tensor(np.asarray(rewards))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "class AbstractFireEnv(metaclass = ABCMeta):\n",
    "\n",
    "  def __init__(self, _height, _width):\n",
    "    self._height = _height\n",
    "    self._width  = _width\n",
    "    self._time_steps = 0\n",
    "    self._observation = None\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "  \n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @property \n",
    "  def time_steps(self):\n",
    "    return self._time_steps\n",
    "\n",
    "  @time_steps.setter\n",
    "  def time_steps(self, _time_steps):\n",
    "    self._time_steps = _time_steps\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return self._observation\n",
    "\n",
    "  @observation.setter\n",
    "  def observation(self, _observation):\n",
    "    self._observation = _observation\n",
    "\n",
    "  def step(self):\n",
    "    self._time_steps += 1\n",
    "    self.observation = self.next_observation()\n",
    "    return self.observation\n",
    "\n",
    "  def plot_heat_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)  \n",
    "    heat_map_plot = ax.imshow(self.observation, cmap='hot')\n",
    "    return heat_map_plot\n",
    "\n",
    "  def reset(self):\n",
    "    self._time_steps = 0\n",
    "    self.observation = self.reset_observation()\n",
    "    seed = self.observation.copy()\n",
    "    for _ in range(30):\n",
    "      self.step()\n",
    "    return seed, self.observation\n",
    "\n",
    "  def fire_in_range(self,margin=2):\n",
    "    burnX, burnY = np.where(self.observation==1)\n",
    "    return min(burnX)>=margin and min(burnY)>=margin and max(burnX)<=99-margin and max(burnY)<=99-margin\n",
    "\n",
    "  @abstractmethod\n",
    "  def next_observation(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def reset_observation(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "D = 2\n",
    "K = 0.05\n",
    "\n",
    "def getNeighbors(point):\n",
    "    neighbors = []\n",
    "    min_x = max(0, point[1]-D)\n",
    "    max_x = min(99, point[1]+D)\n",
    "    min_y = max(0, point[0]-D)\n",
    "    max_y = min(99, point[0]+D)\n",
    "\n",
    "    for y in range(min_y, max_y): \n",
    "      for x in range(min_x, max_x):\n",
    "        neighbors.append((y, x))\n",
    "    return neighbors\n",
    "\n",
    "class ProbabilisticFireEnv(AbstractFireEnv):\n",
    "\n",
    "  def next_observation(self):\n",
    "\n",
    "    probability_map = np.zeros(shape=(HEIGHT,WIDTH), dtype=float)\n",
    "    for row in range(self.height):\n",
    "      for col in range(self.width):\n",
    "        if self.observation[row,col] == 1:\n",
    "          if self.fuel[row, col] > 0:\n",
    "            self.fuel[row, col] -= 1\n",
    "          else:\n",
    "            self.observation[row,col] = 0\n",
    "\n",
    "        elif self.observation[row,col] == 0 and self.fuel[row, col] > 0:\n",
    "          neighboring_cells = getNeighbors((row, col))\n",
    "          pnm = 1\n",
    "          for neighboring_cell in neighboring_cells:\n",
    "            if self.observation[neighboring_cell] == 1:\n",
    "              dnmkl = np.array([a-b for a, b in zip(neighboring_cell, (row,col))])\n",
    "              norm = np.sum(dnmkl**2)\n",
    "              pnmkl0 = K/norm\n",
    "              pnmklw = K*(dnmkl @ self.wind)/norm \n",
    "              pnmkl  = max(0, min(1, (pnmkl0+pnmklw)))\n",
    "              pnm *= (1-pnmkl)\n",
    "          pmn = 1 - pnm\n",
    "          probability_map[row, col] = pmn\n",
    "\n",
    "    self.observation[probability_map > np.random.rand(HEIGHT,WIDTH)]  = 1\n",
    "\n",
    "    return self.observation\n",
    "\n",
    "  def reset_observation(self):\n",
    "    center = [49, 49]\n",
    "    self.observation = np.zeros(shape=(self.height, self.width), dtype=int)\n",
    "    self.observation[center[0]-2:center[0]+2, center[1]-2:center[1]+2] = 1\n",
    "    self.fuel = np.random.randint(low=15, high=20, size=(self.height, self.width))\n",
    "    self.wind = np.random.uniform(low=-0.25, high=0.25, size=2)\n",
    "    return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import random\n",
    "\n",
    "HEIGHT = WIDTH = 100\n",
    "C = 50\n",
    "Cx = 50\n",
    "Cy = 50\n",
    "\n",
    "VELOCITY = 2\n",
    "GRAVITY  = 0.981\n",
    "MINRANGE = 15   # Minimium initial distance from wildfire seed\n",
    "MAXRANGE = 30   # Maximum initial distance from wildfire seed\n",
    "BANK_ANGLE_DELTA  = 5\n",
    "\n",
    "plane_marker = parse_path('M 11.640625 15.0625 L 9.304688 13.015625 L 9.300781 9.621094 L 15.125 11.511719 L 15.117188 10.109375 L 9.257812 5.535156 L 9.25 2.851562 L 9.25 1.296875 C 9.253906 1.019531 9.140625 0.777344 8.960938 0.585938 C 8.738281 0.324219 8.410156 0.15625 8.039062 0.160156 C 8.027344 0.160156 8.011719 0.164062 8 0.164062 C 7.988281 0.164062 7.972656 0.160156 7.960938 0.160156 C 7.589844 0.15625 7.257812 0.324219 7.035156 0.585938 C 6.859375 0.777344 6.746094 1.019531 6.746094 1.296875 L 6.746094 2.851562 L 6.742188 5.535156 L 0.882812 10.109375 L 0.875 11.511719 L 6.699219 9.621094 L 6.691406 13.011719 L 4.359375 15.0625 L 4.355469 15.761719 L 4.628906 15.695312 L 4.628906 15.839844 L 7.511719 14.992188 L 8 14.875 L 8.484375 14.992188 L 11.371094 15.839844 L 11.375 15.695312 L 11.644531 15.761719 Z M 11.640625 15.0625 ')\n",
    "plane_marker.vertices -= plane_marker.vertices.mean(axis=0)\n",
    "#plane_marker = plane_marker.transformed(matplotlib.transforms.Affine2D().rotate_deg(180))\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "  return math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "\n",
    "def shift_matrix(matrix, x, y, padding_value=0):\n",
    "  deltaX = Cx-x\n",
    "  deltaY = Cy-y\n",
    "\n",
    "\n",
    "  if deltaX==0 and deltaY==0:\n",
    "    return matrix\n",
    "\n",
    "  return shift(matrix, (deltaY, deltaX), cval = padding_value)\n",
    "  \n",
    "\n",
    "class Drone:\n",
    "\n",
    "  def __init__(self, _droneEnv, _dt, _dti):\n",
    "    self._bank_angle = 0\n",
    "    self._droneEnv = _droneEnv\n",
    "    self._trajectory = []\n",
    "    self._otherDrone = None\n",
    "    self.dt = _dt\n",
    "    self.dti = _dti\n",
    "\n",
    "  def reset(self):\n",
    "    radius = random.random()*(MAXRANGE-MINRANGE) + MINRANGE\n",
    "    angle = (random.random()-0.5)*2*np.pi\n",
    "    self._x = radius*np.cos(angle) + 50\n",
    "    self._y = radius*np.sin(angle) + 50\n",
    "    self._bank_angle = 0\n",
    "    self._trajectory = [(self.x, self.y)]\n",
    "    self._heading_angle = (random.random()-0.5)*2*np.pi\n",
    "\n",
    "  @property\n",
    "  def otherDrone(self):\n",
    "    return self._otherDrone\n",
    "\n",
    "  @otherDrone.setter\n",
    "  def otherDrone(self, _otherDrone):\n",
    "    self._otherDrone = _otherDrone\n",
    "\n",
    "  @property\n",
    "  def trajectory(self):\n",
    "    return self._trajectory\n",
    "\n",
    "  @trajectory.setter\n",
    "  def trajectory(self, _trajectory):\n",
    "    self._trajectory = _trajectory\n",
    "\n",
    "  @property\n",
    "  def x(self):\n",
    "    return self._x\n",
    "\n",
    "  @x.setter\n",
    "  def x(self, _x):\n",
    "    self._x = _x\n",
    "\n",
    "  @property\n",
    "  def y(self):\n",
    "    return self._y\n",
    "\n",
    "  @y.setter\n",
    "  def y(self, _y):\n",
    "    self._y = _y\n",
    "\n",
    "  @property\n",
    "  def mask(self):\n",
    "    Y, X = np.ogrid[:HEIGHT, :WIDTH]\n",
    "    dist_from_center = np.sqrt((X - self.x)**2 + (Y-self.y)**2)\n",
    "    mask = dist_from_center <= self._droneEnv.scan_radius\n",
    "    return mask\n",
    "\n",
    "\n",
    "  @property\n",
    "  def bank_angle(self):\n",
    "    return self._bank_angle\n",
    "\n",
    "  @bank_angle.setter\n",
    "  def bank_angle(self, _bank_angle):\n",
    "    self._bank_angle = _bank_angle\n",
    "\n",
    "  @property\n",
    "  def heading_angle(self):\n",
    "    return self._heading_angle\n",
    "\n",
    "  @heading_angle.setter\n",
    "  def heading_angle(self, _heading_angle):\n",
    "    self._heading_angle = _heading_angle\n",
    "  \n",
    "  @property\n",
    "  def rho(self):\n",
    "    return euclidean_distance(self.x, self.y, self.otherDrone.x, self.otherDrone.y)\n",
    "\n",
    "  @property\n",
    "  def theta(self):\n",
    "    _theta = np.arctan2((self.otherDrone.y-self.y),(self.otherDrone.x-self.x)) - self.heading_angle\n",
    "    \n",
    "    if (_theta > math.pi):\n",
    "      _theta -= 2*math.pi\n",
    "    elif (_theta<-math.pi):\n",
    "      _theta+= 2*math.pi\n",
    "\n",
    "    return _theta\n",
    "\n",
    "  @property\n",
    "  def psi(self):\n",
    "    _psi = self.otherDrone.heading_angle - self.heading_angle\n",
    "\n",
    "    if (_psi > math.pi):\n",
    "      _psi -= 2*math.pi\n",
    "    elif (_psi<-math.pi):\n",
    "      _psi += 2*math.pi\n",
    "\n",
    "    return _psi\n",
    "    \n",
    "  @property\n",
    "  def state(self):\n",
    "    return np.array([\n",
    "        self.bank_angle, \n",
    "        self.rho,\n",
    "        self.theta,\n",
    "        self.psi,\n",
    "        self.otherDrone.bank_angle\n",
    "    ])\n",
    "\n",
    "  @property\n",
    "  def belief_map(self):\n",
    "    return self._transform_map(self._droneEnv.belief_map_channel.copy())\n",
    "  \n",
    "  @property\n",
    "  def time_elasped_map(self):\n",
    "    return self._transform_map(self._droneEnv.time_map_channel.copy(), 250.0)/250.0\n",
    "\n",
    "  def _transform_map(self, map, padding_value=0):\n",
    "    return rotate(shift_matrix(map, self.x, self.y, padding_value), angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value)\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return np.stack((self.time_elasped_map, self.belief_map), axis=0)\n",
    "    \n",
    "  def step(self, input):\n",
    "\n",
    "    self.x +=  VELOCITY*math.cos(self.heading_angle)\n",
    "    self.y +=  VELOCITY*math.sin(self.heading_angle)\n",
    "    self.trajectory.append((self.x, self.y))  \n",
    "    self.heading_angle += GRAVITY*np.tan(self.bank_angle)/(VELOCITY)\n",
    "\n",
    "    if (self.heading_angle>np.pi):\n",
    "      self.heading_angle-=2*np.pi\n",
    "    elif (self.heading_angle<-math.pi):\n",
    "      self.heading_angle+=2*np.pi\n",
    "\n",
    "    action =  5.0*np.pi/180.0 if input==1 else -5.0*np.pi/180.0\n",
    "    self.bank_angle += action\n",
    "    \n",
    "\n",
    "    if self.bank_angle >  50.0*np.pi/180.0 or self.bank_angle < -50.0*np.pi/180.0:\n",
    "      self.bank_angle -= action\n",
    "\n",
    "  @property\n",
    "  def reward(self):\n",
    "\n",
    "    return self._reward1()+self._reward2()+self._reward3()+self._reward4()\n",
    "      \n",
    "\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self.time_elasped_map*250.0, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot \n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self.belief_map, cmap='gray_r', vmin=0, vmax=1)  \n",
    "    return belief_map_plot\n",
    "\n",
    "\n",
    "class DronesEnv:\n",
    "  def __init__(self, _height, _width, _dt, _dti, _scan_radius=10):\n",
    "    self._drones = [Drone(self, _dt, _dti), Drone(self, _dt, _dti)]\n",
    "    self._drones[0].otherDrone = self._drones[1]\n",
    "    self._drones[1].otherDrone = self._drones[0]\n",
    "    self._height = _height \n",
    "    self._width  = _width\n",
    "    self._scan_radius = _scan_radius\n",
    "\n",
    "  @property \n",
    "  def scan_radius(self):\n",
    "    return self._scan_radius\n",
    "\n",
    "  @scan_radius.setter\n",
    "  def scan_radius(self, _scan_radius):\n",
    "    self._scan_radius = _scan_radius\n",
    "        \n",
    "  def reset(self, seed, fireMap):\n",
    "\n",
    "\n",
    "    self._drones[0].reset()\n",
    "    self._drones[1].reset()\n",
    "\n",
    "    self._belief_map_channel = seed\n",
    "    self._time_elapsed_channel = np.full(shape=(self._height, self._width), fill_value=250)\n",
    "    self._scan(fireMap)\n",
    "\n",
    "\n",
    "  def _reward(self, drone, fireMap):\n",
    "    return np.count_nonzero(drone.mask & (self._belief_map_channel==0) & (fireMap==1))\n",
    "\n",
    "  def _scan(self, fireMap):\n",
    "\n",
    "    mask = self.drones[0].mask | self.drones[1].mask\n",
    "\n",
    "    self._belief_map_channel[mask] = fireMap[mask]\n",
    "    self._time_elapsed_channel[mask] = 0\n",
    "    self._time_elapsed_channel[~mask & (self._time_elapsed_channel < 250)] += 1\n",
    "\n",
    "\n",
    "  @property \n",
    "  def belief_map_channel(self):\n",
    "    return self._belief_map_channel\n",
    "\n",
    "  @belief_map_channel.setter\n",
    "  def belief_map_channel(self, _belief_map_channel):\n",
    "    self._belief_map_channel = _belief_map_channel\n",
    "\n",
    "  @property \n",
    "  def time_map_channel(self):\n",
    "    return self._time_elapsed_channel\n",
    "\n",
    "  @time_map_channel.setter\n",
    "  def time_map_channel(self, _time_elapsed_channel):\n",
    "    self._time_elapsed_channel = _time_elapsed_channel\n",
    "\n",
    "  @property\n",
    "  def drones(self):\n",
    "    return self._drones\n",
    "\n",
    "  def step(self, input, fireMap):\n",
    "    \n",
    "    rewards = []\n",
    "\n",
    "    for move, drone in zip(input,self.drones):\n",
    "      drone.step(move) \n",
    "      rewards.append(self._reward(drone, fireMap))\n",
    "\n",
    "    self._scan(fireMap)\n",
    "    return rewards\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self._time_elapsed_channel, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot\n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self._belief_map_channel, cmap='gray_r', vmin=0, vmax=1)\n",
    "    return belief_map_plot   \n",
    "\n",
    "  def plot_drones(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self.drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self.drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x1 = self._drones[0].x + np.cos(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "    y1 = self._drones[0].y + np.sin(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x2 = self._drones[1].x + np.cos(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    y2 = self._drones[1].y + np.sin(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    \n",
    "    ax.plot(x1, y1, '--')\n",
    "    ax.plot(x2, y2, '--')\n",
    "\n",
    "  def plot_trajectory(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self._drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self._drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    x1, y1 = zip(*self.drones[0].trajectory)\n",
    "    x2, y2 = zip(*self.drones[1].trajectory)\n",
    "\n",
    "    ax.plot(x1, y1, '.')\n",
    "    ax.plot(x2, y2, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class BaseDQN(nn.Module):\n",
    "\n",
    "  def _get_conv_out(self):\n",
    "    o = self.conv(torch.zeros((1, self.channels, self.height, self.width)))\n",
    "    return int(np.prod(o.size()))\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "\n",
    "  @height.setter\n",
    "  def height(self, _height):\n",
    "    self._height = _height\n",
    "\n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @width.setter\n",
    "  def width(self, _width):\n",
    "    self._width = _width\n",
    "\n",
    "  @property\n",
    "  def channels(self):\n",
    "    return self._channels\n",
    "\n",
    "  @channels.setter\n",
    "  def channels(self, _channels):\n",
    "    self._channels = _channels\n",
    "\n",
    "  @property\n",
    "  def outputs(self):\n",
    "    return self._outputs\n",
    "\n",
    "  @outputs.setter\n",
    "  def outputs(self, _outputs):\n",
    "    self._outputs = _outputs\n",
    "    \n",
    "  def __init__(self, _channels, _height, _width, _outputs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.channels = _channels\n",
    "    self.height = _height\n",
    "    self.width = _width\n",
    "    self.outputs = _outputs\n",
    "\n",
    "  def forward(self, belief_map, state_vector):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class DRQN(BaseDQN):\n",
    "\n",
    "  def __init__(self, channels, height, width, outputs):\n",
    "    super().__init__(channels, height, width, outputs)\n",
    "\n",
    "\n",
    "    self.fc1  = nn.Sequential(\n",
    "      nn.Linear(5, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "      nn.Conv2d(2, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "  \n",
    "    conv_out_size = self._get_conv_out()\n",
    "\n",
    "    self.fc2 = nn.Sequential(\n",
    "      nn.Linear(conv_out_size, 500),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(500, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "    self.fc3 = nn.Sequential(\n",
    "      nn.Linear(200, 200),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(200, 200),\n",
    "    )\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = 200, hidden_size = 200, batch_first=True)\n",
    "    \n",
    "    self.fc4 = nn.Linear(200, outputs)\n",
    "\n",
    "  def forward(self, belief_map, state_vector, hidden = None, training=False):\n",
    "    \n",
    "    if training:\n",
    "      state_vector = state_vector.view(-1, 5)\n",
    "      belief_map   = belief_map.view(-1, 2, 100, 100)\n",
    "  \n",
    "    fc1_out = self.fc1(state_vector)\n",
    "    conv_out = self.conv(belief_map)\n",
    "    flatten_out = torch.flatten(conv_out, 1)\n",
    "    fc2_out = self.fc2(flatten_out)\n",
    "    concatenated = torch.cat((fc1_out, fc2_out), dim=1)\n",
    "    \n",
    "    fc3_out = self.fc3(concatenated)\n",
    "    \n",
    "    if training:\n",
    "      fc3_out = fc3_out.view(BATCH_SIZE, SEQ_LENGTH, 200)\n",
    "    else: \n",
    "      fc3_out = fc3_out.view(1, 1, 200)\n",
    "\n",
    "\n",
    "    if hidden is None:\n",
    "      lstm_out, hidden_out = self.lstm(fc3_out)\n",
    "    else:\n",
    "      lstm_out, hidden_out = self.lstm(fc3_out, hidden)\n",
    "    \n",
    "\n",
    "    return self.fc4(lstm_out), hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DRQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "policy_net.load_state_dict(torch.load('rnn_target_weights.pt'))\n",
    "\n",
    "#policy_net = policy_net.to(device)\n",
    "policy_net.train()\n",
    "steps = 0\n",
    "\n",
    "\n",
    "memory = ReplayMemory(1000)\n",
    "target_net = DRQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(belief_map, state_vector, steps, hidden):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output, hidden_out = policy_net(torch.tensor(belief_map, dtype=torch.float).unsqueeze(0).cuda(), torch.tensor(state_vector, dtype=torch.float).unsqueeze(0).cuda(), hidden)\n",
    "  if sample > eps_threshold:\n",
    "\n",
    "      return torch.argmax(output).cpu().item(), hidden_out\n",
    "  else:\n",
    "    return  torch.tensor([[random.randrange(2)]], dtype=torch.long).item(), hidden_out \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "    \n",
    "    belief_maps, state_vectors, actions, next_belief_maps, next_state_vectors, rewards = memory.sample(BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "    policy_output, _ = policy_net(belief_maps.cuda(), state_vectors.cuda(), training=True)\n",
    "    \n",
    "    \n",
    "    one_hot_actions = F.one_hot(torch.LongTensor(actions), 2).to(device)\n",
    "    \n",
    "    state_action_values = torch.sum(policy_output * one_hot_actions, -1)\n",
    "    \n",
    "    \n",
    "    target_out, _ = target_net(next_belief_maps.cuda(), next_state_vectors.cuda(), training=True)\n",
    "\n",
    "\n",
    "    next_state_values = target_out.max(2)[0].view(BATCH_SIZE, SEQ_LENGTH).detach()\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + rewards.cuda()\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./rnn_policy_weights.pt'\n",
    "        target_file_path = f'./rnn_target_weights.pt'\n",
    "        #torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        print('update target')\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episodes completed\n",
      "total reward 994\n",
      "loss None\n",
      "steps done 360\n",
      "2 episodes completed\n",
      "total reward 1198\n",
      "loss None\n",
      "steps done 855\n",
      "3 episodes completed\n",
      "total reward 778\n",
      "loss None\n",
      "steps done 1290\n",
      "4 episodes completed\n",
      "total reward 205\n",
      "loss None\n",
      "steps done 1580\n",
      "5 episodes completed\n",
      "total reward 227\n",
      "loss None\n",
      "steps done 1800\n",
      "6 episodes completed\n",
      "total reward 1218\n",
      "loss None\n",
      "steps done 2135\n",
      "7 episodes completed\n",
      "total reward 1015\n",
      "loss None\n",
      "steps done 2505\n",
      "8 episodes completed\n",
      "total reward 1035\n",
      "loss None\n",
      "steps done 2840\n",
      "9 episodes completed\n",
      "total reward 2130\n",
      "loss None\n",
      "steps done 3425\n",
      "10 episodes completed\n",
      "total reward 1796\n",
      "loss None\n",
      "steps done 3855\n",
      "11 episodes completed\n",
      "total reward 1860\n",
      "loss None\n",
      "steps done 4265\n",
      "12 episodes completed\n",
      "total reward 501\n",
      "loss None\n",
      "steps done 4555\n",
      "13 episodes completed\n",
      "total reward 1179\n",
      "loss None\n",
      "steps done 4860\n",
      "14 episodes completed\n",
      "total reward 2090\n",
      "loss None\n",
      "steps done 5330\n",
      "15 episodes completed\n",
      "total reward 1696\n",
      "loss None\n",
      "steps done 5815\n",
      "16 episodes completed\n",
      "total reward 1866\n",
      "loss None\n",
      "steps done 6140\n",
      "17 episodes completed\n",
      "total reward 1402\n",
      "loss None\n",
      "steps done 6630\n",
      "18 episodes completed\n",
      "total reward 2296\n",
      "loss None\n",
      "steps done 7065\n",
      "19 episodes completed\n",
      "total reward 2197\n",
      "loss None\n",
      "steps done 7495\n",
      "20 episodes completed\n",
      "total reward 1735\n",
      "loss None\n",
      "steps done 7850\n",
      "21 episodes completed\n",
      "total reward 2024\n",
      "loss None\n",
      "steps done 8210\n",
      "22 episodes completed\n",
      "total reward 2239\n",
      "loss None\n",
      "steps done 8585\n",
      "23 episodes completed\n",
      "total reward 2508\n",
      "loss None\n",
      "steps done 9080\n",
      "24 episodes completed\n",
      "total reward 2145\n",
      "loss None\n",
      "steps done 9380\n",
      "25 episodes completed\n",
      "total reward 1153\n",
      "loss None\n",
      "steps done 9575\n",
      "26 episodes completed\n",
      "total reward 2683\n",
      "loss None\n",
      "steps done 10075\n",
      "27 episodes completed\n",
      "total reward 1366\n",
      "loss None\n",
      "steps done 10345\n",
      "28 episodes completed\n",
      "total reward 1149\n",
      "loss None\n",
      "steps done 10650\n",
      "29 episodes completed\n",
      "total reward 2051\n",
      "loss None\n",
      "steps done 11040\n",
      "30 episodes completed\n",
      "total reward 2015\n",
      "loss None\n",
      "steps done 11390\n",
      "31 episodes completed\n",
      "total reward 1984\n",
      "loss None\n",
      "steps done 11730\n",
      "32 episodes completed\n",
      "total reward 1532\n",
      "loss None\n",
      "steps done 12000\n",
      "33 episodes completed\n",
      "total reward 2422\n",
      "loss None\n",
      "steps done 12540\n",
      "34 episodes completed\n",
      "total reward 2020\n",
      "loss None\n",
      "steps done 12855\n",
      "35 episodes completed\n",
      "total reward 2051\n",
      "loss None\n",
      "steps done 13285\n",
      "36 episodes completed\n",
      "total reward 1898\n",
      "loss None\n",
      "steps done 13585\n",
      "37 episodes completed\n",
      "total reward 2380\n",
      "loss None\n",
      "steps done 13995\n",
      "38 episodes completed\n",
      "total reward 1904\n",
      "loss None\n",
      "steps done 14335\n",
      "39 episodes completed\n",
      "total reward 2968\n",
      "loss None\n",
      "steps done 14800\n",
      "40 episodes completed\n",
      "total reward 2109\n",
      "loss None\n",
      "steps done 15080\n",
      "41 episodes completed\n",
      "total reward 2400\n",
      "loss None\n",
      "steps done 15495\n",
      "42 episodes completed\n",
      "total reward 1868\n",
      "loss None\n",
      "steps done 15825\n",
      "43 episodes completed\n",
      "total reward 2474\n",
      "loss None\n",
      "steps done 16240\n",
      "44 episodes completed\n",
      "total reward 2444\n",
      "loss None\n",
      "steps done 16610\n",
      "45 episodes completed\n",
      "total reward 2844\n",
      "loss None\n",
      "steps done 17120\n",
      "46 episodes completed\n",
      "total reward 2480\n",
      "loss None\n",
      "steps done 17565\n",
      "47 episodes completed\n",
      "total reward 1610\n",
      "loss None\n",
      "steps done 17790\n",
      "48 episodes completed\n",
      "total reward 2060\n",
      "loss None\n",
      "steps done 18130\n",
      "49 episodes completed\n",
      "total reward 2170\n",
      "loss None\n",
      "steps done 18540\n",
      "50 episodes completed\n",
      "total reward 1625\n",
      "loss None\n",
      "steps done 18785\n",
      "51 episodes completed\n",
      "total reward 1879\n",
      "loss None\n",
      "steps done 19095\n",
      "52 episodes completed\n",
      "total reward 2269\n",
      "loss None\n",
      "steps done 19435\n",
      "53 episodes completed\n",
      "total reward 1971\n",
      "loss None\n",
      "steps done 19775\n",
      "54 episodes completed\n",
      "total reward 2025\n",
      "loss None\n",
      "steps done 20145\n",
      "55 episodes completed\n",
      "total reward 2552\n",
      "loss None\n",
      "steps done 20525\n",
      "56 episodes completed\n",
      "total reward 1707\n",
      "loss None\n",
      "steps done 20780\n",
      "57 episodes completed\n",
      "total reward 1604\n",
      "loss None\n",
      "steps done 21015\n",
      "58 episodes completed\n",
      "total reward 1808\n",
      "loss None\n",
      "steps done 21305\n",
      "59 episodes completed\n",
      "total reward 3248\n",
      "loss None\n",
      "steps done 21830\n",
      "60 episodes completed\n",
      "total reward 1832\n",
      "loss None\n",
      "steps done 22160\n",
      "61 episodes completed\n",
      "total reward 2581\n",
      "loss None\n",
      "steps done 22645\n",
      "62 episodes completed\n",
      "total reward 3077\n",
      "loss None\n",
      "steps done 23135\n",
      "63 episodes completed\n",
      "total reward 3176\n",
      "loss None\n",
      "steps done 23650\n",
      "64 episodes completed\n",
      "total reward 2343\n",
      "loss None\n",
      "steps done 24005\n",
      "65 episodes completed\n",
      "total reward 2147\n",
      "loss None\n",
      "steps done 24340\n",
      "66 episodes completed\n",
      "total reward 2301\n",
      "loss None\n",
      "steps done 24740\n",
      "67 episodes completed\n",
      "total reward 2934\n",
      "loss None\n",
      "steps done 25230\n",
      "68 episodes completed\n",
      "total reward 2849\n",
      "loss None\n",
      "steps done 25595\n",
      "69 episodes completed\n",
      "total reward 2128\n",
      "loss None\n",
      "steps done 25940\n",
      "70 episodes completed\n",
      "total reward 1975\n",
      "loss None\n",
      "steps done 26280\n",
      "71 episodes completed\n",
      "total reward 1746\n",
      "loss None\n",
      "steps done 26530\n",
      "72 episodes completed\n",
      "total reward 2823\n",
      "loss None\n",
      "steps done 26935\n",
      "73 episodes completed\n",
      "total reward 2441\n",
      "loss None\n",
      "steps done 27320\n",
      "74 episodes completed\n",
      "total reward 2645\n",
      "loss None\n",
      "steps done 27675\n",
      "75 episodes completed\n",
      "total reward 2110\n",
      "loss None\n",
      "steps done 28000\n",
      "76 episodes completed\n",
      "total reward 2081\n",
      "loss None\n",
      "steps done 28345\n",
      "77 episodes completed\n",
      "total reward 2078\n",
      "loss None\n",
      "steps done 28650\n",
      "78 episodes completed\n",
      "total reward 2171\n",
      "loss None\n",
      "steps done 28970\n",
      "79 episodes completed\n",
      "total reward 3173\n",
      "loss None\n",
      "steps done 29575\n",
      "80 episodes completed\n",
      "total reward 2651\n",
      "loss None\n",
      "steps done 30020\n",
      "81 episodes completed\n",
      "total reward 2383\n",
      "loss None\n",
      "steps done 30360\n",
      "82 episodes completed\n",
      "total reward 2325\n",
      "loss None\n",
      "steps done 30740\n",
      "83 episodes completed\n",
      "total reward 2484\n",
      "loss None\n",
      "steps done 31105\n",
      "84 episodes completed\n",
      "total reward 2112\n",
      "loss None\n",
      "steps done 31465\n",
      "85 episodes completed\n",
      "total reward 2384\n",
      "loss None\n",
      "steps done 31825\n",
      "86 episodes completed\n",
      "total reward 2321\n",
      "loss None\n",
      "steps done 32165\n",
      "87 episodes completed\n",
      "total reward 1901\n",
      "loss None\n",
      "steps done 32440\n",
      "88 episodes completed\n",
      "total reward 2356\n",
      "loss None\n",
      "steps done 32785\n",
      "89 episodes completed\n",
      "total reward 2679\n",
      "loss None\n",
      "steps done 33190\n",
      "90 episodes completed\n",
      "total reward 2942\n",
      "loss None\n",
      "steps done 33620\n",
      "91 episodes completed\n",
      "total reward 2658\n",
      "loss None\n",
      "steps done 33965\n",
      "92 episodes completed\n",
      "total reward 2199\n",
      "loss None\n",
      "steps done 34295\n",
      "93 episodes completed\n",
      "total reward 2812\n",
      "loss None\n",
      "steps done 34675\n",
      "94 episodes completed\n",
      "total reward 2291\n",
      "loss None\n",
      "steps done 35060\n",
      "95 episodes completed\n",
      "total reward 2312\n",
      "loss None\n",
      "steps done 35400\n",
      "96 episodes completed\n",
      "total reward 1845\n",
      "loss None\n",
      "steps done 35670\n",
      "97 episodes completed\n",
      "total reward 2416\n",
      "loss None\n",
      "steps done 36045\n",
      "98 episodes completed\n",
      "total reward 1867\n",
      "loss None\n",
      "steps done 36340\n",
      "99 episodes completed\n",
      "total reward 2438\n",
      "loss None\n",
      "steps done 36715\n",
      "100 episodes completed\n",
      "total reward 2275\n",
      "loss None\n",
      "steps done 37010\n",
      "101 episodes completed\n",
      "total reward 2167\n",
      "loss None\n",
      "steps done 37340\n",
      "102 episodes completed\n",
      "total reward 1977\n",
      "loss 6.223766803741455\n",
      "steps done 37670\n",
      "103 episodes completed\n",
      "total reward 2251\n",
      "loss 5.828511714935303\n",
      "steps done 37975\n",
      "104 episodes completed\n",
      "total reward 3591\n",
      "loss 5.160163402557373\n",
      "steps done 38525\n",
      "105 episodes completed\n",
      "total reward 1896\n",
      "loss 4.569874286651611\n",
      "steps done 38835\n",
      "106 episodes completed\n",
      "total reward 2561\n",
      "loss 4.479938983917236\n",
      "steps done 39235\n",
      "107 episodes completed\n",
      "total reward 2043\n",
      "loss 4.111528396606445\n",
      "steps done 39555\n",
      "108 episodes completed\n",
      "total reward 1934\n",
      "loss 3.9182398319244385\n",
      "steps done 39880\n",
      "109 episodes completed\n",
      "total reward 1739\n",
      "loss 3.577988862991333\n",
      "steps done 40165\n",
      "110 episodes completed\n",
      "total reward 2242\n",
      "loss 2.954587936401367\n",
      "steps done 40515\n",
      "111 episodes completed\n",
      "total reward 2674\n",
      "loss 2.799981117248535\n",
      "steps done 40885\n",
      "112 episodes completed\n",
      "total reward 3224\n",
      "loss 2.914954423904419\n",
      "steps done 41385\n",
      "113 episodes completed\n",
      "total reward 2300\n",
      "loss 2.2383038997650146\n",
      "steps done 41715\n",
      "114 episodes completed\n",
      "total reward 2261\n",
      "loss 2.367037773132324\n",
      "steps done 42055\n",
      "115 episodes completed\n",
      "total reward 2522\n",
      "loss 2.602130889892578\n",
      "steps done 42470\n",
      "116 episodes completed\n",
      "total reward 1985\n",
      "loss 2.5665411949157715\n",
      "steps done 42770\n",
      "117 episodes completed\n",
      "total reward 3097\n",
      "loss 2.139106035232544\n",
      "steps done 43290\n",
      "118 episodes completed\n",
      "total reward 2215\n",
      "loss 2.074136972427368\n",
      "steps done 43575\n",
      "119 episodes completed\n",
      "total reward 2112\n",
      "loss 1.9029226303100586\n",
      "steps done 43870\n",
      "120 episodes completed\n",
      "total reward 2319\n",
      "loss 2.5419199466705322\n",
      "steps done 44210\n",
      "121 episodes completed\n",
      "total reward 2326\n",
      "loss 2.0747737884521484\n",
      "steps done 44610\n",
      "122 episodes completed\n",
      "total reward 2000\n",
      "loss 1.8707120418548584\n",
      "steps done 44890\n",
      "123 episodes completed\n",
      "total reward 2743\n",
      "loss 1.6943880319595337\n",
      "steps done 45230\n",
      "124 episodes completed\n",
      "total reward 2776\n",
      "loss 1.9791523218154907\n",
      "steps done 45660\n",
      "125 episodes completed\n",
      "total reward 2599\n",
      "loss 1.7405827045440674\n",
      "steps done 46015\n",
      "126 episodes completed\n",
      "total reward 2745\n",
      "loss 2.0148239135742188\n",
      "steps done 46435\n",
      "127 episodes completed\n",
      "total reward 2341\n",
      "loss 2.1498923301696777\n",
      "steps done 46775\n",
      "128 episodes completed\n",
      "total reward 2906\n",
      "loss 1.6227396726608276\n",
      "steps done 47135\n",
      "129 episodes completed\n",
      "total reward 1928\n",
      "loss 1.3523720502853394\n",
      "steps done 47410\n",
      "130 episodes completed\n",
      "total reward 2685\n",
      "loss 1.6562392711639404\n",
      "steps done 47775\n",
      "131 episodes completed\n",
      "total reward 2483\n",
      "loss 1.615616798400879\n",
      "steps done 48100\n",
      "132 episodes completed\n",
      "total reward 2380\n",
      "loss 1.8369520902633667\n",
      "steps done 48445\n",
      "133 episodes completed\n",
      "total reward 3673\n",
      "loss 1.5383840799331665\n",
      "steps done 49050\n",
      "134 episodes completed\n",
      "total reward 2057\n",
      "loss 1.6682934761047363\n",
      "steps done 49335\n",
      "135 episodes completed\n",
      "total reward 2524\n",
      "loss 1.4998561143875122\n",
      "steps done 49760\n",
      "update target\n",
      "136 episodes completed\n",
      "total reward 1873\n",
      "loss 5.816313743591309\n",
      "steps done 50030\n",
      "137 episodes completed\n",
      "total reward 2013\n",
      "loss 6.0527472496032715\n",
      "steps done 50305\n",
      "138 episodes completed\n",
      "total reward 2283\n",
      "loss 5.2799601554870605\n",
      "steps done 50660\n",
      "139 episodes completed\n",
      "total reward 2390\n",
      "loss 4.666632652282715\n",
      "steps done 51060\n",
      "140 episodes completed\n",
      "total reward 2583\n",
      "loss 4.249128818511963\n",
      "steps done 51455\n",
      "141 episodes completed\n",
      "total reward 1754\n",
      "loss 3.6696393489837646\n",
      "steps done 51735\n",
      "142 episodes completed\n",
      "total reward 2811\n",
      "loss 3.6858839988708496\n",
      "steps done 52090\n",
      "143 episodes completed\n",
      "total reward 2302\n",
      "loss 3.7806427478790283\n",
      "steps done 52365\n",
      "144 episodes completed\n",
      "total reward 2606\n",
      "loss 3.024513006210327\n",
      "steps done 52750\n",
      "145 episodes completed\n",
      "total reward 2572\n",
      "loss 2.931901454925537\n",
      "steps done 53065\n",
      "146 episodes completed\n",
      "total reward 2297\n",
      "loss 2.47263503074646\n",
      "steps done 53420\n",
      "147 episodes completed\n",
      "total reward 1841\n",
      "loss 2.8913629055023193\n",
      "steps done 53705\n",
      "148 episodes completed\n",
      "total reward 3879\n",
      "loss 2.5098724365234375\n",
      "steps done 54340\n",
      "149 episodes completed\n",
      "total reward 2733\n",
      "loss 2.051581621170044\n",
      "steps done 54775\n",
      "150 episodes completed\n",
      "total reward 2644\n",
      "loss 2.0007576942443848\n",
      "steps done 55160\n",
      "151 episodes completed\n",
      "total reward 1958\n",
      "loss 1.9644640684127808\n",
      "steps done 55460\n",
      "152 episodes completed\n",
      "total reward 2084\n",
      "loss 2.062892198562622\n",
      "steps done 55725\n",
      "153 episodes completed\n",
      "total reward 2957\n",
      "loss 2.1191115379333496\n",
      "steps done 56155\n",
      "154 episodes completed\n",
      "total reward 3397\n",
      "loss 1.7540572881698608\n",
      "steps done 56670\n",
      "155 episodes completed\n",
      "total reward 2412\n",
      "loss 1.7370480298995972\n",
      "steps done 56995\n",
      "156 episodes completed\n",
      "total reward 1903\n",
      "loss 2.0391416549682617\n",
      "steps done 57280\n",
      "157 episodes completed\n",
      "total reward 1995\n",
      "loss 1.6831637620925903\n",
      "steps done 57585\n",
      "158 episodes completed\n",
      "total reward 2590\n",
      "loss 1.8881003856658936\n",
      "steps done 57930\n",
      "159 episodes completed\n",
      "total reward 3222\n",
      "loss 1.9808274507522583\n",
      "steps done 58350\n",
      "160 episodes completed\n",
      "total reward 3050\n",
      "loss 1.659010410308838\n",
      "steps done 58860\n",
      "161 episodes completed\n",
      "total reward 2944\n",
      "loss 1.6488847732543945\n",
      "steps done 59305\n",
      "162 episodes completed\n",
      "total reward 2789\n",
      "loss 1.9811204671859741\n",
      "steps done 59745\n",
      "163 episodes completed\n",
      "total reward 2338\n",
      "loss 1.7709929943084717\n",
      "steps done 60040\n",
      "164 episodes completed\n",
      "total reward 2213\n",
      "loss 1.862758755683899\n",
      "steps done 60410\n",
      "165 episodes completed\n",
      "total reward 3928\n",
      "loss 1.307218313217163\n",
      "steps done 60970\n",
      "166 episodes completed\n",
      "total reward 1826\n",
      "loss 1.5182909965515137\n",
      "steps done 61260\n",
      "167 episodes completed\n",
      "total reward 2086\n",
      "loss 1.659953236579895\n",
      "steps done 61515\n",
      "168 episodes completed\n",
      "total reward 3862\n",
      "loss 1.403792142868042\n",
      "steps done 62090\n",
      "update target\n",
      "169 episodes completed\n",
      "total reward 2575\n",
      "loss 6.022437572479248\n",
      "steps done 62480\n",
      "170 episodes completed\n",
      "total reward 3016\n",
      "loss 5.531548023223877\n",
      "steps done 62940\n",
      "171 episodes completed\n",
      "total reward 2142\n",
      "loss 4.921510696411133\n",
      "steps done 63265\n",
      "172 episodes completed\n",
      "total reward 2116\n",
      "loss 4.724460124969482\n",
      "steps done 63580\n",
      "173 episodes completed\n",
      "total reward 3051\n",
      "loss 4.396917819976807\n",
      "steps done 64020\n",
      "174 episodes completed\n",
      "total reward 2977\n",
      "loss 3.7449071407318115\n",
      "steps done 64400\n",
      "175 episodes completed\n",
      "total reward 2307\n",
      "loss 3.8725485801696777\n",
      "steps done 64740\n",
      "176 episodes completed\n",
      "total reward 2521\n",
      "loss 3.317253589630127\n",
      "steps done 65115\n",
      "177 episodes completed\n",
      "total reward 2351\n",
      "loss 2.972947835922241\n",
      "steps done 65415\n",
      "178 episodes completed\n",
      "total reward 2303\n",
      "loss 2.7171146869659424\n",
      "steps done 65720\n",
      "179 episodes completed\n",
      "total reward 2248\n",
      "loss 3.0269479751586914\n",
      "steps done 66070\n",
      "180 episodes completed\n",
      "total reward 2536\n",
      "loss 2.5477731227874756\n",
      "steps done 66425\n",
      "181 episodes completed\n",
      "total reward 2580\n",
      "loss 2.593053102493286\n",
      "steps done 66770\n",
      "182 episodes completed\n",
      "total reward 2523\n",
      "loss 2.621920585632324\n",
      "steps done 67130\n",
      "183 episodes completed\n",
      "total reward 2563\n",
      "loss 2.677691698074341\n",
      "steps done 67480\n",
      "184 episodes completed\n",
      "total reward 2766\n",
      "loss 2.392033815383911\n",
      "steps done 67855\n",
      "185 episodes completed\n",
      "total reward 3900\n",
      "loss 2.164693593978882\n",
      "steps done 68420\n",
      "186 episodes completed\n",
      "total reward 2139\n",
      "loss 2.1845595836639404\n",
      "steps done 68710\n",
      "187 episodes completed\n",
      "total reward 2675\n",
      "loss 1.8900138139724731\n",
      "steps done 69095\n",
      "188 episodes completed\n",
      "total reward 2674\n",
      "loss 1.8237489461898804\n",
      "steps done 69475\n",
      "189 episodes completed\n",
      "total reward 3592\n",
      "loss 2.2469065189361572\n",
      "steps done 69985\n",
      "190 episodes completed\n",
      "total reward 2481\n",
      "loss 1.7446551322937012\n",
      "steps done 70255\n",
      "191 episodes completed\n",
      "total reward 2326\n",
      "loss 2.1436147689819336\n",
      "steps done 70575\n",
      "192 episodes completed\n",
      "total reward 2302\n",
      "loss 2.065978527069092\n",
      "steps done 70925\n",
      "193 episodes completed\n",
      "total reward 2310\n",
      "loss 2.0919668674468994\n",
      "steps done 71250\n",
      "194 episodes completed\n",
      "total reward 2613\n",
      "loss 1.7315768003463745\n",
      "steps done 71575\n",
      "195 episodes completed\n",
      "total reward 4225\n",
      "loss 1.8245983123779297\n",
      "steps done 72235\n",
      "196 episodes completed\n",
      "total reward 2349\n",
      "loss 2.1133811473846436\n",
      "steps done 72560\n",
      "197 episodes completed\n",
      "total reward 2303\n",
      "loss 1.496350884437561\n",
      "steps done 72885\n",
      "198 episodes completed\n",
      "total reward 3330\n",
      "loss 1.7767847776412964\n",
      "steps done 73390\n",
      "199 episodes completed\n",
      "total reward 2167\n",
      "loss 1.4684133529663086\n",
      "steps done 73715\n",
      "200 episodes completed\n",
      "total reward 2285\n",
      "loss 1.6067187786102295\n",
      "steps done 74070\n",
      "201 episodes completed\n",
      "total reward 2826\n",
      "loss 1.4264298677444458\n",
      "steps done 74455\n",
      "202 episodes completed\n",
      "total reward 2483\n",
      "loss 1.7676467895507812\n",
      "steps done 74775\n",
      "update target\n",
      "203 episodes completed\n",
      "total reward 1846\n",
      "loss 5.825202465057373\n",
      "steps done 75020\n",
      "204 episodes completed\n",
      "total reward 2048\n",
      "loss 5.622945308685303\n",
      "steps done 75305\n",
      "205 episodes completed\n",
      "total reward 2384\n",
      "loss 4.555016040802002\n",
      "steps done 75640\n",
      "206 episodes completed\n",
      "total reward 2022\n",
      "loss 4.826920509338379\n",
      "steps done 75925\n",
      "207 episodes completed\n",
      "total reward 2131\n",
      "loss 5.150862216949463\n",
      "steps done 76230\n",
      "208 episodes completed\n",
      "total reward 3556\n",
      "loss 4.31880521774292\n",
      "steps done 76800\n",
      "209 episodes completed\n",
      "total reward 2710\n",
      "loss 3.751095771789551\n",
      "steps done 77160\n",
      "210 episodes completed\n",
      "total reward 3164\n",
      "loss 4.10606050491333\n",
      "steps done 77600\n",
      "211 episodes completed\n",
      "total reward 2257\n",
      "loss 3.3412561416625977\n",
      "steps done 77950\n",
      "212 episodes completed\n",
      "total reward 2299\n",
      "loss 3.3229715824127197\n",
      "steps done 78290\n",
      "213 episodes completed\n",
      "total reward 2020\n",
      "loss 2.7080187797546387\n",
      "steps done 78620\n",
      "214 episodes completed\n",
      "total reward 2280\n",
      "loss 3.069532632827759\n",
      "steps done 78960\n",
      "215 episodes completed\n",
      "total reward 2578\n",
      "loss 2.399332284927368\n",
      "steps done 79330\n",
      "216 episodes completed\n",
      "total reward 3040\n",
      "loss 2.4430410861968994\n",
      "steps done 79795\n",
      "217 episodes completed\n",
      "total reward 2258\n",
      "loss 2.5205183029174805\n",
      "steps done 80135\n",
      "218 episodes completed\n",
      "total reward 3437\n",
      "loss 2.3705861568450928\n",
      "steps done 80655\n",
      "219 episodes completed\n",
      "total reward 2911\n",
      "loss 1.8209969997406006\n",
      "steps done 81095\n",
      "220 episodes completed\n",
      "total reward 3824\n",
      "loss 2.0042786598205566\n",
      "steps done 81615\n",
      "221 episodes completed\n",
      "total reward 4366\n",
      "loss 2.5485341548919678\n",
      "steps done 82280\n",
      "222 episodes completed\n",
      "total reward 2274\n",
      "loss 2.42513108253479\n",
      "steps done 82640\n",
      "223 episodes completed\n",
      "total reward 1780\n",
      "loss 2.5629186630249023\n",
      "steps done 82940\n",
      "224 episodes completed\n",
      "total reward 2534\n",
      "loss 1.9189590215682983\n",
      "steps done 83255\n",
      "225 episodes completed\n",
      "total reward 2307\n",
      "loss 2.0788049697875977\n",
      "steps done 83545\n",
      "226 episodes completed\n",
      "total reward 2096\n",
      "loss 1.761659026145935\n",
      "steps done 83835\n",
      "227 episodes completed\n",
      "total reward 2160\n",
      "loss 2.0370917320251465\n",
      "steps done 84140\n",
      "228 episodes completed\n",
      "total reward 2237\n",
      "loss 2.0325284004211426\n",
      "steps done 84435\n",
      "229 episodes completed\n",
      "total reward 2786\n",
      "loss 2.010714530944824\n",
      "steps done 84825\n",
      "230 episodes completed\n",
      "total reward 2536\n",
      "loss 2.1554818153381348\n",
      "steps done 85210\n",
      "231 episodes completed\n",
      "total reward 4043\n",
      "loss 1.5766178369522095\n",
      "steps done 85815\n",
      "232 episodes completed\n",
      "total reward 2176\n",
      "loss 2.1067302227020264\n",
      "steps done 86105\n",
      "233 episodes completed\n",
      "total reward 2432\n",
      "loss 2.101271390914917\n",
      "steps done 86470\n",
      "234 episodes completed\n",
      "total reward 3057\n",
      "loss 1.7674646377563477\n",
      "steps done 86905\n",
      "235 episodes completed\n",
      "total reward 2052\n",
      "loss 1.6737583875656128\n",
      "steps done 87215\n",
      "update target\n",
      "236 episodes completed\n",
      "total reward 2302\n",
      "loss 5.778132915496826\n",
      "steps done 87535\n",
      "237 episodes completed\n",
      "total reward 3687\n",
      "loss 5.059178829193115\n",
      "steps done 88050\n",
      "238 episodes completed\n",
      "total reward 2565\n",
      "loss 4.947240829467773\n",
      "steps done 88400\n",
      "239 episodes completed\n",
      "total reward 3033\n",
      "loss 5.2289886474609375\n",
      "steps done 88870\n",
      "240 episodes completed\n",
      "total reward 3177\n",
      "loss 4.524784564971924\n",
      "steps done 89325\n",
      "241 episodes completed\n",
      "total reward 2543\n",
      "loss 3.771073579788208\n",
      "steps done 89670\n",
      "242 episodes completed\n",
      "total reward 2370\n",
      "loss 3.8302197456359863\n",
      "steps done 90025\n",
      "243 episodes completed\n",
      "total reward 2060\n",
      "loss 3.783169984817505\n",
      "steps done 90305\n",
      "244 episodes completed\n",
      "total reward 3072\n",
      "loss 3.572354793548584\n",
      "steps done 90720\n",
      "245 episodes completed\n",
      "total reward 3311\n",
      "loss 2.884549379348755\n",
      "steps done 91230\n",
      "246 episodes completed\n",
      "total reward 2385\n",
      "loss 3.436337471008301\n",
      "steps done 91530\n",
      "247 episodes completed\n",
      "total reward 3020\n",
      "loss 2.825066328048706\n",
      "steps done 92020\n",
      "248 episodes completed\n",
      "total reward 2480\n",
      "loss 3.2283132076263428\n",
      "steps done 92350\n",
      "249 episodes completed\n",
      "total reward 3017\n",
      "loss 2.6091930866241455\n",
      "steps done 92800\n",
      "250 episodes completed\n",
      "total reward 2105\n",
      "loss 2.8912954330444336\n",
      "steps done 93095\n",
      "251 episodes completed\n",
      "total reward 2381\n",
      "loss 2.8554224967956543\n",
      "steps done 93420\n",
      "252 episodes completed\n",
      "total reward 2204\n",
      "loss 2.463087320327759\n",
      "steps done 93745\n",
      "253 episodes completed\n",
      "total reward 1594\n",
      "loss 2.303471326828003\n",
      "steps done 93970\n",
      "254 episodes completed\n",
      "total reward 2050\n",
      "loss 2.315741539001465\n",
      "steps done 94280\n",
      "255 episodes completed\n",
      "total reward 2053\n",
      "loss 2.264188051223755\n",
      "steps done 94595\n",
      "256 episodes completed\n",
      "total reward 2703\n",
      "loss 2.2190122604370117\n",
      "steps done 94970\n",
      "257 episodes completed\n",
      "total reward 2672\n",
      "loss 2.4609248638153076\n",
      "steps done 95365\n",
      "258 episodes completed\n",
      "total reward 2010\n",
      "loss 2.429957866668701\n",
      "steps done 95640\n",
      "259 episodes completed\n",
      "total reward 2139\n",
      "loss 2.2063326835632324\n",
      "steps done 95945\n",
      "260 episodes completed\n",
      "total reward 1740\n",
      "loss 2.3907229900360107\n",
      "steps done 96245\n",
      "261 episodes completed\n",
      "total reward 1827\n",
      "loss 1.987229824066162\n",
      "steps done 96540\n",
      "262 episodes completed\n",
      "total reward 2924\n",
      "loss 2.105353832244873\n",
      "steps done 96990\n",
      "263 episodes completed\n",
      "total reward 2584\n",
      "loss 2.4522953033447266\n",
      "steps done 97400\n",
      "264 episodes completed\n",
      "total reward 2990\n",
      "loss 1.8129547834396362\n",
      "steps done 97840\n",
      "265 episodes completed\n",
      "total reward 2914\n",
      "loss 2.068819046020508\n",
      "steps done 98295\n",
      "266 episodes completed\n",
      "total reward 2448\n",
      "loss 1.9247699975967407\n",
      "steps done 98675\n",
      "267 episodes completed\n",
      "total reward 2460\n",
      "loss 1.7527307271957397\n",
      "steps done 99075\n",
      "268 episodes completed\n",
      "total reward 2537\n",
      "loss 1.9320385456085205\n",
      "steps done 99460\n",
      "update target\n",
      "269 episodes completed\n",
      "total reward 2993\n",
      "loss 6.475546836853027\n",
      "steps done 99890\n",
      "270 episodes completed\n",
      "total reward 2603\n",
      "loss 6.17608642578125\n",
      "steps done 100195\n",
      "271 episodes completed\n",
      "total reward 2299\n",
      "loss 5.698449611663818\n",
      "steps done 100525\n",
      "272 episodes completed\n",
      "total reward 2924\n",
      "loss 4.294620037078857\n",
      "steps done 100950\n",
      "273 episodes completed\n",
      "total reward 2371\n",
      "loss 4.409369945526123\n",
      "steps done 101315\n",
      "274 episodes completed\n",
      "total reward 3245\n",
      "loss 4.274559497833252\n",
      "steps done 101790\n",
      "275 episodes completed\n",
      "total reward 3109\n",
      "loss 4.587304592132568\n",
      "steps done 102240\n",
      "276 episodes completed\n",
      "total reward 2615\n",
      "loss 3.9593865871429443\n",
      "steps done 102590\n",
      "277 episodes completed\n",
      "total reward 2053\n",
      "loss 3.5908684730529785\n",
      "steps done 102870\n",
      "278 episodes completed\n",
      "total reward 2258\n",
      "loss 4.024642467498779\n",
      "steps done 103170\n",
      "279 episodes completed\n",
      "total reward 2143\n",
      "loss 3.384467124938965\n",
      "steps done 103480\n",
      "280 episodes completed\n",
      "total reward 3505\n",
      "loss 3.185485601425171\n",
      "steps done 103955\n",
      "281 episodes completed\n",
      "total reward 2750\n",
      "loss 2.9611268043518066\n",
      "steps done 104370\n",
      "282 episodes completed\n",
      "total reward 3448\n",
      "loss 3.1881632804870605\n",
      "steps done 104875\n",
      "283 episodes completed\n",
      "total reward 2076\n",
      "loss 2.958672285079956\n",
      "steps done 105125\n",
      "284 episodes completed\n",
      "total reward 2281\n",
      "loss 2.5673398971557617\n",
      "steps done 105500\n",
      "285 episodes completed\n",
      "total reward 2807\n",
      "loss 2.6063220500946045\n",
      "steps done 105865\n",
      "286 episodes completed\n",
      "total reward 2066\n",
      "loss 2.6212353706359863\n",
      "steps done 106120\n",
      "287 episodes completed\n",
      "total reward 2094\n",
      "loss 2.4570653438568115\n",
      "steps done 106420\n",
      "288 episodes completed\n",
      "total reward 2516\n",
      "loss 2.63690447807312\n",
      "steps done 106725\n",
      "289 episodes completed\n",
      "total reward 2726\n",
      "loss 2.556840181350708\n",
      "steps done 107150\n",
      "290 episodes completed\n",
      "total reward 2284\n",
      "loss 2.541759490966797\n",
      "steps done 107465\n",
      "291 episodes completed\n",
      "total reward 1887\n",
      "loss 2.2078115940093994\n",
      "steps done 107750\n",
      "292 episodes completed\n",
      "total reward 3193\n",
      "loss 2.415679454803467\n",
      "steps done 108210\n",
      "293 episodes completed\n",
      "total reward 3479\n",
      "loss 2.126875400543213\n",
      "steps done 108700\n",
      "294 episodes completed\n",
      "total reward 3759\n",
      "loss 2.3825080394744873\n",
      "steps done 109245\n",
      "295 episodes completed\n",
      "total reward 2719\n",
      "loss 2.1886799335479736\n",
      "steps done 109650\n",
      "296 episodes completed\n",
      "total reward 2431\n",
      "loss 2.1427652835845947\n",
      "steps done 110030\n",
      "297 episodes completed\n",
      "total reward 2000\n",
      "loss 2.0142529010772705\n",
      "steps done 110315\n",
      "298 episodes completed\n",
      "total reward 2207\n",
      "loss 1.8532626628875732\n",
      "steps done 110610\n",
      "299 episodes completed\n",
      "total reward 2615\n",
      "loss 2.0528852939605713\n",
      "steps done 111000\n",
      "300 episodes completed\n",
      "total reward 2884\n",
      "loss 2.370062828063965\n",
      "steps done 111400\n",
      "301 episodes completed\n",
      "total reward 2082\n",
      "loss 2.235645055770874\n",
      "steps done 111735\n",
      "update target\n",
      "302 episodes completed\n",
      "total reward 4095\n",
      "loss 8.378925323486328\n",
      "steps done 112365\n",
      "303 episodes completed\n",
      "total reward 3687\n",
      "loss 5.483975887298584\n",
      "steps done 112940\n",
      "304 episodes completed\n",
      "total reward 2479\n",
      "loss 5.063836574554443\n",
      "steps done 113315\n",
      "305 episodes completed\n",
      "total reward 3145\n",
      "loss 4.660081386566162\n",
      "steps done 113755\n",
      "306 episodes completed\n",
      "total reward 2812\n",
      "loss 4.546134948730469\n",
      "steps done 114160\n",
      "307 episodes completed\n",
      "total reward 1705\n",
      "loss 4.618605613708496\n",
      "steps done 114445\n",
      "308 episodes completed\n",
      "total reward 2754\n",
      "loss 4.395447731018066\n",
      "steps done 114850\n",
      "309 episodes completed\n",
      "total reward 2026\n",
      "loss 4.440383434295654\n",
      "steps done 115140\n",
      "310 episodes completed\n",
      "total reward 2296\n",
      "loss 3.833479166030884\n",
      "steps done 115455\n",
      "311 episodes completed\n",
      "total reward 1712\n",
      "loss 3.3659465312957764\n",
      "steps done 115695\n",
      "312 episodes completed\n",
      "total reward 2927\n",
      "loss 3.628772497177124\n",
      "steps done 116170\n",
      "313 episodes completed\n",
      "total reward 2610\n",
      "loss 3.2095394134521484\n",
      "steps done 116525\n",
      "314 episodes completed\n",
      "total reward 1997\n",
      "loss 3.2088143825531006\n",
      "steps done 116835\n",
      "315 episodes completed\n",
      "total reward 2282\n",
      "loss 3.115229845046997\n",
      "steps done 117170\n",
      "316 episodes completed\n",
      "total reward 1983\n",
      "loss 3.07515549659729\n",
      "steps done 117485\n",
      "317 episodes completed\n",
      "total reward 3580\n",
      "loss 2.848156213760376\n",
      "steps done 117985\n",
      "318 episodes completed\n",
      "total reward 3228\n",
      "loss 2.798917293548584\n",
      "steps done 118510\n",
      "319 episodes completed\n",
      "total reward 2460\n",
      "loss 3.029388904571533\n",
      "steps done 118870\n",
      "320 episodes completed\n",
      "total reward 1969\n",
      "loss 2.9411580562591553\n",
      "steps done 119130\n",
      "321 episodes completed\n",
      "total reward 2325\n",
      "loss 2.6503841876983643\n",
      "steps done 119530\n",
      "322 episodes completed\n",
      "total reward 2397\n",
      "loss 2.324155569076538\n",
      "steps done 119880\n",
      "323 episodes completed\n",
      "total reward 2152\n",
      "loss 2.453468084335327\n",
      "steps done 120210\n",
      "324 episodes completed\n",
      "total reward 1859\n",
      "loss 2.6062986850738525\n",
      "steps done 120480\n",
      "325 episodes completed\n",
      "total reward 1722\n",
      "loss 2.7676894664764404\n",
      "steps done 120775\n",
      "326 episodes completed\n",
      "total reward 2813\n",
      "loss 2.1373674869537354\n",
      "steps done 121205\n",
      "327 episodes completed\n",
      "total reward 2346\n",
      "loss 2.0655386447906494\n",
      "steps done 121590\n",
      "328 episodes completed\n",
      "total reward 2226\n",
      "loss 2.138871431350708\n",
      "steps done 121965\n",
      "329 episodes completed\n",
      "total reward 2408\n",
      "loss 1.8845022916793823\n",
      "steps done 122335\n",
      "330 episodes completed\n",
      "total reward 2436\n",
      "loss 2.0239787101745605\n",
      "steps done 122735\n",
      "331 episodes completed\n",
      "total reward 3101\n",
      "loss 2.0480215549468994\n",
      "steps done 123150\n",
      "332 episodes completed\n",
      "total reward 2856\n",
      "loss 2.4969351291656494\n",
      "steps done 123545\n",
      "333 episodes completed\n",
      "total reward 2438\n",
      "loss 2.001883029937744\n",
      "steps done 123915\n",
      "334 episodes completed\n",
      "total reward 2258\n",
      "loss 2.0583927631378174\n",
      "steps done 124255\n",
      "335 episodes completed\n",
      "total reward 2650\n",
      "loss 1.993468999862671\n",
      "steps done 124635\n",
      "update target\n",
      "336 episodes completed\n",
      "total reward 2674\n",
      "loss 5.665816307067871\n",
      "steps done 125000\n",
      "337 episodes completed\n",
      "total reward 2461\n",
      "loss 5.430313587188721\n",
      "steps done 125340\n",
      "338 episodes completed\n",
      "total reward 3290\n",
      "loss 5.2024359703063965\n",
      "steps done 125815\n",
      "339 episodes completed\n",
      "total reward 2602\n",
      "loss 5.222911834716797\n",
      "steps done 126175\n",
      "340 episodes completed\n",
      "total reward 2240\n",
      "loss 5.2144551277160645\n",
      "steps done 126500\n",
      "341 episodes completed\n",
      "total reward 2519\n",
      "loss 4.430459499359131\n",
      "steps done 126805\n",
      "342 episodes completed\n",
      "total reward 2514\n",
      "loss 4.260411262512207\n",
      "steps done 127130\n",
      "343 episodes completed\n",
      "total reward 2545\n",
      "loss 3.9157590866088867\n",
      "steps done 127480\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://nr811idrwh.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(HEIGHT, WIDTH)\n",
    "dronesEnv = DronesEnv(HEIGHT, WIDTH, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "total_reward = 0\n",
    "seed, observation = fireEnv.reset()\n",
    "dronesEnv.reset(seed, observation)\n",
    "\n",
    "episode_memory_1 = []\n",
    "episode_memory_2 = []\n",
    "sequence_i = 0\n",
    "hidden_1 = None\n",
    "hidden_2 = None\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "\n",
    "  state_vector_1 = dronesEnv.drones[0].state\n",
    "  map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "\n",
    "  state_vector_2 = dronesEnv.drones[1].state\n",
    "  map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "\n",
    "  for i in range(int(DT/DTI)):\n",
    "\n",
    "    action1, hidden_1 = select_action(map_1, state_vector_1, steps, hidden_1)\n",
    "    action2, hidden_2 = select_action(map_2, state_vector_2, steps, hidden_2)\n",
    "    steps += 1\n",
    "    sequence_i += 1\n",
    "    rewards = dronesEnv.step([action1, action2], observation)\n",
    "\n",
    "    next_state_vector_1 = dronesEnv.drones[0].state\n",
    "    next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "    next_state_vector_2 = dronesEnv.drones[1].state\n",
    "    next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "    total_reward += sum(rewards)\n",
    "\n",
    "    episode_memory_1.append(Transition(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, rewards[0]))\n",
    "    episode_memory_2.append(Transition(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, rewards[1]))\n",
    "\n",
    "    state_vector_1 = next_state_vector_1\n",
    "    map_1 = next_map_1\n",
    "\n",
    "\n",
    "    state_vector_2 = next_state_vector_2\n",
    "    map_2 = next_map_2\n",
    "\n",
    "  observation = fireEnv.step()\n",
    "\n",
    "\n",
    "  if len(memory)>INIT_SIZE:\n",
    "    loss = optimize_model()\n",
    "\n",
    "\n",
    "    \n",
    "  if not fireEnv.fire_in_range(6):\n",
    "\n",
    "    hidden_1 = None\n",
    "    hidden_2 = None\n",
    "\n",
    "    memory.push(episode_memory_1)\n",
    "    memory.push(episode_memory_2)\n",
    "    episode_memory_1 = []\n",
    "    episode_memory_2 = []\n",
    "    \n",
    "\n",
    "    print(f'{i_episode} episodes completed')\n",
    "    print(f'total reward {total_reward}')\n",
    "    print(f'loss {loss}')\n",
    "    print(f'steps done {steps}')\n",
    "    i_episode +=1\n",
    "    total_reward = 0\n",
    "        \n",
    "    seed, observation = fireEnv.reset()\n",
    "    dronesEnv.reset(seed, observation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
