{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgpath2mpl in /home/jeremy/.local/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /home/jeremy/.local/lib/python3.8/site-packages (from svgpath2mpl) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /home/jeremy/.local/lib/python3.8/site-packages (from svgpath2mpl) (3.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib->svgpath2mpl) (2.7.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (9.0.1)\n",
      "Requirement already satisfied: six in /home/jeremy/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->svgpath2mpl) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import  deque, namedtuple\n",
    "import random\n",
    "Transition = namedtuple('Transition',('belief_map', 'state_vector', 'action', 'next_belief_map', 'next_state_vector', 'reward'))\n",
    "\n",
    "class EpisodeMemory:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.episode_scene = []\n",
    "\n",
    "  def push(self, *args):\n",
    "    self.episode_scene.append(Transition(*args))\n",
    "\n",
    "  def sample(self, sequence_length):\n",
    "    start_idx = random.randint(0, len(self.episode_scene)-sequence_length)\n",
    "    return self.episode_scene[start_idx:start_idx+sequence_length]\n",
    "\n",
    "  def __del__(self):\n",
    "    del self.episode_scene\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.episode_scene)\n",
    "    \n",
    "\n",
    "class ReplayMemory:\n",
    "  def __init__(self, capacity):\n",
    "    self.episodes = deque([], maxlen=capacity)\n",
    "\n",
    "  def push(self, episode):\n",
    "    self.episodes.append(episode)\n",
    "\n",
    "  def sample(self, batch_size, sequence_length):\n",
    "    \n",
    "    belief_maps = []\n",
    "    state_vectors = []\n",
    "    actions = []\n",
    "    next_belief_maps = []\n",
    "    next_state_vectors = []\n",
    "    rewards = []\n",
    "    \n",
    "    episodes = random.sample(self.episodes, batch_size)\n",
    "\n",
    "    for episode in episodes:\n",
    "\n",
    "      episode_sequence = Transition(*zip(*episode.sample(sequence_length)))\n",
    "      belief_maps.append(list(episode_sequence.belief_map))\n",
    "      state_vectors.append(list(episode_sequence.state_vector))\n",
    "      actions.append(list(episode_sequence.action))\n",
    "      next_belief_maps.append(list(episode_sequence.next_belief_map))\n",
    "      next_state_vectors.append(list(episode_sequence.next_state_vector))\n",
    "      rewards.append(list(episode_sequence.reward)) \n",
    "\n",
    "    return torch.tensor(belief_maps, dtype=torch.float), torch.tensor(state_vectors, dtype=torch.float), torch.tensor(actions), torch.tensor(next_belief_maps, dtype=torch.float), torch.tensor(next_state_vectors, dtype=torch.float), torch.tensor(rewards) \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "class AbstractFireEnv(metaclass = ABCMeta):\n",
    "\n",
    "  def __init__(self, _height, _width):\n",
    "    self._height = _height\n",
    "    self._width  = _width\n",
    "    self._time_steps = 0\n",
    "    self._observation = None\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "  \n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @property \n",
    "  def time_steps(self):\n",
    "    return self._time_steps\n",
    "\n",
    "  @time_steps.setter\n",
    "  def time_steps(self, _time_steps):\n",
    "    self._time_steps = _time_steps\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return self._observation\n",
    "\n",
    "  @observation.setter\n",
    "  def observation(self, _observation):\n",
    "    self._observation = _observation\n",
    "\n",
    "  def step(self):\n",
    "    self._time_steps += 1\n",
    "    self.observation = self.next_observation()\n",
    "    return self.observation\n",
    "\n",
    "  def plot_heat_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)  \n",
    "    heat_map_plot = ax.imshow(self.observation, cmap='hot')\n",
    "    return heat_map_plot\n",
    "\n",
    "  def reset(self):\n",
    "    self._time_steps = 0\n",
    "    self.observation = self.reset_observation()\n",
    "    seed = self.observation.copy()\n",
    "    for _ in range(30):\n",
    "      self.step()\n",
    "    return seed, self.observation\n",
    "\n",
    "  def fire_in_range(self,margin=2):\n",
    "    burnX, burnY = np.where(self.observation==1)\n",
    "    return min(burnX)>=margin and min(burnY)>=margin and max(burnX)<=99-margin and max(burnY)<=99-margin\n",
    "\n",
    "  @abstractmethod\n",
    "  def next_observation(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def reset_observation(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "D = 2\n",
    "K = 0.05\n",
    "\n",
    "def getNeighbors(point):\n",
    "    neighbors = []\n",
    "    min_x = max(0, point[1]-D)\n",
    "    max_x = min(99, point[1]+D)\n",
    "    min_y = max(0, point[0]-D)\n",
    "    max_y = min(99, point[0]+D)\n",
    "\n",
    "    for y in range(min_y, max_y): \n",
    "      for x in range(min_x, max_x):\n",
    "        neighbors.append((y, x))\n",
    "    return neighbors\n",
    "\n",
    "class ProbabilisticFireEnv(AbstractFireEnv):\n",
    "\n",
    "  def next_observation(self):\n",
    "\n",
    "    probability_map = np.zeros(shape=(HEIGHT,WIDTH), dtype=float)\n",
    "    for row in range(self.height):\n",
    "      for col in range(self.width):\n",
    "        if self.observation[row,col] == 1:\n",
    "          if self.fuel[row, col] > 0:\n",
    "            self.fuel[row, col] -= 1\n",
    "          else:\n",
    "            self.observation[row,col] = 0\n",
    "\n",
    "        elif self.observation[row,col] == 0 and self.fuel[row, col] > 0:\n",
    "          neighboring_cells = getNeighbors((row, col))\n",
    "          pnm = 1\n",
    "          for neighboring_cell in neighboring_cells:\n",
    "            if self.observation[neighboring_cell] == 1:\n",
    "              dnmkl = np.array([a-b for a, b in zip(neighboring_cell, (row,col))])\n",
    "              norm = np.sum(dnmkl**2)\n",
    "              pnmkl0 = K/norm\n",
    "              pnmklw = K*(dnmkl @ self.wind)/norm \n",
    "              pnmkl  = max(0, min(1, (pnmkl0+pnmklw)))\n",
    "              pnm *= (1-pnmkl)\n",
    "          pmn = 1 - pnm\n",
    "          probability_map[row, col] = pmn\n",
    "\n",
    "    self.observation[probability_map > np.random.rand(HEIGHT,WIDTH)]  = 1\n",
    "\n",
    "    return self.observation\n",
    "\n",
    "  def reset_observation(self):\n",
    "    center = [49, 49]\n",
    "    self.observation = np.zeros(shape=(self.height, self.width), dtype=int)\n",
    "    self.observation[center[0]-2:center[0]+2, center[1]-2:center[1]+2] = 1\n",
    "    self.fuel = np.random.randint(low=15, high=20, size=(self.height, self.width))\n",
    "    self.wind = np.random.uniform(low=-0.25, high=0.25, size=2)\n",
    "    return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import random\n",
    "\n",
    "HEIGHT = WIDTH = 100\n",
    "C = 50\n",
    "Cx = 50\n",
    "Cy = 50\n",
    "\n",
    "VELOCITY = 2\n",
    "GRAVITY  = 0.981\n",
    "MINRANGE = 15   # Minimium initial distance from wildfire seed\n",
    "MAXRANGE = 30   # Maximum initial distance from wildfire seed\n",
    "BANK_ANGLE_DELTA  = 5\n",
    "\n",
    "plane_marker = parse_path('M 11.640625 15.0625 L 9.304688 13.015625 L 9.300781 9.621094 L 15.125 11.511719 L 15.117188 10.109375 L 9.257812 5.535156 L 9.25 2.851562 L 9.25 1.296875 C 9.253906 1.019531 9.140625 0.777344 8.960938 0.585938 C 8.738281 0.324219 8.410156 0.15625 8.039062 0.160156 C 8.027344 0.160156 8.011719 0.164062 8 0.164062 C 7.988281 0.164062 7.972656 0.160156 7.960938 0.160156 C 7.589844 0.15625 7.257812 0.324219 7.035156 0.585938 C 6.859375 0.777344 6.746094 1.019531 6.746094 1.296875 L 6.746094 2.851562 L 6.742188 5.535156 L 0.882812 10.109375 L 0.875 11.511719 L 6.699219 9.621094 L 6.691406 13.011719 L 4.359375 15.0625 L 4.355469 15.761719 L 4.628906 15.695312 L 4.628906 15.839844 L 7.511719 14.992188 L 8 14.875 L 8.484375 14.992188 L 11.371094 15.839844 L 11.375 15.695312 L 11.644531 15.761719 Z M 11.640625 15.0625 ')\n",
    "plane_marker.vertices -= plane_marker.vertices.mean(axis=0)\n",
    "#plane_marker = plane_marker.transformed(matplotlib.transforms.Affine2D().rotate_deg(180))\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "  return math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "\n",
    "def shift_matrix(matrix, x, y, padding_value=0):\n",
    "  deltaX = Cx-x\n",
    "  deltaY = Cy-y\n",
    "\n",
    "\n",
    "  if deltaX==0 and deltaY==0:\n",
    "    return matrix\n",
    "\n",
    "  return shift(matrix, (deltaY, deltaX), cval = padding_value)\n",
    "  \n",
    "\n",
    "class Drone:\n",
    "\n",
    "  def __init__(self, _droneEnv, _dt, _dti):\n",
    "    self._bank_angle = 0\n",
    "    self._droneEnv = _droneEnv\n",
    "    self._trajectory = []\n",
    "    self._otherDrone = None\n",
    "    self.dt = _dt\n",
    "    self.dti = _dti\n",
    "\n",
    "  def reset(self):\n",
    "    radius = random.random()*(MAXRANGE-MINRANGE) + MINRANGE\n",
    "    angle = (random.random()-0.5)*2*np.pi\n",
    "    self._x = radius*np.cos(angle) + 50\n",
    "    self._y = radius*np.sin(angle) + 50\n",
    "    self._bank_angle = 0\n",
    "    self._trajectory = [(self.x, self.y)]\n",
    "    self._heading_angle = (random.random()-0.5)*2*np.pi\n",
    "\n",
    "  @property\n",
    "  def otherDrone(self):\n",
    "    return self._otherDrone\n",
    "\n",
    "  @otherDrone.setter\n",
    "  def otherDrone(self, _otherDrone):\n",
    "    self._otherDrone = _otherDrone\n",
    "\n",
    "  @property\n",
    "  def trajectory(self):\n",
    "    return self._trajectory\n",
    "\n",
    "  @trajectory.setter\n",
    "  def trajectory(self, _trajectory):\n",
    "    self._trajectory = _trajectory\n",
    "\n",
    "  @property\n",
    "  def x(self):\n",
    "    return self._x\n",
    "\n",
    "  @x.setter\n",
    "  def x(self, _x):\n",
    "    self._x = _x\n",
    "\n",
    "  @property\n",
    "  def y(self):\n",
    "    return self._y\n",
    "\n",
    "  @y.setter\n",
    "  def y(self, _y):\n",
    "    self._y = _y\n",
    "\n",
    "  @property\n",
    "  def mask(self):\n",
    "    Y, X = np.ogrid[:HEIGHT, :WIDTH]\n",
    "    dist_from_center = np.sqrt((X - self.x)**2 + (Y-self.y)**2)\n",
    "    mask = dist_from_center <= self._droneEnv.scan_radius\n",
    "    return mask\n",
    "\n",
    "\n",
    "  @property\n",
    "  def bank_angle(self):\n",
    "    return self._bank_angle\n",
    "\n",
    "  @bank_angle.setter\n",
    "  def bank_angle(self, _bank_angle):\n",
    "    self._bank_angle = _bank_angle\n",
    "\n",
    "  @property\n",
    "  def heading_angle(self):\n",
    "    return self._heading_angle\n",
    "\n",
    "  @heading_angle.setter\n",
    "  def heading_angle(self, _heading_angle):\n",
    "    self._heading_angle = _heading_angle\n",
    "  \n",
    "  @property\n",
    "  def rho(self):\n",
    "    return euclidean_distance(self.x, self.y, self.otherDrone.x, self.otherDrone.y)\n",
    "\n",
    "  @property\n",
    "  def theta(self):\n",
    "    _theta = np.arctan2((self.otherDrone.y-self.y),(self.otherDrone.x-self.x)) - self.heading_angle\n",
    "    \n",
    "    if (_theta > math.pi):\n",
    "      _theta -= 2*math.pi\n",
    "    elif (_theta<-math.pi):\n",
    "      _theta+= 2*math.pi\n",
    "\n",
    "    return _theta\n",
    "\n",
    "  @property\n",
    "  def psi(self):\n",
    "    _psi = self.otherDrone.heading_angle - self.heading_angle\n",
    "\n",
    "    if (_psi > math.pi):\n",
    "      _psi -= 2*math.pi\n",
    "    elif (_psi<-math.pi):\n",
    "      _psi += 2*math.pi\n",
    "\n",
    "    return _psi\n",
    "    \n",
    "  @property\n",
    "  def state(self):\n",
    "    return np.array([\n",
    "        self.bank_angle, \n",
    "        self.rho,\n",
    "        self.theta,\n",
    "        self.psi,\n",
    "        self.otherDrone.bank_angle\n",
    "    ])[np.newaxis,...]\n",
    "\n",
    "  @property\n",
    "  def belief_map(self):\n",
    "    return self._transform_map(self._droneEnv.belief_map_channel.copy())\n",
    "  \n",
    "  @property\n",
    "  def time_elasped_map(self):\n",
    "    return self._transform_map(self._droneEnv.time_map_channel.copy(), 250.0)/250.0\n",
    "\n",
    "  def _transform_map(self, map, padding_value=0):\n",
    "    return rotate(shift_matrix(map, self.x, self.y, padding_value), angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value)\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return np.stack((self.time_elasped_map, self.belief_map), axis=0)[np.newaxis,...]\n",
    "    \n",
    "  def step(self, input):\n",
    "\n",
    "    self.x +=  VELOCITY*math.cos(self.heading_angle)\n",
    "    self.y +=  VELOCITY*math.sin(self.heading_angle)\n",
    "    self.trajectory.append((self.x, self.y))  \n",
    "    self.heading_angle += GRAVITY*np.tan(self.bank_angle)/(VELOCITY)\n",
    "\n",
    "    if (self.heading_angle>np.pi):\n",
    "      self.heading_angle-=2*np.pi\n",
    "    elif (self.heading_angle<-math.pi):\n",
    "      self.heading_angle+=2*np.pi\n",
    "\n",
    "    action =  5.0*np.pi/180.0 if input==1 else -5.0*np.pi/180.0\n",
    "    self.bank_angle += action\n",
    "    \n",
    "\n",
    "    if self.bank_angle >  50.0*np.pi/180.0 or self.bank_angle < -50.0*np.pi/180.0:\n",
    "      self.bank_angle -= action\n",
    "\n",
    "  @property\n",
    "  def reward(self):\n",
    "\n",
    "    return self._reward1()+self._reward2()+self._reward3()+self._reward4()\n",
    "      \n",
    "\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self.time_elasped_map*250.0, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot \n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self.belief_map, cmap='gray_r', vmin=0, vmax=1)  \n",
    "    return belief_map_plot\n",
    "\n",
    "\n",
    "class DronesEnv:\n",
    "  def __init__(self, _height, _width, _dt, _dti, _scan_radius=10):\n",
    "    self._drones = [Drone(self, _dt, _dti), Drone(self, _dt, _dti)]\n",
    "    self._drones[0].otherDrone = self._drones[1]\n",
    "    self._drones[1].otherDrone = self._drones[0]\n",
    "    self._height = _height \n",
    "    self._width  = _width\n",
    "    self._scan_radius = _scan_radius\n",
    "\n",
    "  @property \n",
    "  def scan_radius(self):\n",
    "    return self._scan_radius\n",
    "\n",
    "  @scan_radius.setter\n",
    "  def scan_radius(self, _scan_radius):\n",
    "    self._scan_radius = _scan_radius\n",
    "        \n",
    "  def reset(self, seed, fireMap):\n",
    "\n",
    "\n",
    "    self._drones[0].reset()\n",
    "    self._drones[1].reset()\n",
    "\n",
    "    self._belief_map_channel = seed\n",
    "    self._time_elapsed_channel = np.full(shape=(self._height, self._width), fill_value=250)\n",
    "    self._scan(fireMap)\n",
    "\n",
    "\n",
    "  def _reward(self, drone, fireMap):\n",
    "    return np.count_nonzero(drone.mask & (self._belief_map_channel==0) & (fireMap==1))\n",
    "\n",
    "  def _scan(self, fireMap):\n",
    "\n",
    "    mask = self.drones[0].mask | self.drones[1].mask\n",
    "\n",
    "    self._belief_map_channel[mask] = fireMap[mask]\n",
    "    self._time_elapsed_channel[mask] = 0\n",
    "    self._time_elapsed_channel[~mask & (self._time_elapsed_channel < 250)] += 1\n",
    "\n",
    "\n",
    "  @property \n",
    "  def belief_map_channel(self):\n",
    "    return self._belief_map_channel\n",
    "\n",
    "  @belief_map_channel.setter\n",
    "  def belief_map_channel(self, _belief_map_channel):\n",
    "    self._belief_map_channel = _belief_map_channel\n",
    "\n",
    "  @property \n",
    "  def time_map_channel(self):\n",
    "    return self._time_elapsed_channel\n",
    "\n",
    "  @time_map_channel.setter\n",
    "  def time_map_channel(self, _time_elapsed_channel):\n",
    "    self._time_elapsed_channel = _time_elapsed_channel\n",
    "\n",
    "  @property\n",
    "  def drones(self):\n",
    "    return self._drones\n",
    "\n",
    "  def step(self, input, fireMap):\n",
    "    \n",
    "    rewards = []\n",
    "\n",
    "    for move, drone in zip(input,self.drones):\n",
    "      drone.step(move) \n",
    "      rewards.append(self._reward(drone, fireMap))\n",
    "\n",
    "    self._scan(fireMap)\n",
    "    return rewards\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    time_elasped_plot = ax.imshow(self._time_elapsed_channel, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot\n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=WIDTH)\n",
    "    ax.axis(ymin=0, ymax=HEIGHT)\n",
    "    belief_map_plot = ax.imshow(self._belief_map_channel, cmap='gray_r', vmin=0, vmax=1)\n",
    "    return belief_map_plot   \n",
    "\n",
    "  def plot_drones(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self.drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self.drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x1 = self._drones[0].x + np.cos(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "    y1 = self._drones[0].y + np.sin(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x2 = self._drones[1].x + np.cos(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    y2 = self._drones[1].y + np.sin(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    \n",
    "    ax.plot(x1, y1, '--')\n",
    "    ax.plot(x2, y2, '--')\n",
    "\n",
    "  def plot_trajectory(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self._drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self._drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    x1, y1 = zip(*self.drones[0].trajectory)\n",
    "    x2, y2 = zip(*self.drones[1].trajectory)\n",
    "\n",
    "    ax.plot(x1, y1, '.')\n",
    "    ax.plot(x2, y2, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class BaseDQN(nn.Module):\n",
    "\n",
    "  def _get_conv_out(self):\n",
    "    o = self.conv(torch.zeros((1, self.channels, self.height, self.width)))\n",
    "    return int(np.prod(o.size()))\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "\n",
    "  @height.setter\n",
    "  def height(self, _height):\n",
    "    self._height = _height\n",
    "\n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @width.setter\n",
    "  def width(self, _width):\n",
    "    self._width = _width\n",
    "\n",
    "  @property\n",
    "  def channels(self):\n",
    "    return self._channels\n",
    "\n",
    "  @channels.setter\n",
    "  def channels(self, _channels):\n",
    "    self._channels = _channels\n",
    "\n",
    "  @property\n",
    "  def outputs(self):\n",
    "    return self._outputs\n",
    "\n",
    "  @outputs.setter\n",
    "  def outputs(self, _outputs):\n",
    "    self._outputs = _outputs\n",
    "    \n",
    "  def __init__(self, _channels, _height, _width, _outputs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.channels = _channels\n",
    "    self.height = _height\n",
    "    self.width = _width\n",
    "    self.outputs = _outputs\n",
    "\n",
    "  def forward(self, belief_map, state_vector):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 64\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 200000\n",
    "INIT_SIZE = 25\n",
    "TARGET_UPDATE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class DRQN(BaseDQN):\n",
    "\n",
    "  def __init__(self, channels, height, width, outputs):\n",
    "    super().__init__(channels, height, width, outputs)\n",
    "\n",
    "\n",
    "    self.fc1  = nn.Sequential(\n",
    "      nn.Linear(5, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "      nn.Conv2d(2, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "  \n",
    "    conv_out_size = self._get_conv_out()\n",
    "\n",
    "    self.fc2 = nn.Sequential(\n",
    "      nn.Linear(conv_out_size, 500),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(500, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size = 200, hidden_size = 200, batch_first=True)\n",
    "\n",
    "    self.fc3 = nn.Sequential(\n",
    "      nn.Linear(200, 200),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(200, outputs),\n",
    "    )\n",
    "\n",
    "  def forward(self, belief_map, state_vector, hidden = None, training=False):\n",
    "    \n",
    "\n",
    "    state_vector = state_vector.view(-1, 5)\n",
    "    belief_map   = belief_map.view(-1, 2, 100, 100)\n",
    "  \n",
    "    fc1_out = self.fc1(state_vector)\n",
    "    #print(f'fc1_out.shape: {fc1_out.shape}')\n",
    "    conv_out = self.conv(belief_map)\n",
    "    #print(f'conv_out.shape: {conv_out.shape}')\n",
    "    flatten_out = torch.flatten(conv_out, 1)\n",
    "    #print(f'flatten_out.shape: {flatten_out.shape}')\n",
    "    fc2_out = self.fc2(flatten_out)\n",
    "    #print(f'fc2_out.shape: {fc2_out.shape}')\n",
    "    concatenated = torch.cat((fc1_out, fc2_out), dim=1)\n",
    "\n",
    "    if not training:\n",
    "      concatenated = concatenated.unsqueeze(0)\n",
    "    else:\n",
    "      concatenated = concatenated.view(BATCH_SIZE, SEQ_LENGTH, -1)\n",
    "\n",
    "    if hidden is None:\n",
    "      lstm_out, hidden_out = self.lstm(concatenated)\n",
    "    else:\n",
    "      lstm_out, hidden_out = self.lstm(concatenated, hidden)\n",
    "    \n",
    "\n",
    "    return self.fc3(lstm_out), hidden_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DRQN(channels, screen_height, screen_width, n_actions)\n",
    "policy_net = policy_net.to(device)\n",
    "policy_net.train()\n",
    "steps = 0\n",
    "\n",
    "\n",
    "memory = ReplayMemory(1000)\n",
    "target_net = DRQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "\n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "\n",
    "    belief_maps, state_vectors, actions, next_belief_maps, next_state_vectors, rewards = memory.sample(BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "    policy_output, _ = policy_net(belief_maps.cuda(), state_vectors.cuda(), training=True)\n",
    "    \n",
    "    policy_output = policy_output.view(BATCH_SIZE*SEQ_LENGTH, -1)\n",
    "    actions = actions.view(1, BATCH_SIZE*SEQ_LENGTH)\n",
    "    \n",
    "\n",
    "    state_action_values = policy_output.gather(1, actions.cuda()).squeeze(-1)\n",
    "\n",
    "    target_out, _ = target_net(next_belief_maps.cuda(), next_state_vectors.cuda(), training=True)\n",
    "    target_out = target_out.view(BATCH_SIZE*SEQ_LENGTH, -1)\n",
    "\n",
    "    next_state_values = torch.max(target_out, dim = -1)[0]\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + rewards.view(BATCH_SIZE*SEQ_LENGTH).cuda()\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(0).detach())\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./rnn_policy_weights.pt'\n",
    "        target_file_path = f'./rnn_target_weights.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(belief_map, state_vector, steps, hidden):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output, hidden_out = policy_net(torch.tensor(belief_map, dtype=torch.float).view(1,1,2,25,25).cuda(), torch.tensor(state_vector, dtype=torch.float).view(1,1,5).cuda(), hidden)\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "      return output.max(1)[1].view(1, 1).cpu(), hidden_out\n",
    "  else:\n",
    "    return  torch.tensor([[random.randrange(2)]], dtype=torch.long), hidden_out \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 350000\n",
    "INIT_SIZE = 64\n",
    "TARGET_UPDATE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episodes completed\n",
      "total reward 769\n",
      "loss None\n",
      "steps done 490\n",
      "2 episodes completed\n",
      "total reward 320\n",
      "loss None\n",
      "steps done 1040\n",
      "3 episodes completed\n",
      "total reward 430\n",
      "loss None\n",
      "steps done 1760\n",
      "4 episodes completed\n",
      "total reward 392\n",
      "loss None\n",
      "steps done 2570\n",
      "5 episodes completed\n",
      "total reward 53\n",
      "loss None\n",
      "steps done 3200\n",
      "6 episodes completed\n",
      "total reward 392\n",
      "loss None\n",
      "steps done 3920\n",
      "7 episodes completed\n",
      "total reward 23\n",
      "loss None\n",
      "steps done 4610\n",
      "8 episodes completed\n",
      "total reward 1422\n",
      "loss None\n",
      "steps done 5720\n",
      "9 episodes completed\n",
      "total reward 611\n",
      "loss None\n",
      "steps done 6480\n",
      "10 episodes completed\n",
      "total reward 611\n",
      "loss None\n",
      "steps done 7300\n",
      "11 episodes completed\n",
      "total reward 85\n",
      "loss None\n",
      "steps done 7890\n",
      "12 episodes completed\n",
      "total reward 664\n",
      "loss None\n",
      "steps done 8530\n",
      "13 episodes completed\n",
      "total reward 730\n",
      "loss None\n",
      "steps done 9240\n",
      "14 episodes completed\n",
      "total reward 17\n",
      "loss 0.306121826171875\n",
      "steps done 9860\n",
      "15 episodes completed\n",
      "total reward 452\n",
      "loss 0.5044969916343689\n",
      "steps done 10460\n",
      "16 episodes completed\n",
      "total reward 104\n",
      "loss 0.22695820033550262\n",
      "steps done 11270\n",
      "17 episodes completed\n",
      "total reward 1316\n",
      "loss 0.7365713715553284\n",
      "steps done 12400\n",
      "18 episodes completed\n",
      "total reward 481\n",
      "loss 0.37726420164108276\n",
      "steps done 13080\n",
      "19 episodes completed\n",
      "total reward 2\n",
      "loss 1.320908784866333\n",
      "steps done 14030\n",
      "20 episodes completed\n",
      "total reward 279\n",
      "loss 0.15917813777923584\n",
      "steps done 15060\n",
      "21 episodes completed\n",
      "total reward 181\n",
      "loss 0.5803650617599487\n",
      "steps done 15460\n",
      "22 episodes completed\n",
      "total reward 117\n",
      "loss 0.5471190214157104\n",
      "steps done 16020\n",
      "23 episodes completed\n",
      "total reward 28\n",
      "loss 0.7598332166671753\n",
      "steps done 16940\n",
      "24 episodes completed\n",
      "total reward 1131\n",
      "loss 0.13929979503154755\n",
      "steps done 17790\n",
      "25 episodes completed\n",
      "total reward 851\n",
      "loss 0.5102773904800415\n",
      "steps done 18740\n",
      "update target\n",
      "26 episodes completed\n",
      "total reward 128\n",
      "loss 0.21535708010196686\n",
      "steps done 19970\n",
      "27 episodes completed\n",
      "total reward 5\n",
      "loss 0.8265708684921265\n",
      "steps done 20700\n",
      "28 episodes completed\n",
      "total reward 1217\n",
      "loss 0.1611846685409546\n",
      "steps done 21920\n",
      "29 episodes completed\n",
      "total reward 0\n",
      "loss 0.18769454956054688\n",
      "steps done 22430\n",
      "30 episodes completed\n",
      "total reward 94\n",
      "loss 1.0146183967590332\n",
      "steps done 23100\n",
      "31 episodes completed\n",
      "total reward 983\n",
      "loss 1.4590787887573242\n",
      "steps done 23690\n",
      "32 episodes completed\n",
      "total reward 214\n",
      "loss 0.8943835496902466\n",
      "steps done 24260\n",
      "33 episodes completed\n",
      "total reward 381\n",
      "loss 0.09965170174837112\n",
      "steps done 24830\n",
      "34 episodes completed\n",
      "total reward 360\n",
      "loss 0.9913484454154968\n",
      "steps done 25540\n",
      "35 episodes completed\n",
      "total reward 738\n",
      "loss 0.35763248801231384\n",
      "steps done 26480\n",
      "36 episodes completed\n",
      "total reward 721\n",
      "loss 0.4405129849910736\n",
      "steps done 27190\n",
      "37 episodes completed\n",
      "total reward 424\n",
      "loss 0.6527835130691528\n",
      "steps done 27800\n",
      "38 episodes completed\n",
      "total reward 540\n",
      "loss 0.4804651141166687\n",
      "steps done 28490\n",
      "update target\n",
      "39 episodes completed\n",
      "total reward 1003\n",
      "loss 0.5703459978103638\n",
      "steps done 29360\n",
      "40 episodes completed\n",
      "total reward 1249\n",
      "loss 0.6531621813774109\n",
      "steps done 30390\n",
      "41 episodes completed\n",
      "total reward 1319\n",
      "loss 1.2436890602111816\n",
      "steps done 30960\n",
      "42 episodes completed\n",
      "total reward 391\n",
      "loss 0.2931741774082184\n",
      "steps done 31650\n",
      "43 episodes completed\n",
      "total reward 407\n",
      "loss 0.7600308060646057\n",
      "steps done 32190\n",
      "44 episodes completed\n",
      "total reward 1556\n",
      "loss 0.09184446930885315\n",
      "steps done 33100\n",
      "45 episodes completed\n",
      "total reward 757\n",
      "loss 0.5945014357566833\n",
      "steps done 33740\n",
      "46 episodes completed\n",
      "total reward 0\n",
      "loss 1.2630690336227417\n",
      "steps done 34760\n",
      "47 episodes completed\n",
      "total reward 435\n",
      "loss 0.434562623500824\n",
      "steps done 35480\n",
      "48 episodes completed\n",
      "total reward 704\n",
      "loss 1.2080198526382446\n",
      "steps done 36380\n",
      "49 episodes completed\n",
      "total reward 1323\n",
      "loss 0.09458693861961365\n",
      "steps done 37510\n",
      "50 episodes completed\n",
      "total reward 696\n",
      "loss 0.8453372120857239\n",
      "steps done 38410\n",
      "51 episodes completed\n",
      "total reward 283\n",
      "loss 0.6383000016212463\n",
      "steps done 38960\n",
      "update target\n",
      "52 episodes completed\n",
      "total reward 961\n",
      "loss 0.33568158745765686\n",
      "steps done 39750\n",
      "53 episodes completed\n",
      "total reward 654\n",
      "loss 0.14520590007305145\n",
      "steps done 40510\n",
      "54 episodes completed\n",
      "total reward 899\n",
      "loss 0.7587748765945435\n",
      "steps done 41430\n",
      "55 episodes completed\n",
      "total reward 594\n",
      "loss 1.033735990524292\n",
      "steps done 41950\n",
      "56 episodes completed\n",
      "total reward 740\n",
      "loss 0.7970542907714844\n",
      "steps done 43240\n",
      "57 episodes completed\n",
      "total reward 1226\n",
      "loss 0.27917543053627014\n",
      "steps done 44070\n",
      "58 episodes completed\n",
      "total reward 1486\n",
      "loss 0.16717779636383057\n",
      "steps done 45200\n",
      "59 episodes completed\n",
      "total reward 804\n",
      "loss 0.15785253047943115\n",
      "steps done 45850\n",
      "60 episodes completed\n",
      "total reward 389\n",
      "loss 0.44485464692115784\n",
      "steps done 46520\n",
      "61 episodes completed\n",
      "total reward 1184\n",
      "loss 0.9014750719070435\n",
      "steps done 47210\n",
      "62 episodes completed\n",
      "total reward 1209\n",
      "loss 0.5728221535682678\n",
      "steps done 48330\n",
      "63 episodes completed\n",
      "total reward 218\n",
      "loss 0.4229503870010376\n",
      "steps done 49090\n",
      "update target\n",
      "64 episodes completed\n",
      "total reward 805\n",
      "loss 0.6990032196044922\n",
      "steps done 50070\n",
      "65 episodes completed\n",
      "total reward 622\n",
      "loss 0.783743143081665\n",
      "steps done 50640\n",
      "66 episodes completed\n",
      "total reward 1375\n",
      "loss 0.9091807007789612\n",
      "steps done 51330\n",
      "67 episodes completed\n",
      "total reward 365\n",
      "loss 0.6087046265602112\n",
      "steps done 52180\n",
      "68 episodes completed\n",
      "total reward 1521\n",
      "loss 0.7576979398727417\n",
      "steps done 53410\n",
      "69 episodes completed\n",
      "total reward 535\n",
      "loss 1.0460810661315918\n",
      "steps done 54310\n",
      "70 episodes completed\n",
      "total reward 405\n",
      "loss 0.5522948503494263\n",
      "steps done 55210\n",
      "71 episodes completed\n",
      "total reward 8\n",
      "loss 0.8389889597892761\n",
      "steps done 55780\n",
      "72 episodes completed\n",
      "total reward 584\n",
      "loss 0.49695155024528503\n",
      "steps done 56460\n",
      "73 episodes completed\n",
      "total reward 69\n",
      "loss 1.0000872611999512\n",
      "steps done 57180\n",
      "74 episodes completed\n",
      "total reward 524\n",
      "loss 0.6742225885391235\n",
      "steps done 57910\n",
      "75 episodes completed\n",
      "total reward 771\n",
      "loss 0.8067978620529175\n",
      "steps done 58740\n",
      "update target\n",
      "76 episodes completed\n",
      "total reward 1034\n",
      "loss 0.4978470802307129\n",
      "steps done 59450\n",
      "77 episodes completed\n",
      "total reward 683\n",
      "loss 1.2693605422973633\n",
      "steps done 60240\n",
      "78 episodes completed\n",
      "total reward 660\n",
      "loss 1.3483872413635254\n",
      "steps done 60890\n",
      "79 episodes completed\n",
      "total reward 55\n",
      "loss 0.5175259113311768\n",
      "steps done 61810\n",
      "80 episodes completed\n",
      "total reward 1148\n",
      "loss 0.9994146823883057\n",
      "steps done 62660\n",
      "81 episodes completed\n",
      "total reward 826\n",
      "loss 1.1170518398284912\n",
      "steps done 63680\n",
      "82 episodes completed\n",
      "total reward 765\n",
      "loss 0.7069340944290161\n",
      "steps done 64190\n",
      "83 episodes completed\n",
      "total reward 120\n",
      "loss 0.5288664102554321\n",
      "steps done 64810\n",
      "84 episodes completed\n",
      "total reward 0\n",
      "loss 0.8035236597061157\n",
      "steps done 65390\n",
      "85 episodes completed\n",
      "total reward 800\n",
      "loss 1.1040832996368408\n",
      "steps done 66350\n",
      "86 episodes completed\n",
      "total reward 373\n",
      "loss 0.6697574853897095\n",
      "steps done 67260\n",
      "87 episodes completed\n",
      "total reward 220\n",
      "loss 0.7412188053131104\n",
      "steps done 67860\n",
      "88 episodes completed\n",
      "total reward 1127\n",
      "loss 1.0014657974243164\n",
      "steps done 68680\n",
      "update target\n",
      "89 episodes completed\n",
      "total reward 140\n",
      "loss 0.5959621667861938\n",
      "steps done 69480\n",
      "90 episodes completed\n",
      "total reward 639\n",
      "loss 0.794568657875061\n",
      "steps done 70050\n",
      "91 episodes completed\n",
      "total reward 386\n",
      "loss 0.8155092000961304\n",
      "steps done 71200\n",
      "92 episodes completed\n",
      "total reward 76\n",
      "loss 0.548460841178894\n",
      "steps done 71830\n",
      "93 episodes completed\n",
      "total reward 1025\n",
      "loss 0.6775939464569092\n",
      "steps done 72590\n",
      "94 episodes completed\n",
      "total reward 1172\n",
      "loss 1.0839135646820068\n",
      "steps done 73110\n",
      "95 episodes completed\n",
      "total reward 538\n",
      "loss 0.6316869854927063\n",
      "steps done 73780\n",
      "96 episodes completed\n",
      "total reward 594\n",
      "loss 0.6629126667976379\n",
      "steps done 74410\n",
      "97 episodes completed\n",
      "total reward 856\n",
      "loss 0.8812099695205688\n",
      "steps done 75120\n",
      "98 episodes completed\n",
      "total reward 684\n",
      "loss 0.9314007759094238\n",
      "steps done 75940\n",
      "99 episodes completed\n",
      "total reward 596\n",
      "loss 1.0622162818908691\n",
      "steps done 76600\n",
      "100 episodes completed\n",
      "total reward 489\n",
      "loss 0.7091876268386841\n",
      "steps done 77400\n",
      "101 episodes completed\n",
      "total reward 149\n",
      "loss 0.8092707991600037\n",
      "steps done 78010\n",
      "102 episodes completed\n",
      "total reward 1065\n",
      "loss 0.7542257308959961\n",
      "steps done 78620\n",
      "update target\n",
      "103 episodes completed\n",
      "total reward 550\n",
      "loss 0.6890236139297485\n",
      "steps done 79400\n",
      "104 episodes completed\n",
      "total reward 926\n",
      "loss 0.7890015840530396\n",
      "steps done 79990\n",
      "105 episodes completed\n",
      "total reward 536\n",
      "loss 1.3396626710891724\n",
      "steps done 80680\n",
      "106 episodes completed\n",
      "total reward 290\n",
      "loss 0.8930770754814148\n",
      "steps done 81350\n",
      "107 episodes completed\n",
      "total reward 1324\n",
      "loss 0.6516231298446655\n",
      "steps done 82190\n",
      "108 episodes completed\n",
      "total reward 132\n",
      "loss 0.5304584503173828\n",
      "steps done 82800\n",
      "109 episodes completed\n",
      "total reward 505\n",
      "loss 0.5177396535873413\n",
      "steps done 83720\n",
      "110 episodes completed\n",
      "total reward 117\n",
      "loss 0.8679390549659729\n",
      "steps done 84390\n",
      "111 episodes completed\n",
      "total reward 730\n",
      "loss 0.8687264919281006\n",
      "steps done 85080\n",
      "112 episodes completed\n",
      "total reward 230\n",
      "loss 0.9905483722686768\n",
      "steps done 85810\n",
      "113 episodes completed\n",
      "total reward 468\n",
      "loss 0.7796357274055481\n",
      "steps done 86280\n",
      "114 episodes completed\n",
      "total reward 564\n",
      "loss 1.8438351154327393\n",
      "steps done 87070\n",
      "115 episodes completed\n",
      "total reward 527\n",
      "loss 0.23846375942230225\n",
      "steps done 87750\n",
      "116 episodes completed\n",
      "total reward 629\n",
      "loss 0.7350849509239197\n",
      "steps done 88530\n",
      "update target\n",
      "117 episodes completed\n",
      "total reward 200\n",
      "loss 0.5837814211845398\n",
      "steps done 89470\n",
      "118 episodes completed\n",
      "total reward 1396\n",
      "loss 0.5402997732162476\n",
      "steps done 90460\n",
      "119 episodes completed\n",
      "total reward 647\n",
      "loss 0.7081130743026733\n",
      "steps done 91150\n",
      "120 episodes completed\n",
      "total reward 208\n",
      "loss 0.8042221069335938\n",
      "steps done 91640\n",
      "121 episodes completed\n",
      "total reward 546\n",
      "loss 1.094070553779602\n",
      "steps done 92300\n",
      "122 episodes completed\n",
      "total reward 280\n",
      "loss 0.8026530742645264\n",
      "steps done 92890\n",
      "123 episodes completed\n",
      "total reward 880\n",
      "loss 0.5542583465576172\n",
      "steps done 93690\n",
      "124 episodes completed\n",
      "total reward 347\n",
      "loss 0.5176095962524414\n",
      "steps done 94340\n",
      "125 episodes completed\n",
      "total reward 220\n",
      "loss 1.2453773021697998\n",
      "steps done 95010\n",
      "126 episodes completed\n",
      "total reward 179\n",
      "loss 0.7605831027030945\n",
      "steps done 96050\n",
      "127 episodes completed\n",
      "total reward 245\n",
      "loss 0.4497945308685303\n",
      "steps done 96640\n",
      "128 episodes completed\n",
      "total reward 1281\n",
      "loss 0.554391622543335\n",
      "steps done 97670\n",
      "129 episodes completed\n",
      "total reward 1032\n",
      "loss 0.6360760927200317\n",
      "steps done 98530\n",
      "update target\n",
      "130 episodes completed\n",
      "total reward 98\n",
      "loss 1.0587854385375977\n",
      "steps done 99260\n",
      "131 episodes completed\n",
      "total reward 484\n",
      "loss 0.37886369228363037\n",
      "steps done 99980\n",
      "132 episodes completed\n",
      "total reward 112\n",
      "loss 0.5862644910812378\n",
      "steps done 100640\n",
      "133 episodes completed\n",
      "total reward 537\n",
      "loss 0.6260477304458618\n",
      "steps done 101730\n",
      "134 episodes completed\n",
      "total reward 752\n",
      "loss 0.20984265208244324\n",
      "steps done 102310\n",
      "135 episodes completed\n",
      "total reward 375\n",
      "loss 0.7796007394790649\n",
      "steps done 102930\n",
      "136 episodes completed\n",
      "total reward 1005\n",
      "loss 0.13225910067558289\n",
      "steps done 103940\n",
      "137 episodes completed\n",
      "total reward 346\n",
      "loss 0.9342286586761475\n",
      "steps done 104460\n",
      "138 episodes completed\n",
      "total reward 61\n",
      "loss 0.5880563259124756\n",
      "steps done 105540\n",
      "139 episodes completed\n",
      "total reward 490\n",
      "loss 0.4617953598499298\n",
      "steps done 106250\n",
      "140 episodes completed\n",
      "total reward 107\n",
      "loss 0.5952351093292236\n",
      "steps done 106990\n",
      "141 episodes completed\n",
      "total reward 0\n",
      "loss 0.4187912344932556\n",
      "steps done 107770\n",
      "142 episodes completed\n",
      "total reward 298\n",
      "loss 0.46764320135116577\n",
      "steps done 108600\n",
      "143 episodes completed\n",
      "total reward 251\n",
      "loss 0.5879771709442139\n",
      "steps done 109230\n",
      "update target\n",
      "144 episodes completed\n",
      "total reward 0\n",
      "loss 0.6043621301651001\n",
      "steps done 109790\n",
      "145 episodes completed\n",
      "total reward 122\n",
      "loss 0.30531781911849976\n",
      "steps done 110540\n",
      "146 episodes completed\n",
      "total reward 1001\n",
      "loss 0.5072784423828125\n",
      "steps done 111750\n",
      "147 episodes completed\n",
      "total reward 50\n",
      "loss 0.6256148219108582\n",
      "steps done 112390\n",
      "148 episodes completed\n",
      "total reward 123\n",
      "loss 0.6366788148880005\n",
      "steps done 113110\n",
      "149 episodes completed\n",
      "total reward 643\n",
      "loss 0.8769067525863647\n",
      "steps done 113840\n",
      "150 episodes completed\n",
      "total reward 238\n",
      "loss 0.35415729880332947\n",
      "steps done 114500\n",
      "151 episodes completed\n",
      "total reward 905\n",
      "loss 0.5317728519439697\n",
      "steps done 115210\n",
      "152 episodes completed\n",
      "total reward 81\n",
      "loss 0.2214907705783844\n",
      "steps done 115800\n",
      "153 episodes completed\n",
      "total reward 345\n",
      "loss 0.5600767135620117\n",
      "steps done 116310\n",
      "154 episodes completed\n",
      "total reward 640\n",
      "loss 0.219648540019989\n",
      "steps done 116900\n",
      "155 episodes completed\n",
      "total reward 677\n",
      "loss 0.6491159796714783\n",
      "steps done 117430\n",
      "156 episodes completed\n",
      "total reward 357\n",
      "loss 0.612492024898529\n",
      "steps done 117980\n",
      "157 episodes completed\n",
      "total reward 1748\n",
      "loss 0.879448652267456\n",
      "steps done 118890\n",
      "update target\n",
      "158 episodes completed\n",
      "total reward 459\n",
      "loss 0.9872177839279175\n",
      "steps done 119750\n",
      "159 episodes completed\n",
      "total reward 760\n",
      "loss 0.3787122070789337\n",
      "steps done 120640\n",
      "160 episodes completed\n",
      "total reward 681\n",
      "loss 0.5666302442550659\n",
      "steps done 121630\n",
      "161 episodes completed\n",
      "total reward 96\n",
      "loss 0.2946104109287262\n",
      "steps done 122560\n",
      "162 episodes completed\n",
      "total reward 1153\n",
      "loss 0.2441759705543518\n",
      "steps done 123550\n",
      "163 episodes completed\n",
      "total reward 592\n",
      "loss 0.5705591440200806\n",
      "steps done 124200\n",
      "164 episodes completed\n",
      "total reward 100\n",
      "loss 0.6348739266395569\n",
      "steps done 125000\n",
      "165 episodes completed\n",
      "total reward 739\n",
      "loss 0.9608126878738403\n",
      "steps done 125670\n",
      "166 episodes completed\n",
      "total reward 726\n",
      "loss 0.40581125020980835\n",
      "steps done 126540\n",
      "167 episodes completed\n",
      "total reward 67\n",
      "loss 0.28641408681869507\n",
      "steps done 127520\n",
      "168 episodes completed\n",
      "total reward 1156\n",
      "loss 0.46898162364959717\n",
      "steps done 128220\n",
      "169 episodes completed\n",
      "total reward 507\n",
      "loss 0.1411120444536209\n",
      "steps done 128900\n",
      "update target\n",
      "170 episodes completed\n",
      "total reward 854\n",
      "loss 0.6693048477172852\n",
      "steps done 129730\n",
      "171 episodes completed\n",
      "total reward 575\n",
      "loss 0.15873482823371887\n",
      "steps done 130310\n",
      "172 episodes completed\n",
      "total reward 1103\n",
      "loss 0.15093187987804413\n",
      "steps done 130960\n",
      "173 episodes completed\n",
      "total reward 914\n",
      "loss 0.5077702403068542\n",
      "steps done 131520\n",
      "174 episodes completed\n",
      "total reward 161\n",
      "loss 0.469865083694458\n",
      "steps done 132300\n",
      "175 episodes completed\n",
      "total reward 26\n",
      "loss 0.4232131838798523\n",
      "steps done 132800\n",
      "176 episodes completed\n",
      "total reward 822\n",
      "loss 0.6691238880157471\n",
      "steps done 133600\n",
      "177 episodes completed\n",
      "total reward 838\n",
      "loss 0.6124144792556763\n",
      "steps done 134230\n",
      "178 episodes completed\n",
      "total reward 218\n",
      "loss 0.31500864028930664\n",
      "steps done 135050\n",
      "179 episodes completed\n",
      "total reward 1575\n",
      "loss 0.3147886395454407\n",
      "steps done 136100\n",
      "180 episodes completed\n",
      "total reward 69\n",
      "loss 0.08038970082998276\n",
      "steps done 136810\n",
      "181 episodes completed\n",
      "total reward 596\n",
      "loss 0.8519384264945984\n",
      "steps done 137660\n",
      "182 episodes completed\n",
      "total reward 920\n",
      "loss 0.7733111381530762\n",
      "steps done 138810\n",
      "update target\n",
      "183 episodes completed\n",
      "total reward 369\n",
      "loss 0.7157411575317383\n",
      "steps done 139560\n",
      "184 episodes completed\n",
      "total reward 1157\n",
      "loss 1.2123743295669556\n",
      "steps done 140130\n",
      "185 episodes completed\n",
      "total reward 955\n",
      "loss 0.9620095491409302\n",
      "steps done 141130\n",
      "186 episodes completed\n",
      "total reward 1072\n",
      "loss 1.002901554107666\n",
      "steps done 141690\n",
      "187 episodes completed\n",
      "total reward 795\n",
      "loss 0.5030690431594849\n",
      "steps done 142340\n",
      "188 episodes completed\n",
      "total reward 299\n",
      "loss 0.9688541889190674\n",
      "steps done 142850\n",
      "189 episodes completed\n",
      "total reward 832\n",
      "loss 1.0940366983413696\n",
      "steps done 143700\n",
      "190 episodes completed\n",
      "total reward 30\n",
      "loss 0.9866818189620972\n",
      "steps done 144280\n",
      "191 episodes completed\n",
      "total reward 96\n",
      "loss 0.47559964656829834\n",
      "steps done 144920\n",
      "192 episodes completed\n",
      "total reward 282\n",
      "loss 0.5980483293533325\n",
      "steps done 145660\n",
      "193 episodes completed\n",
      "total reward 118\n",
      "loss 0.8544423580169678\n",
      "steps done 146320\n",
      "194 episodes completed\n",
      "total reward 214\n",
      "loss 1.1636817455291748\n",
      "steps done 146950\n",
      "195 episodes completed\n",
      "total reward 0\n",
      "loss 0.9270246028900146\n",
      "steps done 147480\n",
      "196 episodes completed\n",
      "total reward 688\n",
      "loss 0.8331576585769653\n",
      "steps done 148590\n",
      "update target\n",
      "197 episodes completed\n",
      "total reward 1192\n",
      "loss 0.4644434154033661\n",
      "steps done 149800\n",
      "198 episodes completed\n",
      "total reward 515\n",
      "loss 1.2415881156921387\n",
      "steps done 150890\n",
      "199 episodes completed\n",
      "total reward 997\n",
      "loss 0.8709232807159424\n",
      "steps done 151580\n",
      "200 episodes completed\n",
      "total reward 155\n",
      "loss 0.6735273599624634\n",
      "steps done 152520\n",
      "201 episodes completed\n",
      "total reward 1039\n",
      "loss 1.1601927280426025\n",
      "steps done 153090\n",
      "202 episodes completed\n",
      "total reward 613\n",
      "loss 0.7963621616363525\n",
      "steps done 153710\n",
      "203 episodes completed\n",
      "total reward 830\n",
      "loss 0.6319808959960938\n",
      "steps done 154320\n",
      "204 episodes completed\n",
      "total reward 176\n",
      "loss 0.8254568576812744\n",
      "steps done 155220\n",
      "205 episodes completed\n",
      "total reward 1141\n",
      "loss 0.8584529161453247\n",
      "steps done 156010\n",
      "206 episodes completed\n",
      "total reward 192\n",
      "loss 0.14115017652511597\n",
      "steps done 156710\n",
      "207 episodes completed\n",
      "total reward 649\n",
      "loss 0.8797703981399536\n",
      "steps done 157250\n",
      "208 episodes completed\n",
      "total reward 115\n",
      "loss 1.1997789144515991\n",
      "steps done 158150\n",
      "209 episodes completed\n",
      "total reward 129\n",
      "loss 0.7087403535842896\n",
      "steps done 159170\n",
      "update target\n",
      "210 episodes completed\n",
      "total reward 214\n",
      "loss 0.8006318807601929\n",
      "steps done 159800\n",
      "211 episodes completed\n",
      "total reward 527\n",
      "loss 0.3016034662723541\n",
      "steps done 160360\n",
      "212 episodes completed\n",
      "total reward 240\n",
      "loss 0.7641139626502991\n",
      "steps done 161470\n",
      "213 episodes completed\n",
      "total reward 868\n",
      "loss 0.5476170778274536\n",
      "steps done 162060\n",
      "214 episodes completed\n",
      "total reward 19\n",
      "loss 0.7431116104125977\n",
      "steps done 162670\n",
      "215 episodes completed\n",
      "total reward 31\n",
      "loss 0.4371705949306488\n",
      "steps done 163580\n",
      "216 episodes completed\n",
      "total reward 298\n",
      "loss 0.7504175901412964\n",
      "steps done 164320\n",
      "217 episodes completed\n",
      "total reward 1262\n",
      "loss 0.4421483874320984\n",
      "steps done 165190\n",
      "218 episodes completed\n",
      "total reward 84\n",
      "loss 0.6179336309432983\n",
      "steps done 166180\n",
      "219 episodes completed\n",
      "total reward 949\n",
      "loss 0.3320523500442505\n",
      "steps done 167300\n",
      "220 episodes completed\n",
      "total reward 457\n",
      "loss 0.5228984951972961\n",
      "steps done 168010\n",
      "221 episodes completed\n",
      "total reward 715\n",
      "loss 0.7556849122047424\n",
      "steps done 169040\n",
      "update target\n",
      "222 episodes completed\n",
      "total reward 623\n",
      "loss 1.1767046451568604\n",
      "steps done 169980\n",
      "223 episodes completed\n",
      "total reward 838\n",
      "loss 0.69843590259552\n",
      "steps done 170860\n",
      "224 episodes completed\n",
      "total reward 1223\n",
      "loss 0.40239810943603516\n",
      "steps done 171850\n",
      "225 episodes completed\n",
      "total reward 912\n",
      "loss 0.7946927547454834\n",
      "steps done 172940\n",
      "226 episodes completed\n",
      "total reward 217\n",
      "loss 1.0921008586883545\n",
      "steps done 173520\n",
      "227 episodes completed\n",
      "total reward 108\n",
      "loss 0.5576978921890259\n",
      "steps done 174370\n",
      "228 episodes completed\n",
      "total reward 619\n",
      "loss 0.6106337904930115\n",
      "steps done 175310\n",
      "229 episodes completed\n",
      "total reward 1158\n",
      "loss 0.5574783086776733\n",
      "steps done 176570\n",
      "230 episodes completed\n",
      "total reward 455\n",
      "loss 0.8669595718383789\n",
      "steps done 177280\n",
      "231 episodes completed\n",
      "total reward 646\n",
      "loss 0.9583384394645691\n",
      "steps done 177760\n",
      "232 episodes completed\n",
      "total reward 718\n",
      "loss 0.4742559790611267\n",
      "steps done 178460\n",
      "233 episodes completed\n",
      "total reward 902\n",
      "loss 0.9904915690422058\n",
      "steps done 179170\n",
      "update target\n",
      "234 episodes completed\n",
      "total reward 546\n",
      "loss 0.44914501905441284\n",
      "steps done 179920\n",
      "235 episodes completed\n",
      "total reward 442\n",
      "loss 1.446616768836975\n",
      "steps done 180860\n",
      "236 episodes completed\n",
      "total reward 655\n",
      "loss 0.628193736076355\n",
      "steps done 181610\n",
      "237 episodes completed\n",
      "total reward 994\n",
      "loss 0.1869078278541565\n",
      "steps done 182380\n",
      "238 episodes completed\n",
      "total reward 49\n",
      "loss 0.7611111402511597\n",
      "steps done 183200\n",
      "239 episodes completed\n",
      "total reward 7\n",
      "loss 0.3314855098724365\n",
      "steps done 183800\n",
      "240 episodes completed\n",
      "total reward 1081\n",
      "loss 0.5122693181037903\n",
      "steps done 184410\n",
      "241 episodes completed\n",
      "total reward 730\n",
      "loss 0.5251482725143433\n",
      "steps done 185240\n",
      "242 episodes completed\n",
      "total reward 906\n",
      "loss 0.37261298298835754\n",
      "steps done 185920\n",
      "243 episodes completed\n",
      "total reward 827\n",
      "loss 1.0172526836395264\n",
      "steps done 186670\n",
      "244 episodes completed\n",
      "total reward 583\n",
      "loss 1.2728129625320435\n",
      "steps done 187270\n",
      "245 episodes completed\n",
      "total reward 357\n",
      "loss 0.5925155878067017\n",
      "steps done 188060\n",
      "246 episodes completed\n",
      "total reward 1206\n",
      "loss 0.6693025231361389\n",
      "steps done 188840\n",
      "update target\n",
      "247 episodes completed\n",
      "total reward 69\n",
      "loss 0.8210070133209229\n",
      "steps done 189380\n",
      "248 episodes completed\n",
      "total reward 445\n",
      "loss 1.0878915786743164\n",
      "steps done 189990\n",
      "249 episodes completed\n",
      "total reward 1\n",
      "loss 0.6574222445487976\n",
      "steps done 190720\n",
      "250 episodes completed\n",
      "total reward 619\n",
      "loss 0.7158159017562866\n",
      "steps done 191500\n",
      "251 episodes completed\n",
      "total reward 104\n",
      "loss 0.4738461971282959\n",
      "steps done 192270\n",
      "252 episodes completed\n",
      "total reward 1235\n",
      "loss 0.6943910121917725\n",
      "steps done 193010\n",
      "253 episodes completed\n",
      "total reward 712\n",
      "loss 0.42094314098358154\n",
      "steps done 193720\n",
      "254 episodes completed\n",
      "total reward 946\n",
      "loss 0.9452129602432251\n",
      "steps done 194240\n",
      "255 episodes completed\n",
      "total reward 612\n",
      "loss 1.5168912410736084\n",
      "steps done 194990\n",
      "256 episodes completed\n",
      "total reward 587\n",
      "loss 0.8951830267906189\n",
      "steps done 195630\n",
      "257 episodes completed\n",
      "total reward 122\n",
      "loss 0.7524653673171997\n",
      "steps done 196220\n",
      "258 episodes completed\n",
      "total reward 74\n",
      "loss 0.517071008682251\n",
      "steps done 197280\n",
      "259 episodes completed\n",
      "total reward 495\n",
      "loss 0.6510848999023438\n",
      "steps done 197850\n",
      "260 episodes completed\n",
      "total reward 1034\n",
      "loss 0.7934054732322693\n",
      "steps done 198730\n",
      "update target\n",
      "261 episodes completed\n",
      "total reward 1235\n",
      "loss 0.9441667199134827\n",
      "steps done 199390\n",
      "262 episodes completed\n",
      "total reward 970\n",
      "loss 1.343858003616333\n",
      "steps done 200640\n",
      "263 episodes completed\n",
      "total reward 170\n",
      "loss 0.690574049949646\n",
      "steps done 201350\n",
      "264 episodes completed\n",
      "total reward 1092\n",
      "loss 0.46309423446655273\n",
      "steps done 202420\n",
      "265 episodes completed\n",
      "total reward 390\n",
      "loss 1.2257179021835327\n",
      "steps done 203180\n",
      "266 episodes completed\n",
      "total reward 620\n",
      "loss 0.9268628358840942\n",
      "steps done 204020\n",
      "267 episodes completed\n",
      "total reward 324\n",
      "loss 0.6378483176231384\n",
      "steps done 204790\n",
      "268 episodes completed\n",
      "total reward 460\n",
      "loss 0.3869621157646179\n",
      "steps done 205510\n",
      "269 episodes completed\n",
      "total reward 880\n",
      "loss 0.7727621793746948\n",
      "steps done 206440\n",
      "270 episodes completed\n",
      "total reward 388\n",
      "loss 0.473502516746521\n",
      "steps done 207490\n",
      "271 episodes completed\n",
      "total reward 1062\n",
      "loss 0.9281888008117676\n",
      "steps done 208760\n",
      "update target\n",
      "272 episodes completed\n",
      "total reward 403\n",
      "loss 1.265610694885254\n",
      "steps done 209690\n",
      "273 episodes completed\n",
      "total reward 630\n",
      "loss 0.8702441453933716\n",
      "steps done 210490\n",
      "274 episodes completed\n",
      "total reward 939\n",
      "loss 0.3833267092704773\n",
      "steps done 211540\n",
      "275 episodes completed\n",
      "total reward 760\n",
      "loss 0.8119950890541077\n",
      "steps done 212200\n",
      "276 episodes completed\n",
      "total reward 534\n",
      "loss 0.716785728931427\n",
      "steps done 212810\n",
      "277 episodes completed\n",
      "total reward 331\n",
      "loss 0.616002082824707\n",
      "steps done 213440\n",
      "278 episodes completed\n",
      "total reward 1159\n",
      "loss 0.756600022315979\n",
      "steps done 214360\n",
      "279 episodes completed\n",
      "total reward 970\n",
      "loss 0.37595680356025696\n",
      "steps done 215280\n",
      "280 episodes completed\n",
      "total reward 1044\n",
      "loss 0.6482058763504028\n",
      "steps done 216350\n",
      "281 episodes completed\n",
      "total reward 770\n",
      "loss 0.6007037162780762\n",
      "steps done 217140\n",
      "282 episodes completed\n",
      "total reward 461\n",
      "loss 1.112929105758667\n",
      "steps done 217730\n",
      "283 episodes completed\n",
      "total reward 186\n",
      "loss 0.9437828660011292\n",
      "steps done 218250\n",
      "284 episodes completed\n",
      "total reward 649\n",
      "loss 0.6578164100646973\n",
      "steps done 218870\n",
      "update target\n",
      "285 episodes completed\n",
      "total reward 535\n",
      "loss 0.7864882946014404\n",
      "steps done 219630\n",
      "286 episodes completed\n",
      "total reward 1\n",
      "loss 0.5432605743408203\n",
      "steps done 220500\n",
      "287 episodes completed\n",
      "total reward 174\n",
      "loss 1.1513031721115112\n",
      "steps done 221160\n",
      "288 episodes completed\n",
      "total reward 874\n",
      "loss 0.534541666507721\n",
      "steps done 222170\n",
      "289 episodes completed\n",
      "total reward 307\n",
      "loss 0.7905100584030151\n",
      "steps done 223040\n",
      "290 episodes completed\n",
      "total reward 1049\n",
      "loss 0.28268134593963623\n",
      "steps done 224030\n",
      "291 episodes completed\n",
      "total reward 321\n",
      "loss 0.7920225262641907\n",
      "steps done 224650\n",
      "292 episodes completed\n",
      "total reward 545\n",
      "loss 0.49190762639045715\n",
      "steps done 225260\n",
      "293 episodes completed\n",
      "total reward 988\n",
      "loss 0.6083111763000488\n",
      "steps done 226000\n",
      "294 episodes completed\n",
      "total reward 500\n",
      "loss 0.4114651679992676\n",
      "steps done 226850\n",
      "295 episodes completed\n",
      "total reward 664\n",
      "loss 0.43549421429634094\n",
      "steps done 227650\n",
      "296 episodes completed\n",
      "total reward 143\n",
      "loss 0.44594261050224304\n",
      "steps done 228330\n",
      "297 episodes completed\n",
      "total reward 660\n",
      "loss 0.7767021656036377\n",
      "steps done 229180\n",
      "update target\n",
      "298 episodes completed\n",
      "total reward 0\n",
      "loss 0.750551700592041\n",
      "steps done 229750\n",
      "299 episodes completed\n",
      "total reward 275\n",
      "loss 0.7713667154312134\n",
      "steps done 230460\n",
      "300 episodes completed\n",
      "total reward 334\n",
      "loss 0.4798668324947357\n",
      "steps done 231030\n",
      "301 episodes completed\n",
      "total reward 694\n",
      "loss 0.4751274585723877\n",
      "steps done 231730\n",
      "302 episodes completed\n",
      "total reward 375\n",
      "loss 0.44098272919654846\n",
      "steps done 232370\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_drqn.ipynb Cell 15\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m observation \u001b[39m=\u001b[39m fireEnv\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory)\u001b[39m>\u001b[39mINIT_SIZE:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m   loss \u001b[39m=\u001b[39m optimize_model()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fireEnv\u001b[39m.\u001b[39mfire_in_range(\u001b[39m6\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m   hidden_1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_drqn.ipynb Cell 15\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mglobal\u001b[39;00m update_counter\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m update_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m belief_maps, state_vectors, actions, next_belief_maps, next_state_vectors, rewards \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39;49msample(BATCH_SIZE, SEQ_LENGTH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m policy_output, _ \u001b[39m=\u001b[39m policy_net(belief_maps\u001b[39m.\u001b[39mcuda(), state_vectors\u001b[39m.\u001b[39mcuda(), training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m state_action_values \u001b[39m=\u001b[39m policy_output\u001b[39m.\u001b[39mgather(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, actions\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance_2/train_drqn.ipynb Cell 15\u001b[0m in \u001b[0;36mReplayMemory.sample\u001b[0;34m(self, batch_size, sequence_length)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m   next_state_vectors\u001b[39m.\u001b[39mappend(\u001b[39mlist\u001b[39m(episode_sequence\u001b[39m.\u001b[39mnext_state_vector))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m   rewards\u001b[39m.\u001b[39mappend(\u001b[39mlist\u001b[39m(episode_sequence\u001b[39m.\u001b[39mreward)) \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance_2/train_drqn.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(belief_maps, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat), torch\u001b[39m.\u001b[39mtensor(state_vectors, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat), torch\u001b[39m.\u001b[39mtensor(actions), torch\u001b[39m.\u001b[39;49mtensor(next_belief_maps, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat), torch\u001b[39m.\u001b[39mtensor(next_state_vectors, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat), torch\u001b[39m.\u001b[39mtensor(rewards)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(HEIGHT, WIDTH)\n",
    "dronesEnv = DronesEnv(HEIGHT, WIDTH, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "total_reward = 0\n",
    "seed, observation = fireEnv.reset()\n",
    "dronesEnv.reset(seed, observation)\n",
    "\n",
    "episodeMemory_1 = EpisodeMemory()\n",
    "episodeMemory_2 = EpisodeMemory()\n",
    "\n",
    "hidden_1 = None\n",
    "hidden_2 = None\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "\n",
    "  state_vector_1 = dronesEnv.drones[0].state\n",
    "  map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "\n",
    "  state_vector_2 = dronesEnv.drones[1].state\n",
    "  map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "\n",
    "  for i in range(int(DT/DTI)):\n",
    "\n",
    "    action1, hidden_1 = select_action(map_1, state_vector_1, steps, hidden_1)\n",
    "    action2, hidden_2 = select_action(map_2, state_vector_2, steps, hidden_2)\n",
    "    steps += 2\n",
    "\n",
    "    rewards = dronesEnv.step([action1, action2], observation)\n",
    "\n",
    "    next_state_vector_1 = dronesEnv.drones[0].state\n",
    "    next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "    next_state_vector_2 = dronesEnv.drones[1].state\n",
    "    next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "\n",
    "    reward_1 = rewards[0]\n",
    "    reward_2 = rewards[1]\n",
    "\n",
    "    total_reward += sum(rewards)\n",
    "\n",
    "    episodeMemory_1.push(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1)\n",
    "    episodeMemory_2.push(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2)\n",
    "\n",
    "    \n",
    "    \n",
    "  observation = fireEnv.step()\n",
    "  \n",
    "  if len(memory)>INIT_SIZE:\n",
    "    loss = optimize_model()\n",
    "    \n",
    "  if not fireEnv.fire_in_range(6):\n",
    "\n",
    "    \n",
    "    hidden_1 = None\n",
    "    hidden_2 = None\n",
    "\n",
    "    memory.push(episodeMemory_1)\n",
    "    memory.push(episodeMemory_2)\n",
    "\n",
    "    del episodeMemory_1\n",
    "    del episodeMemory_2\n",
    "    episodeMemory_1 = EpisodeMemory()\n",
    "    episodeMemory_2 = EpisodeMemory()\n",
    "    \n",
    "    print(f'{i_episode} episodes completed')\n",
    "    print(f'total reward {total_reward}')\n",
    "    print(f'loss {loss}')\n",
    "    print(f'steps done {steps}')\n",
    "    i_episode +=1\n",
    "    total_reward = 0\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "        \n",
    "    seed, observation = fireEnv.reset()\n",
    "    dronesEnv.reset(seed, observation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
